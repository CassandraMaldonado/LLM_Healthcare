{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (from peft) (4.49.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from peft) (4.66.5)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/lib/python3.12/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /opt/anaconda3/lib/python3.12/site-packages (from peft) (0.29.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.9.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.12.14)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.6.0 peft-0.15.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Knowledge Distillation for Llama3-OpenBioLLM\")\n",
    "    parser.add_argument(\"--teacher_model_path\", type=str, default=\"aaditya/Llama3-OpenBioLLM-70B\", \n",
    "                        help=\"Path to the teacher model\")\n",
    "    parser.add_argument(\"--student_model_path\", type=str, default=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                        help=\"Path to the student model\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./openbiollm-8b-distilled\", \n",
    "                        help=\"Output directory for the distilled model\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"medical_dialogues\", \n",
    "                        help=\"Dataset name for distillation\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8, \n",
    "                        help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=3e-4, \n",
    "                        help=\"Learning rate for distillation\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=3, \n",
    "                        help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--kd_alpha\", type=float, default=0.5, \n",
    "                        help=\"Weight for KL divergence loss in knowledge distillation\")\n",
    "    parser.add_argument(\"--temperature\", type=float, default=2.0, \n",
    "                        help=\"Temperature for softening probability distributions\")\n",
    "    parser.add_argument(\"--use_lora\", action=\"store_true\", \n",
    "                        help=\"Whether to use LoRA for fine-tuning\")\n",
    "    parser.add_argument(\"--lora_r\", type=int, default=16, \n",
    "                        help=\"LoRA attention dimension\")\n",
    "    parser.add_argument(\"--lora_alpha\", type=int, default=32, \n",
    "                        help=\"LoRA alpha parameter\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "class KnowledgeDistillationTrainer(Trainer):\n",
    "    def __init__(self, teacher_model=None, temperature=1.0, kd_alpha=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.kd_alpha = kd_alpha\n",
    "        # Make sure the teacher model is in evaluation mode\n",
    "        if self.teacher_model is not None:\n",
    "            self.teacher_model.eval()\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Standard cross-entropy loss\n",
    "        outputs = model(**inputs)\n",
    "        ce_loss = outputs.loss\n",
    "        \n",
    "        # Knowledge distillation loss\n",
    "        if self.teacher_model is not None:\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = self.teacher_model(**inputs)\n",
    "            \n",
    "            student_logits = outputs.logits / self.temperature\n",
    "            teacher_logits = teacher_outputs.logits / self.temperature\n",
    "            \n",
    "            # KL divergence loss\n",
    "            kd_loss = torch.nn.functional.kl_div(\n",
    "                torch.nn.functional.log_softmax(student_logits, dim=-1),\n",
    "                torch.nn.functional.softmax(teacher_logits, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            ) * (self.temperature ** 2)\n",
    "            \n",
    "            # Combined loss: (1 - alpha) * CE + alpha * KD\n",
    "            loss = (1 - self.kd_alpha) * ce_loss + self.kd_alpha * kd_loss\n",
    "        else:\n",
    "            loss = ce_loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_length=512):\n",
    "    \"\"\"Tokenize and prepare the examples for distillation.\"\"\"\n",
    "    # For instruction tuning dataset format\n",
    "    prompts = []\n",
    "    for example in examples[\"text\"]:\n",
    "        prompt = f\"<s>[INST] {example} [/INST]\"\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    tokenized_inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "def load_models(args):\n",
    "    \"\"\"Load the teacher and student models.\"\"\"\n",
    "    print(\"Loading the teacher model...\")\n",
    "    teacher_tokenizer = AutoTokenizer.from_pretrained(args.teacher_model_path)\n",
    "    \n",
    "    # Check if accelerate is available\n",
    "    try:\n",
    "        import accelerate\n",
    "        has_accelerate = True\n",
    "    except ImportError:\n",
    "        has_accelerate = False\n",
    "    \n",
    "    # Load teacher model with appropriate settings based on available libraries\n",
    "    if has_accelerate:\n",
    "        teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.teacher_model_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            load_in_8bit=True  # Use quantization for large teacher model\n",
    "        )\n",
    "    else:\n",
    "        # Fallback without device_map and quantization\n",
    "        print(\"Accelerate library not found. Loading model without device_map and quantization.\")\n",
    "        print(\"This might require a lot of GPU memory. Consider installing accelerate: pip install accelerate\")\n",
    "        teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.teacher_model_path,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "    \n",
    "    print(\"Loading the student model...\")\n",
    "    student_tokenizer = AutoTokenizer.from_pretrained(args.student_model_path)\n",
    "    \n",
    "    # Load student model with appropriate settings\n",
    "    if has_accelerate:\n",
    "        student_model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.student_model_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "    else:\n",
    "        student_model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.student_model_path,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "    \n",
    "    # Apply LoRA if requested\n",
    "    if args.use_lora:\n",
    "        print(\"Applying LoRA to the student model...\")\n",
    "        lora_config = LoraConfig(\n",
    "            r=args.lora_r,\n",
    "            lora_alpha=args.lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        student_model = get_peft_model(student_model, lora_config)\n",
    "    \n",
    "    return teacher_model, teacher_tokenizer, student_model, student_tokenizer\n",
    "\n",
    "def load_biomedical_dataset(dataset_name, tokenizer, max_length=512):\n",
    "    \"\"\"Load and preprocess the biomedical dataset for distillation.\"\"\"\n",
    "    # You would need to replace this with your own dataset loading logic\n",
    "    # This is a placeholder example\n",
    "    if dataset_name == \"pubmed_qa\":\n",
    "        dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "        # Process the dataset into a format suitable for distillation\n",
    "        dataset = dataset.map(\n",
    "            lambda x: {\"text\": x[\"question\"] + \" \" + x[\"context\"]},\n",
    "            remove_columns=[\"question\", \"context\", \"pubid\", \"long_answer\", \"label\"]\n",
    "        )\n",
    "    elif dataset_name == \"medical_dialogues\":\n",
    "        # This is a placeholder - replace with actual medical dialogue dataset\n",
    "        dataset = load_dataset(\"csv\", data_files={\"train\": \"medical_dialogues_train.csv\", \n",
    "                                               \"validation\": \"medical_dialogues_val.csv\"})\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer, max_length),\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "def main(teacher_model_path=\"aaditya/Llama3-OpenBioLLM-70B\",\n",
    "         student_model_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "         output_dir=\"./openbiollm-8b-distilled\",\n",
    "         dataset_name=\"medical_dialogues\",\n",
    "         batch_size=8,\n",
    "         learning_rate=3e-4,\n",
    "         num_epochs=3,\n",
    "         kd_alpha=0.5,\n",
    "         temperature=2.0,\n",
    "         use_lora=True,\n",
    "         lora_r=16,\n",
    "         lora_alpha=32):\n",
    "    \"\"\"\n",
    "    Run the knowledge distillation process with provided parameters.\n",
    "    This version works in Jupyter notebooks without requiring command line arguments.\n",
    "    \"\"\"\n",
    "    # Create a simple namespace object to mimic argparse args\n",
    "    class Args:\n",
    "        pass\n",
    "    \n",
    "    args = Args()\n",
    "    args.teacher_model_path = teacher_model_path\n",
    "    args.student_model_path = student_model_path\n",
    "    args.output_dir = output_dir\n",
    "    args.dataset_name = dataset_name\n",
    "    args.batch_size = batch_size\n",
    "    args.learning_rate = learning_rate\n",
    "    args.num_epochs = num_epochs\n",
    "    args.kd_alpha = kd_alpha\n",
    "    args.temperature = temperature\n",
    "    args.use_lora = use_lora\n",
    "    args.lora_r = lora_r\n",
    "    args.lora_alpha = lora_alpha\n",
    "    \n",
    "    # Load models\n",
    "    teacher_model, teacher_tokenizer, student_model, student_tokenizer = load_models(args)\n",
    "    \n",
    "    # Load dataset\n",
    "    tokenized_dataset = load_biomedical_dataset(args.dataset_name, student_tokenizer)\n",
    "    \n",
    "    # Create training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=args.num_epochs,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{args.output_dir}/logs\",\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        fp16=True,\n",
    "        report_to=\"tensorboard\",\n",
    "    )\n",
    "    \n",
    "    # Create custom trainer with knowledge distillation\n",
    "    trainer = KnowledgeDistillationTrainer(\n",
    "        model=student_model,\n",
    "        teacher_model=teacher_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        tokenizer=student_tokenizer,\n",
    "        temperature=args.temperature,\n",
    "        kd_alpha=args.kd_alpha,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting knowledge distillation training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the final model\n",
    "    print(f\"Saving the distilled model to {args.output_dir}\")\n",
    "    trainer.save_model(args.output_dir)\n",
    "    student_tokenizer.save_pretrained(args.output_dir)\n",
    "    \n",
    "    print(\"Knowledge distillation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter environment - using function arguments instead of command line\n",
      "Loading the teacher model...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mipykernel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Call the function directly with defaults when in Jupyter\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning in Jupyter environment - using function arguments instead of command line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     main()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Use argparse when running as a script\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     args \u001b[38;5;241m=\u001b[39m parse_args()\n",
      "Cell \u001b[0;32mIn[17], line 207\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(teacher_model_path, student_model_path, output_dir, dataset_name, batch_size, learning_rate, num_epochs, kd_alpha, temperature, use_lora, lora_r, lora_alpha)\u001b[0m\n\u001b[1;32m    204\u001b[0m args\u001b[38;5;241m.\u001b[39mlora_alpha \u001b[38;5;241m=\u001b[39m lora_alpha\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m teacher_model, teacher_tokenizer, student_model, student_tokenizer \u001b[38;5;241m=\u001b[39m load_models(args)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m    210\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m load_biomedical_dataset(args\u001b[38;5;241m.\u001b[39mdataset_name, student_tokenizer)\n",
      "Cell \u001b[0;32mIn[17], line 99\u001b[0m, in \u001b[0;36mload_models\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Load teacher model with appropriate settings based on available libraries\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_accelerate:\n\u001b[0;32m---> 99\u001b[0m     teacher_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    100\u001b[0m         args\u001b[38;5;241m.\u001b[39mteacher_model_path,\n\u001b[1;32m    101\u001b[0m         device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    102\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m    103\u001b[0m         load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Use quantization for large teacher model\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Fallback without device_map and quantization\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccelerate library not found. Loading model without device_map and quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3611\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3607\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3608\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3609\u001b[0m         )\n\u001b[1;32m   3610\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 3611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   3612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3613\u001b[0m         )\n\u001b[1;32m   3615\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   3616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Check if running in Jupyter\n",
    "    try:\n",
    "        # This will raise NameError if not in IPython/Jupyter\n",
    "        if 'ipykernel' in sys.modules:\n",
    "            # Call the function directly with defaults when in Jupyter\n",
    "            print(\"Running in Jupyter environment - using function arguments instead of command line\")\n",
    "            main()\n",
    "        else:\n",
    "            # Use argparse when running as a script\n",
    "            args = parse_args()\n",
    "            main(\n",
    "                teacher_model_path=args.teacher_model_path,\n",
    "                student_model_path=args.student_model_path,\n",
    "                output_dir=args.output_dir,\n",
    "                dataset_name=args.dataset_name,\n",
    "                batch_size=args.batch_size,\n",
    "                learning_rate=args.learning_rate,\n",
    "                num_epochs=args.num_epochs,\n",
    "                kd_alpha=args.kd_alpha,\n",
    "                temperature=args.temperature,\n",
    "                use_lora=args.use_lora,\n",
    "                lora_r=args.lora_r,\n",
    "                lora_alpha=args.lora_alpha\n",
    "            )\n",
    "    except NameError:\n",
    "        # Default to using argparse\n",
    "        args = parse_args()\n",
    "        main(\n",
    "            teacher_model_path=args.teacher_model_path,\n",
    "            student_model_path=args.student_model_path,\n",
    "            output_dir=args.output_dir,\n",
    "            dataset_name=args.dataset_name,\n",
    "            batch_size=args.batch_size,\n",
    "            learning_rate=args.learning_rate,\n",
    "            num_epochs=args.num_epochs,\n",
    "            kd_alpha=args.kd_alpha,\n",
    "            temperature=args.temperature,\n",
    "            use_lora=args.use_lora,\n",
    "            lora_r=args.lora_r,\n",
    "            lora_alpha=args.lora_alpha\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
