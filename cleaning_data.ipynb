{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>long_answer</th>\n",
       "      <th>final_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21645374</th>\n",
       "      <td>Do mitochondria play a role in remodelling lac...</td>\n",
       "      <td>[Programmed cell death (PCD) is the regulated ...</td>\n",
       "      <td>Results depicted mitochondrial dynamics in viv...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16418930</th>\n",
       "      <td>Landolt C and snellen e acuity: differences in...</td>\n",
       "      <td>[Assessment of visual acuity depends on the op...</td>\n",
       "      <td>Using the charts described, there was only a s...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9488747</th>\n",
       "      <td>Syncope during bathing in infants, a pediatric...</td>\n",
       "      <td>[Apparent life-threatening events in infants a...</td>\n",
       "      <td>\"Aquagenic maladies\" could be a pediatric form...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17208539</th>\n",
       "      <td>Are the long-term results of the transanal pul...</td>\n",
       "      <td>[The transanal endorectal pull-through (TERPT)...</td>\n",
       "      <td>Our long-term study showed significantly bette...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10808977</th>\n",
       "      <td>Can tailored interventions increase mammograph...</td>\n",
       "      <td>[Telephone counseling and tailored print commu...</td>\n",
       "      <td>The effects of the intervention were most pron...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   question  \\\n",
       "21645374  Do mitochondria play a role in remodelling lac...   \n",
       "16418930  Landolt C and snellen e acuity: differences in...   \n",
       "9488747   Syncope during bathing in infants, a pediatric...   \n",
       "17208539  Are the long-term results of the transanal pul...   \n",
       "10808977  Can tailored interventions increase mammograph...   \n",
       "\n",
       "                                                    context  \\\n",
       "21645374  [Programmed cell death (PCD) is the regulated ...   \n",
       "16418930  [Assessment of visual acuity depends on the op...   \n",
       "9488747   [Apparent life-threatening events in infants a...   \n",
       "17208539  [The transanal endorectal pull-through (TERPT)...   \n",
       "10808977  [Telephone counseling and tailored print commu...   \n",
       "\n",
       "                                                long_answer final_answer  \n",
       "21645374  Results depicted mitochondrial dynamics in viv...          yes  \n",
       "16418930  Using the charts described, there was only a s...           no  \n",
       "9488747   \"Aquagenic maladies\" could be a pediatric form...          yes  \n",
       "17208539  Our long-term study showed significantly bette...           no  \n",
       "10808977  The effects of the intervention were most pron...          yes  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"/Users/casey/Documents/GitHub/LLM_Healthcare/ori_pqal.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Transform the dictionary into a DataFrame with the desired columns\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df = df.rename(columns={\n",
    "    'QUESTION': 'question',\n",
    "    'CONTEXTS': 'context',\n",
    "    'LONG_ANSWER': 'long_answer',\n",
    "    'final_decision': 'final_answer'  # Assuming 'final_decision' corresponds to 'final_answer'\n",
    "})\n",
    "df = df[['question', 'context', 'long_answer', 'final_answer']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Normalize Biomedical Text\n",
    "\n",
    "We will handle things like:\n",
    "- Lowercasing (if using uncased model)\n",
    "- Removing excessive whitespace\n",
    "\t•\tOptional: Abbreviation expansion (with domain dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "    return text  # Return the original value if it's not a string\n",
    "\n",
    "for col in ['question', 'context', 'long_answer']:\n",
    "    df[col] = df[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Labels to Integers\n",
    "\n",
    "We’ll map yes / no / maybe -> 0 / 1 / 2 for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'yes': 0, 'no': 1, 'maybe': 2}\n",
    "df['label'] = df['final_answer'].map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with PubMedBERT / BioBERT\n",
    "\n",
    "Using Hugging Face’s transformers to tokenize the input pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Choose the model\n",
    "MODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        str(example['question']),\n",
    "        str(example['context']),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424138c965db47dbb7bc5106b1aa10cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Created dataset and tokenize without batching\n",
    "dataset = Dataset.from_pandas(df[['question', 'context', 'label']])\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Hugging Face Dataset\n",
    "\n",
    "Convert the cleaned DataFrame into the datasets format and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71abd9b74b804db78f6c0e2696dbd2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Batched tokenization (faster)\n",
    "def tokenize_batch_function(examples):\n",
    "    questions = [str(q) for q in examples['question']]\n",
    "    contexts = [str(c) for c in examples['context']]\n",
    "    \n",
    "    return tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Dataset and tokenize with batching\n",
    "dataset = Dataset.from_pandas(df[['question', 'context', 'label']])\n",
    "tokenized_dataset_batched = dataset.map(tokenize_batch_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert the Hugging Face Dataset to a Pandas DataFrame\n",
    "tokenized_df = tokenized_dataset.to_pandas()\n",
    "\n",
    "# Train and temporary sets\n",
    "train_df, temp_df = train_test_split(tokenized_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train, val, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# ting indices to ensure clean dataframes\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataset objects from DataFrames\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cad2eb560c4f81adf58161d0d79c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9130067690f74a5b9f5823555c576d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d38f0210b44a4e8304a3fa816324ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying tokenization to each split\n",
    "train_tokenized = train_dataset.map(tokenize_batch_function, batched=True)\n",
    "val_tokenized = val_dataset.map(tokenize_batch_function, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_batch_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Creating a DatasetDict to organize the datasets\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_tokenized,\n",
    "    'validation': val_tokenized,\n",
    "    'test': test_tokenized\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining metrics function for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36da0c0ee441462db81987dd7056d04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Loading and configuring the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3  # yes, no, maybe -> 0, 1, 2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
