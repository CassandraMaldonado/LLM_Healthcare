{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf056e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    factual_correctness,\n",
    "    semantic_similarity,\n",
    "    answer_relevancy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(df)} examples from {csv_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(\n",
    "    df: pd.DataFrame,\n",
    "    endpoint: str,\n",
    "    model_name: str,\n",
    "    api_key: str,\n",
    "    answer_column: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.0\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    client = OpenAI(\n",
    "        base_url=endpoint,\n",
    "        api_key=api_key\n",
    "    )\n",
    "    \n",
    "    answers = []\n",
    "    \n",
    "    print(f\"Generating answers with {model_name}.\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        question = row['question']\n",
    "        contexts = row['contexts']\n",
    "        \n",
    "        if isinstance(contexts, str):\n",
    "            context_text = contexts\n",
    "        elif isinstance(contexts, list):\n",
    "            context_text = \"\\n\\n\".join(contexts)\n",
    "        else:\n",
    "            context_text = str(contexts)\n",
    "        \n",
    "        prompt = f\"\"\"Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"You are an expert radiologist with over 20 years of clinical experience in diagnostic imaging and medical image interpretation. Provide accurate, evidence-based answers using the provided medical context. Your responses should reflect deep clinical expertise while remaining clear and precise.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=max_new_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer for row {idx}: {e}\")\n",
    "            answer = \"\"\n",
    "        \n",
    "        answers.append(answer)\n",
    "    \n",
    "    df[answer_column] = answers\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ragas_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    answer_column: str\n",
    ") -> Dataset:\n",
    "    \n",
    "    contexts_list = []\n",
    "    for ctx in df['contexts']:\n",
    "        if isinstance(ctx, str):\n",
    "            contexts_list.append([ctx])\n",
    "        elif isinstance(ctx, list):\n",
    "            contexts_list.append(ctx)\n",
    "        else:\n",
    "            contexts_list.append([str(ctx)])\n",
    "    \n",
    "    data_dict = {\n",
    "        'question': df['question'].tolist(),\n",
    "        'contexts': contexts_list,\n",
    "        'answer': df[answer_column].tolist(),\n",
    "        'ground_truth': df['reference_answer'].tolist()\n",
    "    }\n",
    "    \n",
    "    return Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c54b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    df: pd.DataFrame,\n",
    "    answer_column: str,\n",
    "    model_name: str\n",
    ") -> Dict[str, List[float]]:\n",
    "    \n",
    "    print(f\"Evaluating {model_name}.\")\n",
    "    \n",
    "    dataset = prepare_ragas_dataset(df, answer_column)\n",
    "    \n",
    "    metrics = [\n",
    "        factual_correctness,\n",
    "        semantic_similarity,\n",
    "        answer_relevancy\n",
    "    ]\n",
    "    \n",
    "    results = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    # extractinf per example scores.\n",
    "    scores = {\n",
    "        'factual_correctness': results['factual_correctness'],\n",
    "        'semantic_similarity': results['semantic_similarity'],\n",
    "        'answer_relevancy': results['answer_relevancy']\n",
    "    }\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61482b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(scores: List[float]) -> Dict[str, float]:\n",
    "    scores_array = np.array(scores)\n",
    "    return {\n",
    "        'mean': np.mean(scores_array),\n",
    "        'std': np.std(scores_array)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4743fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_df(\n",
    "    df: pd.DataFrame,\n",
    "    baseline_scores: Dict[str, List[float]],\n",
    "    finetuned_scores: Dict[str, List[float]]\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    comparison_data = {\n",
    "        'id': df.get('id', range(len(df))),\n",
    "        'question': df['question'],\n",
    "    }\n",
    "    \n",
    "    # scores for each metric.\n",
    "    for metric in baseline_scores.keys():\n",
    "        comparison_data[f'{metric}_baseline'] = baseline_scores[metric]\n",
    "        comparison_data[f'{metric}_finetuned'] = finetuned_scores[metric]\n",
    "        comparison_data[f'{metric}_diff'] = [\n",
    "            ft - bl for bl, ft in zip(baseline_scores[metric], finetuned_scores[metric])\n",
    "        ]\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "\n",
    "def generate_summary_markdown(\n",
    "    baseline_scores: Dict[str, List[float]],\n",
    "    finetuned_scores: Dict[str, List[float]],\n",
    "    output_path: str\n",
    "):\n",
    "    \n",
    "    summary = \"# RAGAS evaluation results\\n\\n\"\n",
    "    summary += \"## Model comparison\\n\\n\"\n",
    "    summary += \"| Metric | Baseline (Mean ± Std) | Fine-tuned (Mean ± Std) | Difference | Winner |\\n\"\n",
    "    summary += \"|--------|-----------------------|-------------------------|------------|--------|\\n\"\n",
    "    \n",
    "    for metric in baseline_scores.keys():\n",
    "        bl_stats = compute_statistics(baseline_scores[metric])\n",
    "        ft_stats = compute_statistics(finetuned_scores[metric])\n",
    "        diff = ft_stats['mean'] - bl_stats['mean']\n",
    "        winner = \"Fine-tuned\" if diff > 0 else \"Baseline\" if diff < 0 else \"Tie\"\n",
    "        \n",
    "        summary += f\"| {metric} | {bl_stats['mean']:.4f} ± {bl_stats['std']:.4f} | \"\n",
    "        summary += f\"{ft_stats['mean']:.4f} ± {ft_stats['std']:.4f} | \"\n",
    "        summary += f\"{diff:+.4f} | {winner} |\\n\"\n",
    "    \n",
    "    summary += \"\\n## Detailed Statistics\\n\\n\"\n",
    "    \n",
    "    for metric in baseline_scores.keys():\n",
    "        bl_stats = compute_statistics(baseline_scores[metric])\n",
    "        ft_stats = compute_statistics(finetuned_scores[metric])\n",
    "        \n",
    "        summary += f\"### {metric}\\n\\n\"\n",
    "        summary += f\"- **Baseline**: {bl_stats['mean']:.4f} ± {bl_stats['std']:.4f}\\n\"\n",
    "        summary += f\"- **Fine-tuned**: {ft_stats['mean']:.4f} ± {ft_stats['std']:.4f}\\n\"\n",
    "        summary += f\"- **Improvement**: {(ft_stats['mean'] - bl_stats['mean']):.4f} ({((ft_stats['mean'] - bl_stats['mean']) / bl_stats['mean'] * 100):.2f}%)\\n\\n\"\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(summary)\n",
    "    \n",
    "    print(f\"Summary saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(\n",
    "    baseline_scores: Dict[str, List[float]],\n",
    "    finetuned_scores: Dict[str, List[float]],\n",
    "    output_path: str\n",
    "):\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        metrics = list(baseline_scores.keys())\n",
    "        baseline_means = [compute_statistics(baseline_scores[m])['mean'] for m in metrics]\n",
    "        finetuned_means = [compute_statistics(finetuned_scores[m])['mean'] for m in metrics]\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar(x - width/2, baseline_means, width, label='Baseline', alpha=0.8)\n",
    "        ax.bar(x + width/2, finetuned_means, width, label='Fine-tuned', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Metrics')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title('RAGAS Metrics: Baseline vs Fine-tuned')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(metrics, rotation=15, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Plot saved to {output_path}\")\n",
    "    except ImportError:\n",
    "        print(\"Skipping plot generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3bd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Evaluate baseline and fine-tuned models using RAGAS metrics\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('--data_csv', type=str, required=True,\n",
    "                        help='Path to input CSV file')\n",
    "    parser.add_argument('--mode', type=str, choices=['inference', 'precomputed'],\n",
    "                        required=True, help='Evaluation mode')\n",
    "    \n",
    "    # inference mode arguments.\n",
    "    parser.add_argument('--baseline_endpoint', type=str,\n",
    "                        help='Baseline model API endpoint')\n",
    "    parser.add_argument('--finetuned_endpoint', type=str,\n",
    "                        help='Fine-tuned model API endpoint')\n",
    "    parser.add_argument('--baseline_model', type=str, default='Llama-3.1-8B-Instruct',\n",
    "                        help='Baseline model name')\n",
    "    parser.add_argument('--finetuned_model', type=str,\n",
    "                        help='Fine-tuned model name')\n",
    "    parser.add_argument('--api_key', type=str,\n",
    "                        help='API keys in format BASELINE_KEY:FINETUNED_KEY')\n",
    "    \n",
    "    # generation parameters.\n",
    "    parser.add_argument('--max_new_tokens', type=int, default=256,\n",
    "                        help='Maximum new tokens to generate')\n",
    "    parser.add_argument('--temperature', type=float, default=0.0,\n",
    "                        help='Sampling temperature')\n",
    "    \n",
    "    # output arguments.\n",
    "    parser.add_argument('--output_dir', type=str, default='./eval_outputs',\n",
    "                        help='Output directory for results')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df = load_dataset(args.data_csv)\n",
    "    \n",
    "    if args.mode == 'inference':\n",
    "        if not all([args.baseline_endpoint, args.finetuned_endpoint, args.api_key]):\n",
    "            print(\"Error: inference mode requires baseline_endpoint, finetuned_endpoint and api_key\".)\n",
    "            sys.exit(1)\n",
    "        \n",
    "\n",
    "        api_keys = args.api_key.split(':')\n",
    "        if len(api_keys) != 2:\n",
    "            print(\"Error: api_key must be in format BASELINE_KEY:FINETUNED_KEY.\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        baseline_key, finetuned_key = api_keys\n",
    "        \n",
    "        # baseline answers.\n",
    "        df = generate_answers(\n",
    "            df, args.baseline_endpoint, args.baseline_model,\n",
    "            baseline_key, 'answer_baseline',\n",
    "            args.max_new_tokens, args.temperature\n",
    "        )\n",
    "        \n",
    "        # fine-tuned answers.\n",
    "        df = generate_answers(\n",
    "            df, args.finetuned_endpoint, args.finetuned_model,\n",
    "            finetuned_key, 'answer_finetuned',\n",
    "            args.max_new_tokens, args.temperature\n",
    "        )\n",
    "        \n",
    "        output_csv = output_dir / 'dataset_with_answers.csv'\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Dataset with answers saved to {output_csv}\")\n",
    "    \n",
    "    else:\n",
    "        if 'answer_baseline' not in df.columns or 'answer_finetuned' not in df.columns:\n",
    "            print(\"Error: precomputed mode requires answer_baseline and answer_finetuned columns in csv.\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    # evaluating the baseline model.\n",
    "    baseline_scores = evaluate_model(df, 'answer_baseline', 'Baseline')\n",
    "    \n",
    "    # evaluating the fine-tuned model.\n",
    "    finetuned_scores = evaluate_model(df, 'answer_finetuned', 'Fine-tuned')\n",
    "    \n",
    "\n",
    "    comparison_df = create_comparison_df(df, baseline_scores, finetuned_scores)\n",
    "    comparison_csv = output_dir / 'detailed_comparison.csv'\n",
    "    comparison_df.to_csv(comparison_csv, index=False)\n",
    "    print(f\"\\n Detailed comparison saved to {comparison_csv}\")\n",
    "    \n",
    "\n",
    "    summary_md = output_dir / 'summary.md'\n",
    "    generate_summary_markdown(baseline_scores, finetuned_scores, summary_md)\n",
    "    \n",
    "\n",
    "    plot_path = output_dir / 'comparison_plot.png'\n",
    "    plot_comparison(baseline_scores, finetuned_scores, plot_path)\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Evaluation Summary.\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for metric in baseline_scores.keys():\n",
    "        bl_stats = compute_statistics(baseline_scores[metric])\n",
    "        ft_stats = compute_statistics(finetuned_scores[metric])\n",
    "        diff = ft_stats['mean'] - bl_stats['mean']\n",
    "        \n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"  Baseline:    {bl_stats['mean']:.4f} ± {bl_stats['std']:.4f}\")\n",
    "        print(f\"  Fine-tuned:  {ft_stats['mean']:.4f} ± {ft_stats['std']:.4f}\")\n",
    "        print(f\"  Difference:  {diff:+.4f} ({(diff/bl_stats['mean']*100):+.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"Evaluation complete. Results saved to {output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8910610",
   "metadata": {},
   "source": [
    "# How to use it.\n",
    "\n",
    "## Answers already in CSV.\n",
    "python evaluate_with_ragas.py \\\n",
    "  --data_csv ragdology.csv \\\n",
    "  --mode precomputed \\\n",
    "  --output_dir ./eval_outputs\n",
    "\n",
    "## Generate answers from the model endpoints.\n",
    "python evaluate_with_ragas.py \\\n",
    "  --data_csv ragdology.csv \\\n",
    "  --mode inference \\\n",
    "  --baseline_endpoint http://localhost:8000/v1 \\\n",
    "  --finetuned_endpoint http://localhost:9000/v1 \\\n",
    "  --baseline_model Llama-3.1-8B-Instruct \\\n",
    "  --finetuned_model my-llama-3.1-8b-instruct-finetuned \\\n",
    "  --api_key BASEKEY:FINEKEY \\\n",
    "  --output_dir ./eval_outputs\n",
    "\n",
    "## Inference mode with custom generation parameters.\n",
    "python evaluate_with_ragas.py \\\n",
    "  --data_csv ragdology.csv \\\n",
    "  --mode inference \\\n",
    "  --baseline_endpoint http://localhost:8000/v1 \\\n",
    "  --finetuned_endpoint http://localhost:9000/v1 \\\n",
    "  --baseline_model Llama-3.1-8B-Instruct \\\n",
    "  --finetuned_model my-llama-3.1-8b-instruct-finetuned \\\n",
    "  --api_key BASEKEY:FINEKEY \\\n",
    "  --max_new_tokens 512 \\\n",
    "  --temperature 0.1 \\\n",
    "  --output_dir ./eval_outputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
