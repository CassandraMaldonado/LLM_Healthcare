{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3"
      ],
      "metadata": {
        "id": "YRA6wpapbENF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Chain-of-Thought (CoT) Generation Script\n",
        "=======================================\n",
        "\n",
        "This script generates Chain-of-Thought reasoning for medical chatbot training data.\n",
        "It supports multiple AI models and includes robust error handling, batching, and retry logic.\n",
        "\n",
        "Supported Models:\n",
        "- OpenAI GPT-4, GPT-3.5-turbo\n",
        "- Anthropic Claude models (via API)\n",
        "- Hugging Face models (local or API)\n",
        "- Ollama models (local)"
      ],
      "metadata": {
        "id": "Fx-bXC1HbGle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install backoff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcqpGjglbhVJ",
        "outputId": "15c1d5d4-69b8-42b1-e4b9-39488c574d89"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting backoff\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: backoff\n",
            "Successfully installed backoff-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import argparse\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import backoff\n",
        "from tqdm.asyncio import tqdm\n",
        "import openai\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "j1RexoOibJWM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dnfZbNsla8SI"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration class for CoT generation.\n",
        "@dataclass\n",
        "class CoTConfig:\n",
        "    model_name: str = \"gpt-3.5-turbo\"\n",
        "    api_key: Optional[str] = None\n",
        "    batch_size: int = 10\n",
        "    max_retries: int = 3\n",
        "    retry_delay: float = 1.0\n",
        "    max_tokens: int = 512\n",
        "    temperature: float = 0.7\n",
        "    concurrent_requests: int = 5\n",
        "    rate_limit_delay: float = 0.1\n",
        "    local_model_path: Optional[str] = None\n",
        "    use_gpu: bool = True\n",
        "\n",
        "    # API endpoints.\n",
        "    openai_base_url: str = \"https://api.openai.com/v1\"\n",
        "    anthropic_base_url: str = \"https://api.anthropic.com/v1\"\n",
        "    huggingface_base_url: str = \"https://api-inference.huggingface.co/models\"\n",
        "    ollama_base_url: str = \"http://localhost:11434/api\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Chain-of-Thought reasoning.\n",
        "class CoTGenerator:\n",
        "\n",
        "    def __init__(self, config: CoTConfig):\n",
        "        self.config = config\n",
        "        self.session = None\n",
        "        self.local_model = None\n",
        "        self.local_tokenizer = None\n",
        "        self.semaphore = asyncio.Semaphore(config.concurrent_requests)\n",
        "\n",
        "        # Set up API.\n",
        "        self.setup_apis()\n",
        "\n",
        "# API clients and local models.\n",
        "    def setup_apis(self):\n",
        "        if self.config.model_name.startswith(('gpt-', 'text-davinci')):\n",
        "            if not self.config.api_key:\n",
        "                self.config.api_key = os.getenv('OPENAI_API_KEY')\n",
        "            if not self.config.api_key:\n",
        "                logger.warning(\"OpenAI API key not found.\")\n",
        "            else:\n",
        "                openai.api_key = self.config.api_key\n",
        "\n",
        "        elif self.config.model_name.startswith('claude'):\n",
        "            if not self.config.api_key:\n",
        "                self.config.api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "            if not self.config.api_key:\n",
        "                logger.warning(\"Anthropic API key not found. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        elif self.config.local_model_path or self.config.model_name in ['phi-2', 'phi-3']:\n",
        "            self.setup_local_model()\n",
        "\n",
        "    # Set up local model for inference.\n",
        "    def setup_local_model(self):\n",
        "        try:\n",
        "            model_path = self.config.local_model_path or self.config.model_name\n",
        "\n",
        "            logger.info(f\"Loading local model: {model_path}\")\n",
        "\n",
        "            device = \"cuda\" if self.config.use_gpu and torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "            self.local_tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "            self.local_model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "                device_map=\"auto\" if device == \"cuda\" else None,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "            if device == \"cpu\":\n",
        "                self.local_model = self.local_model.to(device)\n",
        "\n",
        "            logger.info(f\"Local model loaded successfully on {device}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load local model: {e}\")\n",
        "            raise\n",
        "\n",
        "# Prompt for generating CoT reasoning. Instruction: The original instruction/question. Answer: The expected answer. Returns: Formatted prompt for CoT generation.\n",
        "    def create_cot_prompt(self, instruction: str, answer: str) -> str:\n",
        "        prompt = f\"\"\"You are a medical expert assistant. For the given medical question and answer, provide detailed Chain-of-Thought reasoning that explains the step-by-step thinking process leading to the answer.\n",
        "\n",
        "Your reasoning should:\n",
        "1. Break down the problem systematically\n",
        "2. Identify key medical concepts and symptoms\n",
        "3. Consider differential diagnoses where applicable\n",
        "4. Explain the logical progression of thoughts\n",
        "5. Be clear, educational, and medically accurate\n",
        "\n",
        "Question: {instruction}\n",
        "\n",
        "Expected Answer: {answer}\n",
        "\n",
        "Please provide the Chain-of-Thought reasoning that leads to this answer. Start your response with \"Chain-of-Thought:\" and then provide the detailed reasoning.\n",
        "\n",
        "Chain-of-Thought:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    # Generate CoT using OpenAI API.\n",
        "    @backoff.on_exception(\n",
        "        backoff.expo,\n",
        "        (aiohttp.ClientError, asyncio.TimeoutError, openai.RateLimitError),\n",
        "        max_tries=3\n",
        "    )\n",
        "    async def generate_cot_openai(self, prompt: str) -> str:\n",
        "        try:\n",
        "            response = await openai.ChatCompletion.acreate(\n",
        "                model=self.config.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=self.config.max_tokens,\n",
        "                temperature=self.config.temperature,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"OpenAI API error: {e}\")\n",
        "            raise\n",
        "\n",
        "    @backoff.on_exception(\n",
        "        backoff.expo,\n",
        "        (aiohttp.ClientError, asyncio.TimeoutError),\n",
        "        max_tries=3\n",
        "    )\n",
        "    async def generate_cot_anthropic(self, prompt: str) -> str:\n",
        "       # Anthropic API.\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"x-api-key\": self.config.api_key,\n",
        "            \"anthropic-version\": \"2023-06-01\"\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "            \"model\": self.config.model_name,\n",
        "            \"max_tokens\": self.config.max_tokens,\n",
        "            \"temperature\": self.config.temperature,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        }\n",
        "\n",
        "        async with self.session.post(\n",
        "            f\"{self.config.anthropic_base_url}/messages\",\n",
        "            headers=headers,\n",
        "            json=payload,\n",
        "            timeout=30\n",
        "        ) as response:\n",
        "            if response.status == 200:\n",
        "                result = await response.json()\n",
        "                return result[\"content\"][0][\"text\"].strip()\n",
        "            else:\n",
        "                error_text = await response.text()\n",
        "                raise aiohttp.ClientError(f\"Anthropic API error: {response.status} - {error_text}\")\n",
        "\n",
        "    @backoff.on_exception(\n",
        "        backoff.expo,\n",
        "        (aiohttp.ClientError, asyncio.TimeoutError),\n",
        "        max_tries=3\n",
        "    )\n",
        "    async def generate_cot_huggingface(self, prompt: str) -> str:\n",
        "        # Hugging Face API.\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.config.api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "            \"inputs\": prompt,\n",
        "            \"parameters\": {\n",
        "                \"max_new_tokens\": self.config.max_tokens,\n",
        "                \"temperature\": self.config.temperature,\n",
        "                \"return_full_text\": False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async with self.session.post(\n",
        "            f\"{self.config.huggingface_base_url}/{self.config.model_name}\",\n",
        "            headers=headers,\n",
        "            json=payload,\n",
        "            timeout=30\n",
        "        ) as response:\n",
        "            if response.status == 200:\n",
        "                result = await response.json()\n",
        "                return result[0][\"generated_text\"].strip()\n",
        "            else:\n",
        "                error_text = await response.text()\n",
        "                raise aiohttp.ClientError(f\"Hugging Face API error: {response.status} - {error_text}\")\n",
        "\n",
        "    @backoff.on_exception(\n",
        "        backoff.expo,\n",
        "        (aiohttp.ClientError, asyncio.TimeoutError),\n",
        "        max_tries=3\n",
        "    )\n",
        "    async def generate_cot_ollama(self, prompt: str) -> str:\n",
        "        # Ollama API.\n",
        "        payload = {\n",
        "            \"model\": self.config.model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False,\n",
        "            \"options\": {\n",
        "                \"temperature\": self.config.temperature,\n",
        "                \"num_predict\": self.config.max_tokens\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async with self.session.post(\n",
        "            f\"{self.config.ollama_base_url}/generate\",\n",
        "            json=payload,\n",
        "            timeout=60\n",
        "        ) as response:\n",
        "            if response.status == 200:\n",
        "                result = await response.json()\n",
        "                return result[\"response\"].strip()\n",
        "            else:\n",
        "                error_text = await response.text()\n",
        "                raise aiohttp.ClientError(f\"Ollama API error: {response.status} - {error_text}\")\n",
        "\n",
        "    def generate_cot_local(self, prompt: str) -> str:\n",
        "        #Local model.\n",
        "        try:\n",
        "            inputs = self.local_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "            if self.config.use_gpu and torch.cuda.is_available():\n",
        "                inputs = inputs.to(\"cuda\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.local_model.generate(\n",
        "                    inputs,\n",
        "                    max_new_tokens=self.config.max_tokens,\n",
        "                    temperature=self.config.temperature,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.local_tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            # Decode only the generated part.\n",
        "            generated_tokens = outputs[0][inputs.shape[1]:]\n",
        "            response = self.local_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "            return response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Local model error: {e}\")\n",
        "            raise\n",
        "\n",
        "    # CoT for a single sample.\n",
        "    async def generate_single_cot(self, sample: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        async with self.semaphore:\n",
        "            try:\n",
        "                instruction = sample.get(\"instruction\", \"\")\n",
        "                answer = sample.get(\"output\", \"\")\n",
        "\n",
        "                if not instruction or not answer:\n",
        "                    logger.warning(f\"Skipping sample with missing instruction or answer\")\n",
        "                    return {**sample, \"cot_reasoning\": \"\"}\n",
        "\n",
        "                prompt = self.create_cot_prompt(instruction, answer)\n",
        "\n",
        "                # Route to the model.\n",
        "                if self.config.model_name.startswith(('gpt-', 'text-davinci')):\n",
        "                    cot_reasoning = await self.generate_cot_openai(prompt)\n",
        "                elif self.config.model_name.startswith('claude'):\n",
        "                    cot_reasoning = await self.generate_cot_anthropic(prompt)\n",
        "                elif self.config.model_name.startswith('hf-'):\n",
        "                    cot_reasoning = await self.generate_cot_huggingface(prompt)\n",
        "                elif self.config.model_name.startswith('ollama-'):\n",
        "                    cot_reasoning = await self.generate_cot_ollama(prompt)\n",
        "                else:\n",
        "                    # Local model.\n",
        "                    cot_reasoning = self.generate_cot_local(prompt)\n",
        "\n",
        "                # Cleaning up the reasoning.\n",
        "                if cot_reasoning.startswith(\"Chain-of-Thought:\"):\n",
        "                    cot_reasoning = cot_reasoning[len(\"Chain-of-Thought:\"):].strip()\n",
        "\n",
        "                # Add rate limiting delay.\n",
        "                await asyncio.sleep(self.config.rate_limit_delay)\n",
        "\n",
        "                return {**sample, \"cot_reasoning\": cot_reasoning}\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error generating CoT for sample: {e}\")\n",
        "                return {**sample, \"cot_reasoning\": f\"Error: {str(e)}\"}\n",
        "\n",
        "    # Process a batch of samples.\n",
        "    async def process_batch(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        if self.local_model is None:\n",
        "            # Use async for API calls\n",
        "            tasks = [self.generate_single_cot(sample) for sample in batch]\n",
        "            return await asyncio.gather(*tasks, return_exceptions=True)\n",
        "        else:\n",
        "            # Use sync for local model\n",
        "            results = []\n",
        "            for sample in batch:\n",
        "                try:\n",
        "                    result = await self.generate_single_cot(sample)\n",
        "                    results.append(result)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing sample: {e}\")\n",
        "                    results.append({**sample, \"cot_reasoning\": f\"Error: {str(e)}\"})\n",
        "            return results\n",
        "\n",
        "    async def generate_cot_for_dataset(self, input_file: str, output_file: str):\n",
        "        \"\"\"Generate CoT for entire dataset.\"\"\"\n",
        "        logger.info(f\"Loading dataset from {input_file}\")\n",
        "\n",
        "        # Load the dataset.\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            dataset = json.load(f)\n",
        "\n",
        "        logger.info(f\"Loaded {len(dataset)} samples\")\n",
        "\n",
        "        # Create batches.\n",
        "        batches = [\n",
        "            dataset[i:i + self.config.batch_size]\n",
        "            for i in range(0, len(dataset), self.config.batch_size)\n",
        "        ]\n",
        "\n",
        "        logger.info(f\"Processing {len(batches)} batches of size {self.config.batch_size}\")\n",
        "\n",
        "        # Set up session for API calls.\n",
        "        if self.local_model is None:\n",
        "            connector = aiohttp.TCPConnector(limit=self.config.concurrent_requests)\n",
        "            timeout = aiohttp.ClientTimeout(total=60)\n",
        "            self.session = aiohttp.ClientSession(connector=connector, timeout=timeout)\n",
        "\n",
        "        try:\n",
        "            results = []\n",
        "\n",
        "            # Process batches with progress bar.\n",
        "            with tqdm(total=len(dataset), desc=\"Generating CoT\") as pbar:\n",
        "                for batch in batches:\n",
        "                    batch_results = await self.process_batch(batch)\n",
        "\n",
        "                    # Handle exceptions in results.\n",
        "                    for result in batch_results:\n",
        "                        if isinstance(result, Exception):\n",
        "                            logger.error(f\"Batch processing error: {result}\")\n",
        "                            results.append({\"error\": str(result)})\n",
        "                        else:\n",
        "                            results.append(result)\n",
        "\n",
        "                    pbar.update(len(batch))\n",
        "\n",
        "                    if len(results) % (self.config.batch_size * 10) == 0:\n",
        "                        self.save_intermediate_results(results, output_file)\n",
        "\n",
        "            # Final results.\n",
        "            logger.info(f\"Saving final results to {output_file}\")\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            # Statistics.\n",
        "            self.generate_statistics(results, input_file, output_file)\n",
        "\n",
        "        finally:\n",
        "            if self.session:\n",
        "                await self.session.close()\n",
        "\n",
        "    # Intermediate results.\n",
        "    def save_intermediate_results(self, results: List[Dict[str, Any]], output_file: str):\n",
        "        intermediate_file = output_file.replace('.json', '_intermediate.json')\n",
        "        with open(intermediate_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        logger.info(f\"Saved intermediate results: {len(results)} samples\")\n",
        "\n",
        "    # Save statistics about CoT generation.\n",
        "    def generate_statistics(self, results: List[Dict[str, Any]], input_file: str, output_file: str):\n",
        "        stats = {\n",
        "            \"total_samples\": len(results),\n",
        "            \"successful_generations\": sum(1 for r in results if r.get(\"cot_reasoning\") and not r.get(\"cot_reasoning\", \"\").startswith(\"Error:\")),\n",
        "            \"failed_generations\": sum(1 for r in results if r.get(\"cot_reasoning\", \"\").startswith(\"Error:\") or not r.get(\"cot_reasoning\")),\n",
        "            \"average_cot_length\": 0,\n",
        "            \"model_config\": {\n",
        "                \"model_name\": self.config.model_name,\n",
        "                \"max_tokens\": self.config.max_tokens,\n",
        "                \"temperature\": self.config.temperature,\n",
        "                \"batch_size\": self.config.batch_size\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Calculate the average of CoT length.\n",
        "        valid_cots = [r.get(\"cot_reasoning\", \"\") for r in results if r.get(\"cot_reasoning\") and not r.get(\"cot_reasoning\", \"\").startswith(\"Error:\")]\n",
        "        if valid_cots:\n",
        "            stats[\"average_cot_length\"] = sum(len(cot.split()) for cot in valid_cots) / len(valid_cots)\n",
        "\n",
        "        # Statistics.\n",
        "        stats_file = output_file.replace('.json', '_cot_stats.json')\n",
        "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"CoT generation statistics saved to {stats_file}\")\n",
        "        logger.info(f\"Success rate: {stats['successful_generations']}/{stats['total_samples']} ({stats['successful_generations']/stats['total_samples']*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "sKw87drfcC9x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    parser = argparse.ArgumentParser(description='Generate Chain-of-Thought reasoning for medical chatbot training data')\n",
        "\n",
        "    parser.add_argument('--input_file', type=str, required=True,\n",
        "                       help='Path to input JSON file (e.g., train.json)')\n",
        "    parser.add_argument('--output_file', type=str, required=True,\n",
        "                       help='Path to output JSON file (e.g., train_with_cot.json)')\n",
        "    parser.add_argument('--model', type=str, default='gpt-3.5-turbo',\n",
        "                       help='Model to use for CoT generation')\n",
        "    parser.add_argument('--api_key', type=str,\n",
        "                       help='API key for the model (can also use environment variables)')\n",
        "    parser.add_argument('--batch_size', type=int, default=10,\n",
        "                       help='Batch size for processing')\n",
        "    parser.add_argument('--max_tokens', type=int, default=512,\n",
        "                       help='Maximum tokens for CoT generation')\n",
        "    parser.add_argument('--temperature', type=float, default=0.7,\n",
        "                       help='Temperature for text generation')\n",
        "    parser.add_argument('--concurrent_requests', type=int, default=5,\n",
        "                       help='Number of concurrent API requests')\n",
        "    parser.add_argument('--local_model_path', type=str,\n",
        "                       help='Path to local model for inference')\n",
        "    parser.add_argument('--use_gpu', action='store_true', default=True,\n",
        "                       help='Use GPU for local model inference')\n",
        "\n",
        "    # In a Colab environment, we need to pass arguments differently\n",
        "    # as argparse expects command-line arguments.\n",
        "    # We will parse known arguments and ignore the rest.\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "\n",
        "    if not os.path.exists(args.input_file):\n",
        "        logger.error(f\"Input file not found: {args.input_file}\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)\n",
        "\n",
        "    config = CoTConfig(\n",
        "        model_name=args.model,\n",
        "        api_key=args.api_key,\n",
        "        batch_size=args.batch_size,\n",
        "        max_tokens=args.max_tokens,\n",
        "        temperature=args.temperature,\n",
        "        concurrent_requests=args.concurrent_requests,\n",
        "        local_model_path=args.local_model_path,\n",
        "        use_gpu=args.use_gpu\n",
        "    )\n",
        "\n",
        "    generator = CoTGenerator(config)\n",
        "\n",
        "    try:\n",
        "        # Generate CoT for dataset.\n",
        "        await generator.generate_cot_for_dataset(args.input_file, args.output_file)\n",
        "\n",
        "        logger.info(\"CoT generation completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during CoT generation: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if there's an existing event loop (like in Colab)\n",
        "    try:\n",
        "        loop = asyncio.get_running_loop()\n",
        "        if loop.is_running():\n",
        "            # If in an environment with a running loop, use await\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            await main()\n",
        "        else:\n",
        "            # Otherwise, use asyncio.run()\n",
        "            asyncio.run(main())\n",
        "    except RuntimeError:\n",
        "        # If get_running_loop() raises RuntimeError, it means no loop is running\n",
        "        asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "URkvCil4dptz",
        "outputId": "3fd30b5b-83a3-46e8-96b4-d24a7d226e2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "main() got an unexpected keyword argument 'input_file'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3031015050.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Pass arguments directly to main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/path/to/your/input.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/path/to/your/output.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Otherwise, use asyncio.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: main() got an unexpected keyword argument 'input_file'"
          ]
        }
      ]
    }
  ]
}