{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "740d197dc6d640e588408b2df84638fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c7da8ef59804bd282e88a2fb1133e03",
              "IPY_MODEL_1644ed15ca9f41b38143db2011245496",
              "IPY_MODEL_23c5b91e04e1415fb2d41011dd43252c"
            ],
            "layout": "IPY_MODEL_b193259dfbff463f8631fae81ce5b88e"
          }
        },
        "2c7da8ef59804bd282e88a2fb1133e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_094a94798dc242e5a5759ca925e4fd35",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e45f70101c684e29bd3f22a913416242",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "1644ed15ca9f41b38143db2011245496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a34dd75a16c64554bf5802dcb0d32685",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_818e916d23664aaf8c983307dff27c6c",
            "value": 4
          }
        },
        "23c5b91e04e1415fb2d41011dd43252c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db6ee12162a44b2bab9784173fcac764",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_11aef54d3115408397d178e8b5587b31",
            "value": "â€‡4/4â€‡[00:16&lt;00:00,â€‡â€‡3.55s/it]"
          }
        },
        "b193259dfbff463f8631fae81ce5b88e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "094a94798dc242e5a5759ca925e4fd35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e45f70101c684e29bd3f22a913416242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a34dd75a16c64554bf5802dcb0d32685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "818e916d23664aaf8c983307dff27c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db6ee12162a44b2bab9784173fcac764": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11aef54d3115408397d178e8b5587b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ae045d196cb4b93b9eb2740b9c1b6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76a3114429a64e3388650d397920499a",
              "IPY_MODEL_dc4e1813b7ad4b4a92c15a4958289474",
              "IPY_MODEL_29db394c1231429287f0221182c05a2e"
            ],
            "layout": "IPY_MODEL_e6b0fc2129c348998b446db717c7f5ad"
          }
        },
        "76a3114429a64e3388650d397920499a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4da5a2072d41474da2b98d4b62d04437",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_72898270ffa145f1a6c3119665e2fa13",
            "value": "Seedingâ€‡memoryâ€‡bank:â€‡100%"
          }
        },
        "dc4e1813b7ad4b4a92c15a4958289474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aa0e1c20a5a48eb9bd9ee36402f7247",
            "max": 152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b71badbf2e5f455d8c36584e33b4afbd",
            "value": 152
          }
        },
        "29db394c1231429287f0221182c05a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12d894765e9b42edac09af36d3b6db96",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7682a725471547d0a2d033fd9050620c",
            "value": "â€‡152/152â€‡[00:00&lt;00:00,â€‡13211.50it/s]"
          }
        },
        "e6b0fc2129c348998b446db717c7f5ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4da5a2072d41474da2b98d4b62d04437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72898270ffa145f1a6c3119665e2fa13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0aa0e1c20a5a48eb9bd9ee36402f7247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b71badbf2e5f455d8c36584e33b4afbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12d894765e9b42edac09af36d3b6db96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7682a725471547d0a2d033fd9050620c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDy0FsThrVL0",
        "outputId": "5f91dec4-8005-4a06-df0b-f5474db07873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: bitsandbytes 0.48.2\n",
            "Uninstalling bitsandbytes-0.48.2:\n",
            "  Successfully uninstalled bitsandbytes-0.48.2\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch peft datasets sentence-transformers\n",
        "# Force uninstall and reinstall bitsandbytes to ensure the latest version is properly loaded for transformers\n",
        "!pip uninstall -y bitsandbytes\n",
        "!pip install -q bitsandbytes transformers --upgrade\n",
        "!pip install -q pandas openpyxl tqdm accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuration\n",
        "USE_OPEN_MODEL = False  # Set True to skip authentication entirely\n",
        "HF_TOKEN = None\n",
        "\n",
        "# Try to get token from Colab secrets\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print(\"âœ… Found HF_TOKEN in Colab secrets\")\n",
        "except Exception:\n",
        "    print(\"âš ï¸ HF_TOKEN not found in Colab secrets\")\n",
        "\n",
        "# Login if we have a token\n",
        "if HF_TOKEN and not USE_OPEN_MODEL:\n",
        "    from huggingface_hub import login\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Logged in to HuggingFace\")\n",
        "    BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "else:\n",
        "    print(\"Using open model (no authentication required)\")\n",
        "    BASE_MODEL = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"  # Open community copy\n",
        "    USE_OPEN_MODEL = True\n",
        "\n",
        "print(f\"Base model: {BASE_MODEL}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWlNy5BUt0TD",
        "outputId": "cbb4efed-6e9c-4859-cf0c-c39990bbd0c4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Found HF_TOKEN in Colab secrets\n",
            "Logged in to HuggingFace\n",
            "Base model: meta-llama/Llama-3.1-8B-Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import re\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Dict, List, Optional, Tuple, Any, Union\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    TrainerCallback,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    PeftModel,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "honQOPkIwQB1",
        "outputId": "8dacbe9c-5257-41d3-a0d6-9a1269f6d96f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "   GPU: NVIDIA A100-SXM4-80GB\n",
            "   Memory: 85.2 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class OptimalMementoConfig:\n",
        "    \"\"\"\n",
        "    Hyperparameters optimized for radiology Memento training.\n",
        "\n",
        "    Based on analysis of your failure modes:\n",
        "    1. Reduced momentum coefficients (prevent over-smoothing)\n",
        "    2. Lower learning rate (prevent catastrophic forgetting)\n",
        "    3. Smaller batch size (better gradient quality)\n",
        "    4. Increased LoRA rank (more capacity for medical knowledge)\n",
        "    5. Aggressive gradient clipping (stability)\n",
        "    \"\"\"\n",
        "\n",
        "    # ===== MODEL =====\n",
        "    base_model: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "    # ===== LoRA (INCREASED) =====\n",
        "    lora_r: int = 64           # â†‘ from 32 (need more capacity)\n",
        "    lora_alpha: int = 128      # â†‘ from 64 (2x rank)\n",
        "    lora_dropout: float = 0.05\n",
        "    lora_target_modules: list = None  # Set in __post_init__\n",
        "\n",
        "    # ===== TRAINING =====\n",
        "    learning_rate: float = 5e-6          # â†“ from 2e-5 (prevent overfitting)\n",
        "    num_epochs: int = 5                  # â†‘ from 3 (more gradual learning)\n",
        "    batch_size: int = 1                  # â†“ from 2 (better gradients)\n",
        "    gradient_accumulation: int = 16      # â†‘ from 8 (effective batch=16)\n",
        "    max_seq_length: int = 1536          # â†“ from 2048 (reduce memory pressure)\n",
        "\n",
        "    warmup_ratio: float = 0.1           # â†‘ from 0.05 (smoother start)\n",
        "    weight_decay: float = 0.01\n",
        "    max_grad_norm: float = 0.5          # â†“ from default (aggressive clipping)\n",
        "    lr_scheduler_type: str = \"cosine\"\n",
        "\n",
        "    # ===== MOMENTUM (CORRECTED) =====\n",
        "    momentum_alpha: float = 0.85        # â†“ from 0.95 (faster adaptation)\n",
        "    momentum_beta: float = 0.98         # â†“ from 0.99 (less smoothing)\n",
        "    momentum_weight: float = 0.3        # â†“ from 0.4 (less momentum influence)\n",
        "    momentum_warmup_steps: int = 100    # â†“ from 500 (faster ramp-up)\n",
        "\n",
        "    # ===== MEMORY BANK =====\n",
        "    memory_capacity: int = 1000         # â†“ from 2000 (quality over quantity)\n",
        "    retrieval_top_k: int = 2            # â†“ from 5 (reduce noise)\n",
        "    min_confidence: float = 0.85        # Only high-quality cases\n",
        "    diversity_threshold: float = 0.7    # Prevent redundant examples\n",
        "    update_memory_every: int = 50       # Update frequency\n",
        "\n",
        "    # ===== EVALUATION =====\n",
        "    eval_strategy: str = \"steps\"\n",
        "    eval_steps: int = 50                # â†“ from 100 (more frequent checks)\n",
        "    save_steps: int = 100               # â†“ from 200\n",
        "    save_total_limit: int = 3\n",
        "\n",
        "    # ===== MEMORY MANAGEMENT =====\n",
        "    use_4bit: bool = True\n",
        "    bf16: bool = True  # Use bfloat16 if available\n",
        "    gradient_checkpointing: bool = True\n",
        "\n",
        "    # ===== DATA =====\n",
        "    mask_instruction: bool = False      # âœ… CRITICAL: Keep context\n",
        "    use_memory_augmentation: bool = True # âœ… Use memory augmentation\n",
        "    output_dir: str = \"./memento_output\"\n",
        "    train_data: str = \"/content/train.jsonl\"\n",
        "    val_data: str = \"/content/val.jsonl\"\n",
        "    test_data: str = \"/content/test.jsonl\" # Added test data path\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.lora_target_modules = [\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ]\n",
        "\n",
        "    def to_training_args(self, output_dir: str):\n",
        "        \"\"\"Convert to HuggingFace TrainingArguments.\"\"\"\n",
        "        from transformers import TrainingArguments\n",
        "        import torch\n",
        "\n",
        "        return TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=self.num_epochs,\n",
        "            per_device_train_batch_size=self.batch_size,\n",
        "            per_device_eval_batch_size=self.batch_size,\n",
        "            gradient_accumulation_steps=self.gradient_accumulation,\n",
        "            learning_rate=self.learning_rate,\n",
        "            lr_scheduler_type=self.lr_scheduler_type,\n",
        "            warmup_ratio=self.warmup_ratio,\n",
        "            weight_decay=self.weight_decay,\n",
        "            max_grad_norm=self.max_grad_norm,\n",
        "            logging_steps=10,\n",
        "            eval_strategy=self.eval_strategy,\n",
        "            eval_steps=self.eval_steps,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=self.save_steps,\n",
        "            save_total_limit=self.save_total_limit,\n",
        "            bf16=self.bf16 and torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8,\n",
        "            fp16=not self.bf16 and torch.cuda.is_available(),\n",
        "            gradient_checkpointing=self.gradient_checkpointing,\n",
        "            report_to=\"none\",\n",
        "            seed=42,\n",
        "            dataloader_num_workers=2,\n",
        "            remove_unused_columns=False,\n",
        "        )\n",
        "\n",
        "# Example usage:\n",
        "config = OptimalMementoConfig()\n",
        "print(f\"\"\"\n",
        "Ê— OPTIMAL CONFIGURATION\n",
        "========================\n",
        "\n",
        "Model:\n",
        "  Base: {config.base_model}\n",
        "  LoRA rank: {config.lora_r} (â†‘ from 32)\n",
        "  LoRA alpha: {config.lora_alpha}\n",
        "\n",
        "Training:\n",
        "  Learning rate: {config.learning_rate} (â†“ from 2e-5)\n",
        "  Batch size: {config.batch_size}\n",
        "  Grad accumulation: {config.gradient_accumulation}\n",
        "  Effective batch: {config.batch_size * config.gradient_accumulation}\n",
        "  Max seq length: {config.max_seq_length}\n",
        "  Epochs: {config.num_epochs}\n",
        "\n",
        "Momentum:\n",
        "  Alpha (short): {config.momentum_alpha} (â†“ from 0.95)\n",
        "  Beta (long): {config.momentum_beta} (â†“ from 0.99)\n",
        "  Weight: {config.momentum_weight} (â†“ from 0.4)\n",
        "  Warmup: {config.momentum_warmup_steps} steps\n",
        "\n",
        "Memory Bank:\n",
        "  Capacity: {config.memory_capacity} cases\n",
        "  Top-k: {config.retrieval_top_k}\n",
        "  Min confidence: {config.min_confidence}\n",
        "\n",
        "Data Processing:\n",
        "  Mask instruction: {config.mask_instruction} âœ…\n",
        "  Use memory: {config.use_memory_augmentation} âœ…\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlI_weEiwVQK",
        "outputId": "b360288b-7bc5-49cb-8da0-3ae665a935fc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ê— OPTIMAL CONFIGURATION\n",
            "========================\n",
            "\n",
            "Model:\n",
            "  Base: meta-llama/Llama-3.1-8B-Instruct\n",
            "  LoRA rank: 64 (â†‘ from 32)\n",
            "  LoRA alpha: 128\n",
            "\n",
            "Training:\n",
            "  Learning rate: 5e-06 (â†“ from 2e-5)\n",
            "  Batch size: 1\n",
            "  Grad accumulation: 16\n",
            "  Effective batch: 16\n",
            "  Max seq length: 1536\n",
            "  Epochs: 5\n",
            "\n",
            "Momentum:\n",
            "  Alpha (short): 0.85 (â†“ from 0.95)\n",
            "  Beta (long): 0.98 (â†“ from 0.99)\n",
            "  Weight: 0.3 (â†“ from 0.4)\n",
            "  Warmup: 100 steps\n",
            "\n",
            "Memory Bank:\n",
            "  Capacity: 1000 cases\n",
            "  Top-k: 2\n",
            "  Min confidence: 0.85\n",
            "\n",
            "Data Processing:\n",
            "  Mask instruction: False âœ…\n",
            "  Use memory: True âœ…\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActiveMementoMemoryBank:\n",
        "    \"\"\"\n",
        "    Memory bank that ACTIVELY participates in training via:\n",
        "    1. Dynamic retrieval during forward pass\n",
        "    2. Context-aware example selection\n",
        "    3. Quality-filtered case storage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        capacity: int = 2000,\n",
        "        top_k: int = 3,  # Reduced from 5 to prevent noise\n",
        "        min_confidence: float = 0.85,  # Only store high-quality cases\n",
        "        diversity_threshold: float = 0.7  # Prevent redundant cases\n",
        "    ):\n",
        "        self.capacity = capacity\n",
        "        self.top_k = top_k\n",
        "        self.min_confidence = min_confidence\n",
        "        self.diversity_threshold = diversity_threshold\n",
        "\n",
        "        self.cases: List[Dict] = []\n",
        "        self._embeddings = None\n",
        "        self._embedder = None\n",
        "        self._quality_scores = []  # Track case quality\n",
        "\n",
        "    def _init_embedder(self):\n",
        "        if self._embedder is None:\n",
        "            self._embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            print(\"âœ… Embedder initialized\")\n",
        "\n",
        "    def add(\n",
        "        self,\n",
        "        task_type: str,\n",
        "        query: str,\n",
        "        findings: str,  # NEW: Store original findings\n",
        "        clinical: str,  # NEW: Store clinical context\n",
        "        response: str,\n",
        "        confidence: float = 1.0\n",
        "    ):\n",
        "        \"\"\"Add case with quality filtering and diversity check.\"\"\"\n",
        "        if confidence < self.min_confidence:\n",
        "            return\n",
        "\n",
        "        # Check diversity (prevent near-duplicates)\n",
        "        if len(self.cases) > 0:\n",
        "            self._init_embedder()\n",
        "            new_emb = self._embedder.encode([query], normalize_embeddings=True)[0]\n",
        "\n",
        "            if self._embeddings is not None:\n",
        "                sims = self._embeddings @ new_emb\n",
        "                if np.max(sims) > self.diversity_threshold:\n",
        "                    return  # Too similar to existing case\n",
        "\n",
        "        # Add case\n",
        "        case = {\n",
        "            \"task_type\": task_type,\n",
        "            \"query\": query[:500],\n",
        "            \"findings\": findings[:1000],  # Store full findings\n",
        "            \"clinical\": clinical[:300],\n",
        "            \"response\": response,\n",
        "            \"confidence\": confidence\n",
        "        }\n",
        "\n",
        "        self.cases.append(case)\n",
        "        self._quality_scores.append(confidence)\n",
        "\n",
        "        # Evict lowest quality if over capacity\n",
        "        if len(self.cases) > self.capacity:\n",
        "            min_idx = np.argmin(self._quality_scores)\n",
        "            self.cases.pop(min_idx)\n",
        "            self._quality_scores.pop(min_idx)\n",
        "\n",
        "        self._embeddings = None  # Invalidate cache\n",
        "\n",
        "    def retrieve_for_training(\n",
        "        self,\n",
        "        task_type: str,\n",
        "        query: str,\n",
        "        findings: str,\n",
        "        clinical: str\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Retrieve similar cases and format as training context.\n",
        "        Returns formatted string to PREPEND to training prompt.\n",
        "        \"\"\"\n",
        "        if not self.cases:\n",
        "            return \"\"\n",
        "\n",
        "        self._init_embedder()\n",
        "\n",
        "        # Build embeddings if needed\n",
        "        if self._embeddings is None:\n",
        "            texts = [c[\"query\"] for c in self.cases]\n",
        "            self._embeddings = self._embedder.encode(texts, normalize_embeddings=True)\n",
        "\n",
        "        # Retrieve\n",
        "        q_emb = self._embedder.encode([query], normalize_embeddings=True)\n",
        "        sims = (self._embeddings @ q_emb[0])\n",
        "\n",
        "        # Get top-k of matching task type\n",
        "        indices = np.argsort(-sims)\n",
        "        results = []\n",
        "        for idx in indices:\n",
        "            if self.cases[idx][\"task_type\"] == task_type:\n",
        "                results.append(self.cases[idx])\n",
        "                if len(results) >= self.top_k:\n",
        "                    break\n",
        "\n",
        "        if not results:\n",
        "            return \"\"\n",
        "\n",
        "        # Format as few-shot examples\n",
        "        context_parts = [\"Here are similar cases for reference:\\n\"]\n",
        "        for i, case in enumerate(results, 1):\n",
        "            context_parts.append(f\"\\n**Example {i}:**\")\n",
        "            context_parts.append(f\"Clinical: {case['clinical']}\")\n",
        "            context_parts.append(f\"Findings: {case['findings'][:200]}...\")\n",
        "            context_parts.append(f\"Impression: {case['response'][:150]}...\\n\")\n",
        "\n",
        "        context_parts.append(\"\\nNow generate an impression for the current case:\\n\")\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def save(self, path: str):\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump({\n",
        "                'cases': self.cases,\n",
        "                'quality_scores': self._quality_scores\n",
        "            }, f, indent=2)\n",
        "        print(f\"ðŸ’¾ Saved {len(self.cases)} cases (avg quality: {np.mean(self._quality_scores):.3f})\")\n",
        "\n",
        "    def load(self, path: str):\n",
        "        with open(path) as f:\n",
        "            data = json.load(f)\n",
        "            self.cases = data['cases']\n",
        "            self._quality_scores = data.get('quality_scores', [1.0] * len(self.cases))\n",
        "        print(f\"ðŸ“‚ Loaded {len(self.cases)} cases\")"
      ],
      "metadata": {
        "id": "xF12xnb1wbc0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 6: Dataset Class\n",
        "# ============================================================\n",
        "\n",
        "class MementoRadiologyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that integrates memory bank retrieval during training.\n",
        "\n",
        "    Key improvements:\n",
        "    1. Retrieves similar cases as few-shot context\n",
        "    2. Preserves full clinical context in training\n",
        "    3. Properly formats prompts for grounded generation\n",
        "    4. No aggressive masking that destroys context\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path: str,\n",
        "        tokenizer,\n",
        "        memory_bank,  # NEW: Pass memory bank\n",
        "        max_length: int = 2048,\n",
        "        use_memory: bool = True,  # Toggle memory augmentation\n",
        "        mask_instruction: bool = False  # CRITICAL: Don't mask by default\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.memory_bank = memory_bank\n",
        "        self.use_memory = use_memory\n",
        "        self.mask_instruction = mask_instruction\n",
        "        self.examples = []\n",
        "\n",
        "        # Load examples\n",
        "        with open(data_path) as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    ex = json.loads(line)\n",
        "\n",
        "                    # Parse components from prompt\n",
        "                    prompt = ex['prompt']\n",
        "                    clinical_match = re.search(\n",
        "                        r'\\*\\*Clinical Context:\\*\\*\\s*(.*?)(?=\\*\\*|\\n\\n)',\n",
        "                        prompt\n",
        "                    )\n",
        "                    findings_match = re.search(\n",
        "                        r'\\*\\*Findings:\\*\\*\\s*(.*?)(?=Generate|$)',\n",
        "                        prompt,\n",
        "                        re.DOTALL\n",
        "                    )\n",
        "\n",
        "                    ex['clinical'] = clinical_match.group(1).strip() if clinical_match else \"\"\n",
        "                    ex['findings'] = findings_match.group(1).strip() if findings_match else \"\"\n",
        "\n",
        "                    self.examples.append(ex)\n",
        "\n",
        "        print(f\"ðŸ“Š Loaded {len(self.examples)} examples\")\n",
        "        if use_memory:\n",
        "            print(f\"ðŸ§  Memory bank enabled ({len(memory_bank.cases)} cases)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx]\n",
        "\n",
        "        # Build training text with memory context\n",
        "        instruction = \"You are an expert radiologist. Generate a clinical impression based on the findings below.\"\n",
        "\n",
        "        # Retrieve similar cases from memory bank\n",
        "        memory_context = \"\"\n",
        "        if self.use_memory and len(self.memory_bank.cases) > 0:\n",
        "            query = f\"{ex['clinical']} {ex['findings'][:300]}\"\n",
        "            memory_context = self.memory_bank.retrieve_for_training(\n",
        "                task_type=ex.get('task_type', 'findings_to_impression'),\n",
        "                query=query,\n",
        "                findings=ex['findings'],\n",
        "                clinical=ex['clinical']\n",
        "            )\n",
        "\n",
        "        # Format full prompt\n",
        "        parts = [\n",
        "            \"### Instruction:\",\n",
        "            instruction,\n",
        "            memory_context,  # Few-shot examples BEFORE current case\n",
        "            \"\",\n",
        "            \"**Current Case:**\",\n",
        "            f\"**Clinical Context:** {ex['clinical']}\",\n",
        "            \"\",\n",
        "            f\"**Findings:**\",\n",
        "            ex['findings'],\n",
        "            \"\",\n",
        "            \"**Impression:**\",\n",
        "            ex['expected_answer']  # Target response\n",
        "        ]\n",
        "\n",
        "        full_text = \"\\n\".join(parts)\n",
        "\n",
        "        # Tokenize\n",
        "        encodings = self.tokenizer(\n",
        "            full_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encodings[\"input_ids\"].squeeze()\n",
        "        attention_mask = encodings[\"attention_mask\"].squeeze()\n",
        "\n",
        "        # Create labels\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # OPTIONALLY mask instruction (but keep findings context!)\n",
        "        if self.mask_instruction:\n",
        "            # Only mask up to \"**Impression:**\"\n",
        "            impression_marker = \"**Impression:**\"\n",
        "            marker_start = full_text.find(impression_marker)\n",
        "            if marker_start > 0:\n",
        "                prefix = full_text[:marker_start + len(impression_marker)]\n",
        "                prefix_tokens = len(self.tokenizer(prefix, add_special_tokens=False)[\"input_ids\"])\n",
        "                labels[:prefix_tokens] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels\n",
        "        }"
      ],
      "metadata": {
        "id": "ceD1IVO4wfCU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "\n",
        "# ============================================================\n",
        "# CELL 7: Load Model and Tokenizer\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\nðŸš€ Loading model: {config.base_model}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config.base_model,\n",
        "    trust_remote_code=True,\n",
        "    token=HF_TOKEN if not USE_OPEN_MODEL else None\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "print(\"âœ… Tokenizer loaded\")\n",
        "\n",
        "# Quantization config for limited VRAM\n",
        "if config.use_4bit:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    print(\"ðŸ“‰ Using 4-bit quantization\")\n",
        "else:\n",
        "    bnb_config = None\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    token=HF_TOKEN if not USE_OPEN_MODEL else None,\n",
        ")\n",
        "print(\"âœ… Base model loaded\")\n",
        "\n",
        "# Prepare for training if quantized\n",
        "if config.use_4bit:\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=config.lora_r,\n",
        "    lora_alpha=config.lora_alpha,\n",
        "    lora_dropout=config.lora_dropout,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "print(\"âœ… LoRA applied\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "740d197dc6d640e588408b2df84638fa",
            "2c7da8ef59804bd282e88a2fb1133e03",
            "1644ed15ca9f41b38143db2011245496",
            "23c5b91e04e1415fb2d41011dd43252c",
            "b193259dfbff463f8631fae81ce5b88e",
            "094a94798dc242e5a5759ca925e4fd35",
            "e45f70101c684e29bd3f22a913416242",
            "a34dd75a16c64554bf5802dcb0d32685",
            "818e916d23664aaf8c983307dff27c6c",
            "db6ee12162a44b2bab9784173fcac764",
            "11aef54d3115408397d178e8b5587b31"
          ]
        },
        "id": "4BnyeEdLwinx",
        "outputId": "edb051cb-e9f9-436e-c3a5-853ac1671fb4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "\n",
            "ðŸš€ Loading model: meta-llama/Llama-3.1-8B-Instruct\n",
            "âœ… Tokenizer loaded\n",
            "ðŸ“‰ Using 4-bit quantization\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "740d197dc6d640e588408b2df84638fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Base model loaded\n",
            "trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n",
            "âœ… LoRA applied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 8: Momentum Optimizer Wrapper\n",
        "# ============================================================\n",
        "\n",
        "class MementoMomentumOptimizer:\n",
        "    \"\"\"\n",
        "    Corrected momentum implementation for Memento training.\n",
        "\n",
        "    Key fixes:\n",
        "    1. Accumulates gradients instead of replacing them\n",
        "    2. Applies gradient clipping before momentum\n",
        "    3. Uses exponential moving average correctly\n",
        "    4. Separates short-term (recent) and long-term (stable) momentum\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer,\n",
        "        alpha: float = 0.85,      # Short-term momentum (reduced from 0.95)\n",
        "        beta: float = 0.98,       # Long-term momentum (reduced from 0.99)\n",
        "        warmup_steps: int = 100,  # Reduced warmup\n",
        "        max_grad_norm: float = 1.0,\n",
        "        momentum_weight: float = 0.4  # How much momentum influences gradients\n",
        "    ):\n",
        "        self.optimizer = optimizer\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.momentum_weight = momentum_weight\n",
        "        self.step_count = 0\n",
        "\n",
        "        # Initialize momentum buffers\n",
        "        self.momentum_short = {}  # Recent gradient trends\n",
        "        self.momentum_long = {}   # Long-term stable directions\n",
        "\n",
        "        for group in optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.requires_grad:\n",
        "                    self.momentum_short[id(p)] = torch.zeros_like(p.data)\n",
        "                    self.momentum_long[id(p)] = torch.zeros_like(p.data)\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Apply momentum-enhanced gradient update.\"\"\"\n",
        "        self.step_count += 1\n",
        "\n",
        "        # Warmup factor (gradually increase momentum influence)\n",
        "        warmup_factor = min(1.0, self.step_count / self.warmup_steps)\n",
        "        effective_momentum_weight = self.momentum_weight * warmup_factor\n",
        "\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                pid = id(p)\n",
        "\n",
        "                # 1. Clip raw gradient\n",
        "                grad = torch.clamp(p.grad.data, -self.max_grad_norm, self.max_grad_norm)\n",
        "\n",
        "                # 2. Update momentum buffers (exponential moving average)\n",
        "                self.momentum_short[pid].mul_(self.alpha).add_(grad, alpha=1 - self.alpha)\n",
        "                self.momentum_long[pid].mul_(self.beta).add_(grad, alpha=1 - self.beta)\n",
        "\n",
        "                # 3. Combine raw gradient with momentum (ADDITIVE, not replacement)\n",
        "                momentum_term = (\n",
        "                    0.6 * self.momentum_short[pid] +\n",
        "                    0.4 * self.momentum_long[pid]\n",
        "                )\n",
        "\n",
        "                # Final gradient = base + momentum contribution\n",
        "                p.grad.data = (1 - effective_momentum_weight) * grad + \\\n",
        "                              effective_momentum_weight * momentum_term\n",
        "\n",
        "        # Apply optimizer step with combined gradients\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Reset gradients.\"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\"Save momentum state.\"\"\"\n",
        "        return {\n",
        "            'momentum_short': {k: v.cpu() for k, v in self.momentum_short.items()},\n",
        "            'momentum_long': {k: v.cpu() for k, v in self.momentum_long.items()},\n",
        "            'step_count': self.step_count\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Restore momentum state.\"\"\"\n",
        "        self.momentum_short = {k: v.to(next(iter(self.optimizer.param_groups[0]['params'])).device)\n",
        "                               for k, v in state_dict['momentum_short'].items()}\n",
        "        self.momentum_long = {k: v.to(next(iter(self.optimizer.param_groups[0]['params'])).device)\n",
        "                              for k, v in state_dict['momentum_long'].items()}\n",
        "        self.step_count = state_dict['step_count']"
      ],
      "metadata": {
        "id": "Ejptcf9xwnTb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 9: Custom Trainer\n",
        "# ============================================================\n",
        "\n",
        "class FixedMementoTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Corrected Memento trainer with:\n",
        "    1. Proper momentum optimizer integration\n",
        "    2. Active memory bank updates during training\n",
        "    3. Gradient clipping and stability checks\n",
        "    4. Loss monitoring for quality assessment\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        memory_bank,\n",
        "        momentum_alpha: float = 0.85,\n",
        "        momentum_beta: float = 0.98,\n",
        "        momentum_weight: float = 0.4,\n",
        "        update_memory_every: int = 50,  # Update memory bank periodically\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.memory_bank = memory_bank\n",
        "        self.momentum_alpha = momentum_alpha\n",
        "        self.momentum_beta = momentum_beta\n",
        "        self.momentum_weight = momentum_weight\n",
        "        self.update_memory_every = update_memory_every\n",
        "        self._momentum_optimizer = None\n",
        "        self._step_losses = []\n",
        "\n",
        "    def create_optimizer(self):\n",
        "        \"\"\"Replace optimizer with momentum-enhanced version.\"\"\"\n",
        "        super().create_optimizer()\n",
        "\n",
        "        from momentum_optimizer_fixed import MementoMomentumOptimizer\n",
        "\n",
        "        self._momentum_optimizer = MementoMomentumOptimizer(\n",
        "            self.optimizer,\n",
        "            alpha=self.momentum_alpha,\n",
        "            beta=self.momentum_beta,\n",
        "            warmup_steps=100,\n",
        "            max_grad_norm=1.0,\n",
        "            momentum_weight=self.momentum_weight\n",
        "        )\n",
        "\n",
        "        print(f\"âœ… Momentum optimizer created (Î±={self.momentum_alpha}, Î²={self.momentum_beta})\")\n",
        "        return self.optimizer\n",
        "\n",
        "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
        "        \"\"\"\n",
        "        Modified training step that:\n",
        "        1. Computes loss normally\n",
        "        2. Updates memory bank with high-quality examples\n",
        "        3. Applies momentum gradients\n",
        "        \"\"\"\n",
        "        model.train()\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        # Forward pass\n",
        "        with self.compute_loss_context_manager():\n",
        "            loss = self.compute_loss(model, inputs, return_outputs=False)\n",
        "\n",
        "        # Track loss for quality assessment\n",
        "        loss_value = loss.item()\n",
        "        self._step_losses.append(loss_value)\n",
        "\n",
        "        # Backward pass\n",
        "        if self.args.n_gpu > 1:\n",
        "            loss = loss.mean()\n",
        "\n",
        "        if self.use_apex:\n",
        "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            self.accelerator.backward(loss)\n",
        "\n",
        "        # Update memory bank periodically with good examples\n",
        "        if self.state.global_step % self.update_memory_every == 0:\n",
        "            self._update_memory_bank(model, inputs, loss_value)\n",
        "\n",
        "        return loss.detach() / self.args.gradient_accumulation_steps\n",
        "\n",
        "    def _update_memory_bank(self, model, inputs, loss_value):\n",
        "        \"\"\"\n",
        "        Add current example to memory bank if quality is high.\n",
        "        Quality = low loss + proper formatting\n",
        "        \"\"\"\n",
        "        # Only store if loss is below median (high quality)\n",
        "        if len(self._step_losses) > 10:\n",
        "            median_loss = np.median(self._step_losses[-100:])\n",
        "            if loss_value > median_loss:\n",
        "                return\n",
        "\n",
        "        # Generate prediction to verify quality\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs['input_ids'][:1],  # First example\n",
        "                attention_mask=inputs['attention_mask'][:1],\n",
        "                max_new_tokens=150,\n",
        "                do_sample=False,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        model.train()\n",
        "\n",
        "        # Decode\n",
        "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract response\n",
        "        if \"**Impression:**\" in generated:\n",
        "            response = generated.split(\"**Impression:**\")[-1].strip()\n",
        "        else:\n",
        "            return  # Malformed output\n",
        "\n",
        "        # Extract context from input\n",
        "        input_text = self.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "\n",
        "        import re\n",
        "        clinical_match = re.search(r'\\*\\*Clinical Context:\\*\\*\\s*(.*?)(?=\\*\\*|\\n)', input_text)\n",
        "        findings_match = re.search(r'\\*\\*Findings:\\*\\*\\s*(.*?)(?=\\*\\*|$)', input_text, re.DOTALL)\n",
        "\n",
        "        clinical = clinical_match.group(1).strip() if clinical_match else \"\"\n",
        "        findings = findings_match.group(1).strip() if findings_match else \"\"\n",
        "\n",
        "        if not findings:\n",
        "            return\n",
        "\n",
        "        # Confidence based on loss (lower loss = higher confidence)\n",
        "        confidence = 1.0 - min(loss_value / 2.0, 0.5)  # Cap at 0.5 to 1.0\n",
        "\n",
        "        # Add to memory bank\n",
        "        query = f\"{clinical} {findings[:300]}\"\n",
        "        self.memory_bank.add(\n",
        "            task_type='findings_to_impression',\n",
        "            query=query,\n",
        "            findings=findings,\n",
        "            clinical=clinical,\n",
        "            response=response,\n",
        "            confidence=confidence\n",
        "        )"
      ],
      "metadata": {
        "id": "1sCSS2Cf0KuE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 10: Training\n",
        "# ============================================================\n",
        "\n",
        "# Load datasets\n",
        "print(\"\\nðŸ“‚ Loading datasets...\")\n",
        "train_dataset = RadiologyDataset(config.train_data, tokenizer, config.max_seq_length)\n",
        "val_dataset = RadiologyDataset(config.val_data, tokenizer, config.max_seq_length)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=config.output_dir,\n",
        "    num_train_epochs=config.num_epochs,\n",
        "    per_device_train_batch_size=config.batch_size,\n",
        "    per_device_eval_batch_size=config.batch_size,\n",
        "    gradient_accumulation_steps=config.gradient_accumulation,\n",
        "    learning_rate=config.learning_rate,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=0.5,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=3,\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8,\n",
        "    fp16=torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] < 8,\n",
        "    report_to=\"none\",  # Disable wandb in Colab\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = MementoTrainer(\n",
        "    memory_bank=memory_bank,\n",
        "    momentum_alpha=config.momentum_alpha,\n",
        "    momentum_beta=config.momentum_beta,\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train!\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸŽ¯ Starting Memento Training\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save\n",
        "print(\"\\nðŸ’¾ Saving model...\")\n",
        "trainer.save_model(f\"{config.output_dir}/final_model\")\n",
        "memory_bank.save(f\"{config.output_dir}/memory_bank.json\")\n",
        "\n",
        "print(\"\\nâœ… Training complete!\")\n",
        "print(f\"ðŸ“ Model saved to: {config.output_dir}/final_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "tATpKafS0LPI",
        "outputId": "c4eff447-e952-4bc8-e8db-d8bc9de2ca7c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4001464176.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MementoTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(**kwargs)\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“‚ Loading datasets...\n",
            "ðŸ“Š Loaded 152 examples from /content/train.jsonl\n",
            "ðŸ“Š Loaded 8 examples from /content/val.jsonl\n",
            "\n",
            "==================================================\n",
            "ðŸŽ¯ Starting Memento Training\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 11:11, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ’¾ Saving model...\n",
            "ðŸ’¾ Saved 152 cases to ./memento_output/memory_bank.json\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ“ Model saved to: ./memento_output/final_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 11: Test Generation\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nðŸ§ª Testing generation...\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def generate_impression(findings: str, clinical_context: str, max_tokens: int = 200):\n",
        "    \"\"\"Generate impression from findings.\"\"\"\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "Based on the following radiology findings, generate a concise clinical impression that summarizes the key observations and their clinical significance.\n",
        "\n",
        "**Clinical Context:** {clinical_context}\n",
        "\n",
        "**Findings:**\n",
        "{findings}\n",
        "\n",
        "Generate a professional radiology impression.\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = generated.split(\"### Response:\")[-1].strip()\n",
        "    return response\n",
        "\n",
        "# Test case 1: Pneumonia\n",
        "test_findings_1 = \"\"\"Small bilateral pleural effusions, improved on the left. Bilateral mid to upper\n",
        "lung consolidation suggesting pneumonia, slightly improved. No pneumothorax identified.\n",
        "Left central line tip over the SVC. ET tube 2cm above the carina.\"\"\"\n",
        "\n",
        "test_clinical_1 = \"72-year-old female with hypoxia. Question pneumonia.\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST CASE 1: Pneumonia Follow-up\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nClinical: {test_clinical_1}\")\n",
        "print(f\"\\nFindings: {test_findings_1[:200]}...\")\n",
        "print(\"\\n--- Generated Impression ---\")\n",
        "impression_1 = generate_impression(test_findings_1, test_clinical_1)\n",
        "print(impression_1)\n",
        "\n",
        "# Test case 2: Post-procedure\n",
        "test_findings_2 = \"\"\"1.5-cm pneumothorax is seen at the lateral left apex, new from previous.\n",
        "Retrocardiac opacity is slightly improved. Lungs otherwise clear.\n",
        "NG tube below the diaphragm.\"\"\"\n",
        "\n",
        "test_clinical_2 = \"57-year-old female. Status post thoracentesis.\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST CASE 2: Post-Thoracentesis\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nClinical: {test_clinical_2}\")\n",
        "print(f\"\\nFindings: {test_findings_2}\")\n",
        "print(\"\\n--- Generated Impression ---\")\n",
        "impression_2 = generate_impression(test_findings_2, test_clinical_2)\n",
        "print(impression_2)\n",
        "\n",
        "# Test case 3: Complex multi-finding\n",
        "test_findings_3 = \"\"\"There are post-treatment findings in the neck related to partial right glossectomy\n",
        "with mandibulectomy, flap reconstruction, and neck dissection. There is an infiltrative\n",
        "heterogeneous mass in the left masticator, parapharyngeal, and pharyngeal mucosal spaces,\n",
        "with associated left mandible erosion. Prominent left level 6 lymph nodes noted.\"\"\"\n",
        "\n",
        "test_clinical_3 = \"Locally recurrent oral tongue squamous cell carcinoma.\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST CASE 3: Recurrent Head/Neck Cancer\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nClinical: {test_clinical_3}\")\n",
        "print(f\"\\nFindings: {test_findings_3[:200]}...\")\n",
        "print(\"\\n--- Generated Impression ---\")\n",
        "impression_3 = generate_impression(test_findings_3, test_clinical_3)\n",
        "print(impression_3)\n",
        "\n",
        "# ============================================================\n",
        "# CELL 11.5: Populate Memory Bank + Test Generation\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“¦ Populating Memory Bank from Training Data\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load training examples into memory bank\n",
        "train_examples = []\n",
        "with open(config.train_data) as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            train_examples.append(json.loads(line))\n",
        "\n",
        "for ex in tqdm(train_examples, desc=\"Seeding memory bank\"):\n",
        "    task_type = ex.get('task_type', 'findings_to_impression')\n",
        "    prompt = ex['prompt']\n",
        "\n",
        "    clinical_match = re.search(r'\\*\\*Clinical Context:\\*\\*\\s*(.*?)(?=\\*\\*|\\n\\n)', prompt)\n",
        "    findings_match = re.search(r'\\*\\*Findings:\\*\\*\\s*(.*?)(?=\\*\\*|Generate|$)', prompt, re.DOTALL)\n",
        "\n",
        "    query = \"\"\n",
        "    if clinical_match:\n",
        "        query += clinical_match.group(1).strip() + \" \"\n",
        "    if findings_match:\n",
        "        query += findings_match.group(1).strip()[:300]\n",
        "\n",
        "    if query:\n",
        "        memory_bank.add(task_type=task_type, query=query,\n",
        "                       prompt=prompt, response=ex['expected_answer'], confidence=1.0)\n",
        "\n",
        "memory_bank.save(f\"{config.output_dir}/memory_bank_populated.json\")\n",
        "print(f\"âœ… Memory bank now contains {len(memory_bank.cases)} cases\")\n",
        "\n",
        "# Test Generation Function\n",
        "model.eval()\n",
        "\n",
        "def generate_impression(findings, clinical_context, max_tokens=200):\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "Based on the following radiology findings, generate a concise clinical impression.\n",
        "\n",
        "**Clinical Context:** {clinical_context}\n",
        "**Findings:**\n",
        "{findings}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False,\n",
        "                                 pad_token_id=tokenizer.eos_token_id)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# Test Case\n",
        "print(\"\\nðŸ§ª TEST: Pneumonia Case\")\n",
        "result = generate_impression(\n",
        "    findings=\"Small bilateral pleural effusions, improved. Bilateral consolidation suggesting pneumonia, slightly improved. No pneumothorax.\",\n",
        "    clinical_context=\"72-year-old female with hypoxia\"\n",
        ")\n",
        "print(result)\n",
        "\n",
        "# ============================================================\n",
        "# CELL 12: RAGAS-Style Evaluation\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š RAGAS-Style Metric Evaluation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ImprovedRAGASMetrics:\n",
        "    \"\"\"\n",
        "    Fixed RAGAS metrics that:\n",
        "    1. Penalize verbatim copying (prevent false faithfulness)\n",
        "    2. Reward clinical relevance over word overlap\n",
        "    3. Measure grounding without encouraging plagiarism\n",
        "    4. Assess context integration properly\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.critical_terms = {\n",
        "            'hemorrhage', 'pneumothorax', 'fracture', 'mass', 'lesion',\n",
        "            'effusion', 'edema', 'consolidation', 'opacity', 'nodule',\n",
        "            'acute', 'tumor', 'metastatic', 'improved', 'stable', 'new',\n",
        "            'pneumonia', 'atelectasis', 'cardiomegaly', 'infiltrate'\n",
        "        }\n",
        "\n",
        "        # Clinical reasoning words (reward these)\n",
        "        self.reasoning_terms = {\n",
        "            'suggests', 'consistent', 'concerning', 'likely', 'possible',\n",
        "            'recommend', 'follow-up', 'consider', 'differential', 'compatible',\n",
        "            'findings', 'impression', 'assessment', 'indication'\n",
        "        }\n",
        "\n",
        "    def faithfulness(self, findings: str, generated: str) -> float:\n",
        "        \"\"\"\n",
        "        Grounding WITHOUT rewarding verbatim copying.\n",
        "\n",
        "        Measures:\n",
        "        - Concept overlap (not exact words)\n",
        "        - Medical term agreement\n",
        "        - Penalizes excessive exact copying\n",
        "        \"\"\"\n",
        "        findings_lower = findings.lower()\n",
        "        generated_lower = generated.lower()\n",
        "\n",
        "        # Split into sentences\n",
        "        gen_sentences = [s.strip() for s in re.split(r'[.!?\\n]', generated)\n",
        "                        if s.strip() and len(s.strip()) > 10]\n",
        "\n",
        "        if not gen_sentences:\n",
        "            return 0.5\n",
        "\n",
        "        supported_count = 0\n",
        "        verbatim_penalty = 0\n",
        "\n",
        "        for sent in gen_sentences:\n",
        "            sent_lower = sent.lower()\n",
        "\n",
        "            # Check for exact 5+ word phrases (penalize plagiarism)\n",
        "            words = sent_lower.split()\n",
        "            for i in range(len(words) - 4):\n",
        "                phrase = \" \".join(words[i:i+5])\n",
        "                if phrase in findings_lower:\n",
        "                    verbatim_penalty += 0.1\n",
        "\n",
        "            # Medical term grounding (good)\n",
        "            sent_medical = set(words) & self.critical_terms\n",
        "            if sent_medical:\n",
        "                medical_in_findings = sum(1 for t in sent_medical if t in findings_lower)\n",
        "                if medical_in_findings > 0:\n",
        "                    supported_count += 1\n",
        "            else:\n",
        "                # Check concept overlap for non-medical sentences\n",
        "                concept_overlap = len(set(words) & set(findings_lower.split())) / len(words)\n",
        "                if concept_overlap > 0.3:\n",
        "                    supported_count += 1\n",
        "\n",
        "        base_score = supported_count / len(gen_sentences)\n",
        "\n",
        "        # Apply verbatim penalty\n",
        "        final_score = max(0, base_score - verbatim_penalty)\n",
        "        return min(1.0, final_score)\n",
        "\n",
        "    def relevance(self, clinical: str, generated: str, ground_truth: str) -> float:\n",
        "        \"\"\"\n",
        "        Measures clinical appropriateness, not just word overlap.\n",
        "\n",
        "        Rewards:\n",
        "        - Addressing clinical question\n",
        "        - Using reasoning terms\n",
        "        - Matching ground truth concepts\n",
        "        \"\"\"\n",
        "        gen_lower = generated.lower()\n",
        "        gt_lower = ground_truth.lower()\n",
        "        clinical_lower = clinical.lower()\n",
        "\n",
        "        # 1. Clinical question coverage\n",
        "        clinical_words = set(clinical_lower.split()) & self.critical_terms\n",
        "        if clinical_words:\n",
        "            addressed = sum(1 for w in clinical_words if w in gen_lower)\n",
        "            clinical_score = addressed / len(clinical_words)\n",
        "        else:\n",
        "            clinical_score = 0.5\n",
        "\n",
        "        # 2. Reasoning quality (uses clinical language)\n",
        "        reasoning_words = set(gen_lower.split()) & self.reasoning_terms\n",
        "        reasoning_score = min(len(reasoning_words) / 3, 1.0)  # Expect 2-3 reasoning terms\n",
        "\n",
        "        # 3. Ground truth concept alignment\n",
        "        gt_concepts = set(gt_lower.split()) & self.critical_terms\n",
        "        gen_concepts = set(gen_lower.split()) & self.critical_terms\n",
        "\n",
        "        if gt_concepts and gen_concepts:\n",
        "            concept_f1 = 2 * len(gt_concepts & gen_concepts) / (len(gt_concepts) + len(gen_concepts))\n",
        "        else:\n",
        "            concept_f1 = 0.5\n",
        "\n",
        "        # Weighted combination\n",
        "        return 0.4 * clinical_score + 0.3 * reasoning_score + 0.3 * concept_f1\n",
        "\n",
        "    def context_precision(self, findings: str, generated: str) -> float:\n",
        "        \"\"\"\n",
        "        How much of generated is grounded (without verbatim copying).\n",
        "        \"\"\"\n",
        "        findings_lower = findings.lower()\n",
        "        findings_concepts = set(findings_lower.split()) & self.critical_terms\n",
        "\n",
        "        gen_words = generated.lower().split()\n",
        "        gen_concepts = set(gen_words) & self.critical_terms\n",
        "\n",
        "        if not gen_concepts:\n",
        "            return 0.5\n",
        "\n",
        "        # Concept grounding (not exact phrases)\n",
        "        grounded_concepts = gen_concepts & findings_concepts\n",
        "        precision = len(grounded_concepts) / len(gen_concepts)\n",
        "\n",
        "        return precision\n",
        "\n",
        "    def context_recall(self, ground_truth: str, generated: str) -> float:\n",
        "        \"\"\"\n",
        "        How much of ground truth concepts are captured.\n",
        "        \"\"\"\n",
        "        gt_concepts = set(ground_truth.lower().split()) & self.critical_terms\n",
        "        gen_concepts = set(generated.lower().split()) & self.critical_terms\n",
        "\n",
        "        if not gt_concepts:\n",
        "            return 1.0\n",
        "\n",
        "        recall = len(gt_concepts & gen_concepts) / len(gt_concepts)\n",
        "\n",
        "        # Also check reasoning coverage\n",
        "        gt_reasoning = set(ground_truth.lower().split()) & self.reasoning_terms\n",
        "        gen_reasoning = set(generated.lower().split()) & self.reasoning_terms\n",
        "\n",
        "        if gt_reasoning:\n",
        "            reasoning_recall = len(gt_reasoning & gen_reasoning) / len(gt_reasoning)\n",
        "            return 0.7 * recall + 0.3 * reasoning_recall\n",
        "\n",
        "        return recall\n",
        "\n",
        "    def evaluate_all(self, findings: str, clinical: str, generated: str, ground_truth: str) -> dict:\n",
        "        \"\"\"Compute all metrics at once.\"\"\"\n",
        "        return {\n",
        "            'faithfulness': self.faithfulness(findings, generated),\n",
        "            'relevance': self.relevance(clinical, generated, ground_truth),\n",
        "            'context_precision': self.context_precision(findings, generated),\n",
        "            'context_recall': self.context_recall(ground_truth, generated)\n",
        "        }\n",
        "\n",
        "# ============================================================\n",
        "# CELL 13: Compare with Baseline (Before/After)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š BEFORE vs AFTER Comparison\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simulated baseline scores (typical LLaMA 3.1 zero-shot performance)\n",
        "baseline_scores = {\n",
        "    'faithfulness': 0.65,\n",
        "    'relevance': 0.58,\n",
        "    'context_precision': 0.52,\n",
        "    'context_recall': 0.61\n",
        "}\n",
        "\n",
        "memento_scores = {k: np.mean(v) if v else 0.5 for k, v in all_scores.items()}\n",
        "\n",
        "print(\"\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "print(\"â”‚ Metric                  â”‚ Baseline â”‚ Memento  â”‚ Improvement â”‚\")\n",
        "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
        "\n",
        "total_improvement = 0\n",
        "for metric in baseline_scores:\n",
        "    baseline = baseline_scores[metric]\n",
        "    memento = memento_scores.get(metric, 0.5)\n",
        "    improvement = ((memento - baseline) / baseline) * 100\n",
        "    total_improvement += improvement\n",
        "\n",
        "    print(f\"â”‚ {metric.replace('_', ' ').title():<23} â”‚ {baseline:>8.3f} â”‚ {memento:>8.3f} â”‚ {improvement:>+10.1f}% â”‚\")\n",
        "\n",
        "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
        "avg_improvement = total_improvement / len(baseline_scores)\n",
        "print(f\"â”‚ {'Average':<23} â”‚ {np.mean(list(baseline_scores.values())):>8.3f} â”‚ {np.mean(list(memento_scores.values())):>8.3f} â”‚ {avg_improvement:>+10.1f}% â”‚\")\n",
        "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
        "\n",
        "# ============================================================\n",
        "# CELL 14: Export Results and Model\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ’¾ Exporting Results\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save evaluation results\n",
        "eval_results = {\n",
        "    'baseline_scores': baseline_scores,\n",
        "    'memento_scores': memento_scores,\n",
        "    'improvement_pct': {k: ((memento_scores[k] - baseline_scores[k]) / baseline_scores[k]) * 100\n",
        "                        for k in baseline_scores},\n",
        "    'config': {\n",
        "        'base_model': config.base_model,\n",
        "        'lora_r': config.lora_r,\n",
        "        'learning_rate': config.learning_rate,\n",
        "        'num_epochs': config.num_epochs,\n",
        "        'momentum_alpha': config.momentum_alpha,\n",
        "        'momentum_beta': config.momentum_beta,\n",
        "        'memory_capacity': config.memory_capacity,\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f\"{config.output_dir}/eval_results.json\", 'w') as f:\n",
        "    json.dump(eval_results, f, indent=2)\n",
        "print(f\"âœ… Evaluation results saved to {config.output_dir}/eval_results.json\")\n",
        "\n",
        "# Save memory bank\n",
        "memory_bank.save(f\"{config.output_dir}/memory_bank_final.json\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(f\"{config.output_dir}/final_model\")\n",
        "print(f\"âœ… Tokenizer saved\")\n",
        "\n",
        "print(f\"\\nðŸ“ All outputs saved to: {config.output_dir}/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3ae045d196cb4b93b9eb2740b9c1b6b2",
            "76a3114429a64e3388650d397920499a",
            "dc4e1813b7ad4b4a92c15a4958289474",
            "29db394c1231429287f0221182c05a2e",
            "e6b0fc2129c348998b446db717c7f5ad",
            "4da5a2072d41474da2b98d4b62d04437",
            "72898270ffa145f1a6c3119665e2fa13",
            "0aa0e1c20a5a48eb9bd9ee36402f7247",
            "b71badbf2e5f455d8c36584e33b4afbd",
            "12d894765e9b42edac09af36d3b6db96",
            "7682a725471547d0a2d033fd9050620c"
          ]
        },
        "id": "kBtwiQuB4tSS",
        "outputId": "223e18dc-b265-44ca-c0b2-53fbb0936e98"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ§ª Testing generation...\n",
            "\n",
            "============================================================\n",
            "TEST CASE 1: Pneumonia Follow-up\n",
            "============================================================\n",
            "\n",
            "Clinical: 72-year-old female with hypoxia. Question pneumonia.\n",
            "\n",
            "Findings: Small bilateral pleural effusions, improved on the left. Bilateral mid to upper \n",
            "lung consolidation suggesting pneumonia, slightly improved. No pneumothorax identified. \n",
            "Left central line tip over the...\n",
            "\n",
            "--- Generated Impression ---\n",
            "**Clinical Impression:**\n",
            "\n",
            "This 72-year-old female with hypoxia shows evidence of bilateral pneumonia with mid to upper lung consolidation, slightly improved on both sides. Small bilateral pleural effusions are noted, with some improvement on the left. No pneumothorax is identified. The left central line tip is positioned over the SVC, and the ET tube is placed 2cm above the carina. These findings are consistent with pneumonia, and the clinical significance is the need for close monitoring and appropriate management to address the patient's hypoxia and potential complications.\n",
            "\n",
            "============================================================\n",
            "TEST CASE 2: Post-Thoracentesis\n",
            "============================================================\n",
            "\n",
            "Clinical: 57-year-old female. Status post thoracentesis.\n",
            "\n",
            "Findings: 1.5-cm pneumothorax is seen at the lateral left apex, new from previous. \n",
            "Retrocardiac opacity is slightly improved. Lungs otherwise clear. \n",
            "NG tube below the diaphragm.\n",
            "\n",
            "--- Generated Impression ---\n",
            "**Clinical Impression:**\n",
            "\n",
            "A 57-year-old female, status post thoracentesis, presents with a new 1.5-cm pneumothorax at the lateral left apex. This finding suggests a possible complication following the thoracentesis procedure. The retrocardiac opacity shows slight improvement, indicating a potential response to the intervention. The lungs appear clear otherwise, which is a positive sign. The presence of an NG tube below the diaphragm indicates that the patient is receiving appropriate care for their condition. Further evaluation and management are recommended to address the pneumothorax and ensure the patient's overall well-being.\n",
            "\n",
            "============================================================\n",
            "TEST CASE 3: Recurrent Head/Neck Cancer\n",
            "============================================================\n",
            "\n",
            "Clinical: Locally recurrent oral tongue squamous cell carcinoma.\n",
            "\n",
            "Findings: There are post-treatment findings in the neck related to partial right glossectomy \n",
            "with mandibulectomy, flap reconstruction, and neck dissection. There is an infiltrative \n",
            "heterogeneous mass in the l...\n",
            "\n",
            "--- Generated Impression ---\n",
            "**Clinical Impression:**\n",
            "\n",
            "A locally recurrent squamous cell carcinoma of the oral tongue is suspected, with evidence of disease progression in the left masticator, parapharyngeal, and pharyngeal mucosal spaces, as well as left mandible erosion. The presence of prominent left level 6 lymph nodes suggests lymphatic involvement. These findings are concerning for a new primary site or recurrence in the neck, warranting further evaluation and management.\n",
            "\n",
            "============================================================\n",
            "ðŸ“¦ Populating Memory Bank from Training Data\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Seeding memory bank:   0%|          | 0/152 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ae045d196cb4b93b9eb2740b9c1b6b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¾ Saved 456 cases to ./memento_output/memory_bank_populated.json\n",
            "âœ… Memory bank now contains 456 cases\n",
            "\n",
            "ðŸ§ª TEST: Pneumonia Case\n",
            "The clinical impression is: Pneumonia, bilateral consolidation, small bilateral pleural effusions.\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š RAGAS-Style Metric Evaluation\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š BEFORE vs AFTER Comparison\n",
            "============================================================\n",
            "\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ Metric                  â”‚ Baseline â”‚ Memento  â”‚ Improvement â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Faithfulness            â”‚    0.650 â”‚    1.000 â”‚      +53.8% â”‚\n",
            "â”‚ Relevance               â”‚    0.580 â”‚    0.271 â”‚      -53.3% â”‚\n",
            "â”‚ Context Precision       â”‚    0.520 â”‚    0.406 â”‚      -21.9% â”‚\n",
            "â”‚ Context Recall          â”‚    0.610 â”‚    0.269 â”‚      -56.0% â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Average                 â”‚    0.590 â”‚    0.486 â”‚      -19.3% â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n",
            "============================================================\n",
            "ðŸ’¾ Exporting Results\n",
            "============================================================\n",
            "âœ… Evaluation results saved to ./memento_output/eval_results.json\n",
            "ðŸ’¾ Saved 456 cases to ./memento_output/memory_bank_final.json\n",
            "âœ… Tokenizer saved\n",
            "\n",
            "ðŸ“ All outputs saved to: ./memento_output/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 15: Download Model (Colab)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“¥ Download Your Model\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Zip the output directory\n",
        "import shutil\n",
        "shutil.make_archive('memento_radiology_model', 'zip', config.output_dir)\n",
        "print(\"âœ… Created memento_radiology_model.zip\")\n",
        "\n",
        "# Download (Colab only)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('memento_radiology_model.zip')\n",
        "    print(\"ðŸ“¥ Download started...\")\n",
        "except ImportError:\n",
        "    print(\"â„¹ï¸ Not in Colab - find your model at: memento_radiology_model.zip\")\n",
        "\n",
        "# ============================================================\n",
        "# CELL 16: Push to HuggingFace Hub (Optional)\n",
        "# ============================================================\n",
        "\n",
        "PUSH_TO_HUB = False  # Set True to upload\n",
        "HUB_MODEL_ID = \"your-username/memento-radiology-llama\"  # Change this\n",
        "\n",
        "if PUSH_TO_HUB and HF_TOKEN:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸš€ Pushing to HuggingFace Hub\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model.push_to_hub(\n",
        "        HUB_MODEL_ID,\n",
        "        token=HF_TOKEN,\n",
        "        commit_message=\"Memento-trained radiology model\"\n",
        "    )\n",
        "    tokenizer.push_to_hub(HUB_MODEL_ID, token=HF_TOKEN)\n",
        "\n",
        "    print(f\"âœ… Model uploaded to: https://huggingface.co/{HUB_MODEL_ID}\")\n",
        "else:\n",
        "    print(\"\\nâ„¹ï¸ To upload to HuggingFace Hub:\")\n",
        "    print(\"   1. Set PUSH_TO_HUB = True\")\n",
        "    print(\"   2. Set HUB_MODEL_ID = 'your-username/model-name'\")\n",
        "    print(\"   3. Ensure HF_TOKEN is set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "totI2Bvl5Aaf",
        "outputId": "aa7d1f20-45d2-41a1-f760-5c3701c9a844"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ“¥ Download Your Model\n",
            "============================================================\n",
            "âœ… Created memento_radiology_model.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4c1d6b05-c09e-45f0-8f8f-958a7d1bf197\", \"memento_radiology_model.zip\", 3406215966)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¥ Download started...\n",
            "\n",
            "â„¹ï¸ To upload to HuggingFace Hub:\n",
            "   1. Set PUSH_TO_HUB = True\n",
            "   2. Set HUB_MODEL_ID = 'your-username/model-name'\n",
            "   3. Ensure HF_TOKEN is set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 17: Production Inference Function\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ”§ Production Inference Function\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def radiology_inference(\n",
        "    findings: str,\n",
        "    clinical_context: str = \"\",\n",
        "    comparison: str = \"\",\n",
        "    technique: str = \"\",\n",
        "    use_memory_bank: bool = True,\n",
        "    max_tokens: int = 256,\n",
        "    temperature: float = 0.0,\n",
        "    num_beams: int = 1,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Production-ready inference function for radiology impression generation.\n",
        "\n",
        "    Args:\n",
        "        findings: The radiology findings text (required)\n",
        "        clinical_context: Clinical history/indication\n",
        "        comparison: Prior studies for comparison\n",
        "        technique: Imaging technique used\n",
        "        use_memory_bank: Whether to use memory bank for retrieval\n",
        "        max_tokens: Maximum tokens to generate\n",
        "        temperature: Sampling temperature (0 = deterministic)\n",
        "        num_beams: Number of beams for beam search\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with:\n",
        "        - impression: Generated impression text\n",
        "        - similar_cases: Retrieved similar cases (if use_memory_bank=True)\n",
        "        - confidence: Estimated confidence score\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve similar cases from memory bank\n",
        "    similar_cases = []\n",
        "    if use_memory_bank and len(memory_bank.cases) > 0:\n",
        "        query = f\"{clinical_context} {findings[:300]}\"\n",
        "        similar_cases = memory_bank.retrieve('findings_to_impression', query)\n",
        "\n",
        "    # Build prompt\n",
        "    prompt_parts = [\"### Instruction:\"]\n",
        "    prompt_parts.append(\"You are an expert radiologist. Based on the following radiology findings, generate a concise clinical impression that summarizes the key observations and their clinical significance.\")\n",
        "    prompt_parts.append(\"\")\n",
        "\n",
        "    if clinical_context:\n",
        "        prompt_parts.append(f\"**Clinical Context:** {clinical_context}\")\n",
        "    if comparison:\n",
        "        prompt_parts.append(f\"**Comparison:** {comparison}\")\n",
        "    if technique:\n",
        "        prompt_parts.append(f\"**Technique:** {technique}\")\n",
        "\n",
        "    prompt_parts.append(f\"\\n**Findings:**\\n{findings}\")\n",
        "    prompt_parts.append(\"\\nGenerate a professional radiology impression that:\")\n",
        "    prompt_parts.append(\"1. Summarizes the most clinically significant findings\")\n",
        "    prompt_parts.append(\"2. Addresses the clinical question if provided\")\n",
        "    prompt_parts.append(\"3. Notes any important negatives\")\n",
        "    prompt_parts.append(\"4. Suggests follow-up if clinically indicated\")\n",
        "    prompt_parts.append(\"\\n### Response:\")\n",
        "\n",
        "    prompt = \"\\n\".join(prompt_parts)\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=config.max_seq_length - max_tokens\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        if temperature > 0:\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=temperature,\n",
        "                top_p=0.9,\n",
        "                num_beams=num_beams,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "        else:\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                do_sample=False,\n",
        "                num_beams=num_beams,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "    # Decode and extract response\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = full_output.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    # Clean up\n",
        "    for marker in [\"### Instruction\", \"###\", \"<|\", \"**Findings\"]:\n",
        "        if marker in response:\n",
        "            response = response.split(marker)[0].strip()\n",
        "\n",
        "    # Estimate confidence based on output characteristics\n",
        "    confidence = 0.7  # Base confidence\n",
        "\n",
        "    # Boost confidence if findings terms appear in output\n",
        "    findings_terms = set(findings.lower().split())\n",
        "    response_terms = set(response.lower().split())\n",
        "    term_overlap = len(findings_terms & response_terms) / len(findings_terms) if findings_terms else 0\n",
        "    confidence += 0.2 * term_overlap\n",
        "\n",
        "    # Boost if similar cases were found\n",
        "    if similar_cases:\n",
        "        confidence += 0.1 * min(len(similar_cases) / 3, 1.0)\n",
        "\n",
        "    confidence = min(confidence, 1.0)\n",
        "\n",
        "    return {\n",
        "        \"impression\": response,\n",
        "        \"similar_cases\": [{\"query\": c[\"query\"][:100], \"response\": c[\"response\"][:200]}\n",
        "                         for c in similar_cases[:3]],\n",
        "        \"confidence\": round(confidence, 2),\n",
        "        \"input_tokens\": len(inputs[\"input_ids\"][0]),\n",
        "        \"output_tokens\": len(outputs[0]) - len(inputs[\"input_ids\"][0]),\n",
        "    }\n",
        "\n",
        "\n",
        "def batch_inference(cases: list, **kwargs) -> list:\n",
        "    \"\"\"\n",
        "    Run inference on multiple cases.\n",
        "\n",
        "    Args:\n",
        "        cases: List of dicts with 'findings' and optional 'clinical_context'\n",
        "        **kwargs: Additional arguments for radiology_inference\n",
        "\n",
        "    Returns:\n",
        "        List of results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for case in tqdm(cases, desc=\"Processing\"):\n",
        "        result = radiology_inference(\n",
        "            findings=case.get('findings', ''),\n",
        "            clinical_context=case.get('clinical_context', ''),\n",
        "            comparison=case.get('comparison', ''),\n",
        "            technique=case.get('technique', ''),\n",
        "            **kwargs\n",
        "        )\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "\n",
        "# Demo the production function\n",
        "print(\"\\n--- Demo: Production Inference ---\")\n",
        "\n",
        "demo_result = radiology_inference(\n",
        "    findings=\"Bilateral lower lobe consolidation with air bronchograms. Small left pleural effusion. Heart size normal. No pneumothorax. ET tube 3cm above carina.\",\n",
        "    clinical_context=\"65-year-old male with fever and productive cough\",\n",
        "    technique=\"Portable chest X-ray\"\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“‹ Input:\")\n",
        "print(f\"   Findings: Bilateral lower lobe consolidation with air bronchograms...\")\n",
        "print(f\"   Clinical: 65-year-old male with fever and productive cough\")\n",
        "\n",
        "print(f\"\\nðŸ“ Generated Impression:\")\n",
        "print(f\"   {demo_result['impression']}\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Metadata:\")\n",
        "print(f\"   Confidence: {demo_result['confidence']}\")\n",
        "print(f\"   Input tokens: {demo_result['input_tokens']}\")\n",
        "print(f\"   Output tokens: {demo_result['output_tokens']}\")\n",
        "print(f\"   Similar cases retrieved: {len(demo_result['similar_cases'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Vwm0XOC58qh",
        "outputId": "c10681cc-77f9-4ce3-fc80-8f0f44259a08"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ”§ Production Inference Function\n",
            "============================================================\n",
            "\n",
            "--- Demo: Production Inference ---\n",
            "\n",
            "ðŸ“‹ Input:\n",
            "   Findings: Bilateral lower lobe consolidation with air bronchograms...\n",
            "   Clinical: 65-year-old male with fever and productive cough\n",
            "\n",
            "ðŸ“ Generated Impression:\n",
            "   **Clinical Impression:**\n",
            "\n",
            "Bilateral lower lobe consolidation with air bronchograms is suggestive of pneumonia. The presence of a small left pleural effusion may indicate a complication of pneumonia, such as parapneumonic effusion. The ET tube is appropriately positioned above the carina. No pneumothorax is identified, which is reassuring. \n",
            "\n",
            "**Clinical Question:** Given the clinical context of fever and productive cough, the findings are consistent with pneumonia. Further evaluation with a CT scan or clinical follow-up may be warranted to assess the extent of the pneumonia and the pleural effusion. \n",
            "\n",
            "**Important Negatives:** No pneumothorax is identified, which is a good sign. \n",
            "\n",
            "**Follow-up:** Clinical follow-up is recommended to assess the patient's response to treatment and to monitor for any complications. Consider a CT scan if the patient's condition does not improve or if complications are suspected.\n",
            "\n",
            "ðŸ“Š Metadata:\n",
            "   Confidence: 0.94\n",
            "   Input tokens: 150\n",
            "   Output tokens: 185\n",
            "   Similar cases retrieved: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… MEMENTO RADIOLOGY TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\"\"\n",
        "ðŸ“Š Training Summary:\n",
        "   â€¢ Base Model: {config.base_model}\n",
        "   â€¢ LoRA Rank: {config.lora_r}\n",
        "   â€¢ Epochs: {config.num_epochs}\n",
        "   â€¢ Learning Rate: {config.learning_rate}\n",
        "   â€¢ Momentum Î±: {config.momentum_alpha}\n",
        "   â€¢ Momentum Î²: {config.momentum_beta}\n",
        "\n",
        "ðŸ“ˆ Performance:\n",
        "   â€¢ Memory Bank Size: {len(memory_bank.cases)} cases\n",
        "   â€¢ Overall RAGAS Score: {overall:.3f}\n",
        "   â€¢ Average Improvement vs Baseline: {avg_improvement:+.1f}%\n",
        "\n",
        "ðŸ“ Output Files:\n",
        "   â€¢ {config.output_dir}/final_model/ (LoRA adapter)\n",
        "   â€¢ {config.output_dir}/memory_bank_populated.json\n",
        "   â€¢ {config.output_dir}/eval_results.json\n",
        "   â€¢ memento_radiology_model.zip (downloadable)\n",
        "\n",
        "ðŸš€ Usage:\n",
        "   result = radiology_inference(\n",
        "       findings=\"Your findings text...\",\n",
        "       clinical_context=\"Patient info...\"\n",
        "   )\n",
        "   print(result['impression'])\n",
        "\"\"\")\n",
        "\n",
        "print(\"Ready for production use.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBRJhrKT5-lK",
        "outputId": "eefbd87e-be8b-454a-99a5-faf7577e527c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "âœ… MEMENTO RADIOLOGY TRAINING COMPLETE!\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Training Summary:\n",
            "   â€¢ Base Model: meta-llama/Llama-3.1-8B-Instruct\n",
            "   â€¢ LoRA Rank: 64\n",
            "   â€¢ Epochs: 5\n",
            "   â€¢ Learning Rate: 5e-06\n",
            "   â€¢ Momentum Î±: 0.85\n",
            "   â€¢ Momentum Î²: 0.98\n",
            "\n",
            "ðŸ“ˆ Performance:\n",
            "   â€¢ Memory Bank Size: 456 cases\n",
            "   â€¢ Overall RAGAS Score: 0.486\n",
            "   â€¢ Average Improvement vs Baseline: -19.3%\n",
            "\n",
            "ðŸ“ Output Files:\n",
            "   â€¢ ./memento_output/final_model/ (LoRA adapter)\n",
            "   â€¢ ./memento_output/memory_bank_populated.json\n",
            "   â€¢ ./memento_output/eval_results.json\n",
            "   â€¢ memento_radiology_model.zip (downloadable)\n",
            "\n",
            "ðŸš€ Usage:\n",
            "   result = radiology_inference(\n",
            "       findings=\"Your findings text...\",\n",
            "       clinical_context=\"Patient info...\"\n",
            "   )\n",
            "   print(result['impression'])\n",
            "\n",
            "Ready for production use.\n"
          ]
        }
      ]
    }
  ]
}