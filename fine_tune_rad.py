# -*- coding: utf-8 -*-
"""fine tune rad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ny6V0xJKXWZwSVFbHdsMfi5iqLM0MMs2

# **Fine-tuning for Llama-medx_v3.2**
"""

!pip install -q transformers datasets peft trl bitsandbytes accelerate

import json
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
from trl import SFTTrainer
import os

print("Packages installed.")
print(f"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}")

# To upload the CoT training file.
from google.colab import files

def upload_training_file():
    print("Upload the training JSON file:")
    uploaded = files.upload()

    filename = list(uploaded.keys())[0]
    print(f"File uploaded: {filename}")
    return filename

# Data processing.
def process_data(filename, tokenizer=None):
    print(f"Processing {filename}.")

    with open(filename, 'r') as f:
        data = json.load(f)

    print(f"Loaded {len(data)} samples.")

    # Format.
    def format_prompt(sample):
        instruction = sample.get('instruction', '')
        output = sample.get('output', '')

        text = f"### Human: {instruction}\n### Assistant: {output}"
        return {"text": text}

    formatted_data = [format_prompt(sample) for sample in data]

    # Splitting into train/eval (90/10).
    split_idx = int(len(formatted_data) * 0.9)
    train_data = formatted_data[:split_idx]
    eval_data = formatted_data[split_idx:]

    print(f"Split: {len(train_data)} train, {len(eval_data)} eval.")

    train_dataset = Dataset.from_list(train_data)
    eval_dataset = Dataset.from_list(eval_data)

    if tokenizer is not None:
        print("Tokenizing data.")

        def tokenize_function(examples):
            # Tokenizing the text.
            tokenized = tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=512,
                return_tensors=None
            )

            tokenized["labels"] = tokenized["input_ids"].copy()

            return tokenized

        train_dataset = train_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )

        eval_dataset = eval_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )

        print("Data tokenized.")

    return train_dataset, eval_dataset

# Loading the model and the tokenizer.
def load_model():
    print("Loading Llama-medx model.")

    model_name = "skumar9/Llama-medx_v3.2"

    # Tokenizer.
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Quantization config.
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

    # Loading the model.
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    print("Model and tokenizer loaded.")
    return model, tokenizer

# LoRA setup.
def setup_lora(model):
    print("Setting up LoRA.")

    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    print("LoRA setup done.")
    return model

# Training.
def train_model(model, tokenizer, train_dataset, eval_dataset):
    print("Starting training.")

    # Training arguments.
    training_args = TrainingArguments(
        output_dir="./llama-medx-finetuned",
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=4,
        num_train_epochs=3,
        learning_rate=2e-5,
        fp16=True,
        logging_steps=10,
        eval_steps=50,
        save_steps=100,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        report_to="none",
        remove_unused_columns=False,
    )

    # Standard Trainer with data collator.
    from transformers import DataCollatorForLanguageModeling

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )


    trainer.processing_class = tokenizer

    print("Using standard Trainer with tokenization.")

    # Train.
    trainer.train()

    # Saving.
    trainer.save_model("./llama-medx-finetuned")
    tokenizer.save_pretrained("./llama-medx-finetuned")

    print("Training complete. Model saved to ./llama-medx-finetuned")
    return trainer

# Testing.
def test_model():
    print("Testing the model.")

    # Loading the fine-tuned model.
    tokenizer = AutoTokenizer.from_pretrained("./llama-medx-finetuned")
    model = AutoModelForCausalLM.from_pretrained("./llama-medx-finetuned")

    # Test prompts.
    test_prompts = [
        "### Human: What are the symptoms of diabetes?\n### Assistant:",
        "### Human: How is hypertension treated?\n### Assistant:",
    ]

    for prompt in test_prompts:
        inputs = tokenizer(prompt, return_tensors="pt")

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=inputs.input_ids.shape[1] + 100,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated = response[len(prompt):].strip()

        print(f"Q: {prompt.split('### Human: ')[1].split('### Assistant:')[0]}")
        print(f"A: {generated}")
        print("-" * 50)

def run_fine_tuning():

    try:
        # Uploading the file.
        filename = upload_training_file()

        # Loading the model.
        model, tokenizer = load_model()

        # Processing the data with tokenizer.
        train_dataset, eval_dataset = process_data(filename, tokenizer)

        # Setting up LoRA.
        model = setup_lora(model)

        # Training.
        trainer = train_model(model, tokenizer, train_dataset, eval_dataset)

        # Test.
        test_model()

        print("Your model is fine-tuned and tested.")
        return trainer

    except Exception as e:
        print(f"X Error: {e}")
        print("Please check your data format.")
        import traceback
        traceback.print_exc()

# Saving the model to Google Drive.
def save_to_google_drive():
    print("Saving the fine-tunned model to Google Drive.")

    from google.colab import drive
    drive.mount('/content/drive')

    import shutil
    import os

    # Source directory.
    source_dir = "./llama-medx-finetuned"

    # Destination directory.
    drive_dir = "/content/drive/MyDrive/llama-medx-finetuned"

    try:
        if os.path.exists(source_dir):
            shutil.copytree(source_dir, drive_dir, dirs_exist_ok=True)
            print(f"Model saved to Google Drive: {drive_dir}")

            files = os.listdir(drive_dir)
            print(f"Saved files: {files}")

            return drive_dir
        else:
            print("Model directory not found.")
            return None

    except Exception as e:
        print(f"X Error saving to Drive: {e}")
        return None

# Loading the fine-tuned model from Google Drive.
def load_from_google_drive(drive_path=None):
    print("Loading the fine-tuned model from Google Drive.")

    from google.colab import drive
    drive.mount('/content/drive')


    if drive_path is None:
        drive_path = "/content/drive/MyDrive/llama-medx-finetuned"

    try:
        if os.path.exists(drive_path):
            tokenizer = AutoTokenizer.from_pretrained(drive_path)
            model = AutoModelForCausalLM.from_pretrained(drive_path)

            print(f"Model loaded from: {drive_path}")
            return model, tokenizer
        else:
            print(f"X Model not found at: {drive_path}")
            return None, None

    except Exception as e:
        print(f"X Error loading from Drive: {e}")
        return None, None

# Testing the model from Drive.
def test_model_from_drive(drive_path=None):
    print("Testing model from Google Drive.")

    model, tokenizer = load_from_google_drive(drive_path)

    if model is None or tokenizer is None:
        print("X Could not load model from Drive.")
        return

    # Test prompts.
    test_prompts = [
        "### Human: What are the symptoms of diabetes?\n### Assistant:",
        "### Human: How is hypertension treated?\n### Assistant:",
        "### Human: What causes chest pain?\n### Assistant:",
    ]


    print("Testing the model from Google Drive.")
    print("-"*50)

    for i, prompt in enumerate(test_prompts, 1):
        question = prompt.split("### Human: ")[1].split("\n### Assistant:")[0]
        print(f"\n Test {i}: {question}")
        print("-" * 40)

        try:
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=inputs.input_ids.shape[1] + 100,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )

            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated = response[len(prompt):].strip()

            print(f"Answer: {generated}")

        except Exception as e:
            print(f"X Error: {e}")

    print("\n Testing complete.")

# Downloading the model as a ZIP.
def download_model_as_zip():
    print("Creating the ZIP file.")

    import zipfile
    import os

    source_dir = "./llama-medx-finetuned"
    zip_filename = "llama-medx-finetuned.zip"

    try:
        if os.path.exists(source_dir):
            with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(source_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, source_dir)
                        zipf.write(file_path, arcname)

            print(f"ZIP file created: {zip_filename}")

            from google.colab import files
            files.download(zip_filename)

            print("Download started.")

        else:
            print("X Model directory not found.")

    except Exception as e:
        print(f"X Error creating ZIP: {e}")

# Example of the expected JSON format,
print("\n Expected JSON format:")
print("""
[
    {
        "instruction": "What are the symptoms of diabetes?",
        "output": "Common symptoms include increased thirst, frequent urination, fatigue..."
    },
    {
        "instruction": "How is hypertension diagnosed?",
        "output": "Hypertension is diagnosed through blood pressure measurements..."
    }
]
""")

print("\n Run this command to start:")
print("trainer = run_fine_tuning()")

trainer = run_fine_tuning()

save_to_google_drive()
download_model_as_zip()