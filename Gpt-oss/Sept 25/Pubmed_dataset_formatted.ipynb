{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1xApJOpy45k",
        "outputId": "312ffad0-5918-4d1b-fd12-79e6fd779ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ori_pqal.json...\n",
            "✓ ori_pqal.json downloaded\n",
            "Downloading ori_pqaa.json...\n",
            "✓ ori_pqaa.json downloaded\n",
            "Downloading ori_pqau.json...\n",
            "✓ ori_pqau.json downloaded\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "os.makedirs('pubmedqa', exist_ok=True)\n",
        "\n",
        "files = {\n",
        "    'ori_pqal.json': 'https://github.com/pubmedqa/pubmedqa/raw/master/data/ori_pqal.json',\n",
        "    'ori_pqaa.json': 'https://github.com/pubmedqa/pubmedqa/raw/master/data/ori_pqaa.json',\n",
        "    'ori_pqau.json': 'https://github.com/pubmedqa/pubmedqa/raw/master/data/ori_pqau.json'\n",
        "}\n",
        "\n",
        "for filename, url in files.items():\n",
        "    print(f\"Downloading {filename}...\")\n",
        "    response = requests.get(url)\n",
        "    with open(f'pubmedqa/{filename}', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"✓ {filename} downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json('pubmedqa/ori_pqal.json')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "9nHG32RvzdZI",
        "outputId": "f7085720-dceb-45ae-8265-b3c95d47ba29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1624980746.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pubmedqa/ori_pqal.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "import logging\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return str(text)\n",
        "\n",
        "    text = ' '.join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "def format_context(contexts: list) -> str:\n",
        "    if not contexts:\n",
        "        return \"No context provided.\"\n",
        "\n",
        "    formatted_contexts = []\n",
        "    for i, context in enumerate(contexts, 1):\n",
        "        clean_context = clean_text(context)\n",
        "        if clean_context:\n",
        "            formatted_contexts.append(f\"[{i}] {clean_context}\")\n",
        "\n",
        "    return \"\\n\\n\".join(formatted_contexts)\n",
        "\n",
        "def convert_answer(answer: str) -> str:\n",
        "    answer = answer.lower().strip()\n",
        "\n",
        "\n",
        "    answer_mapping = {\n",
        "        'yes': 'Yes',\n",
        "        'no': 'No',\n",
        "        'maybe': 'Maybe/Uncertain'\n",
        "    }\n",
        "\n",
        "    return answer_mapping.get(answer, 'Maybe/Uncertain')\n",
        "\n",
        "def create_reasoning(question: str, contexts: list, answer: str, long_answer: str = None) -> str:\n",
        "    reasoning_parts = []\n",
        "\n",
        "    if long_answer and long_answer.strip():\n",
        "        reasoning_parts.append(clean_text(long_answer))\n",
        "    else:\n",
        "        if answer.lower() == 'yes':\n",
        "            reasoning_parts.append(\"The provided research context supports an affirmative answer to the question.\")\n",
        "        elif answer.lower() == 'no':\n",
        "            reasoning_parts.append(\"The provided research context does not support the proposed relationship or claim.\")\n",
        "        else:\n",
        "            reasoning_parts.append(\"The available evidence is insufficient or conflicting to provide a definitive answer.\")\n",
        "\n",
        "\n",
        "    if contexts and len(contexts) > 0:\n",
        "        reasoning_parts.append(f\"This conclusion is based on analysis of {len(contexts)} research context provided.\")\n",
        "\n",
        "    return \" \".join(reasoning_parts)\n",
        "\n",
        "def process_pubmedqa_entry(key: str, entry: Dict[str, Any]) -> Dict[str, str]:\n",
        "    try:\n",
        "        question = clean_text(entry.get('QUESTION', ''))\n",
        "        contexts = entry.get('CONTEXTS', [])\n",
        "        answer = entry.get('final_decision', entry.get('ANSWER', ''))\n",
        "        long_answer = entry.get('LONG_ANSWER', '')\n",
        "\n",
        "        if not question:\n",
        "            raise ValueError(\"Missing question\")\n",
        "\n",
        "        if not answer:\n",
        "            raise ValueError(\"Missing answer\")\n",
        "\n",
        "\n",
        "        formatted_contexts = format_context(contexts)\n",
        "        standardized_answer = convert_answer(answer)\n",
        "        reasoning = create_reasoning(question, contexts, answer, long_answer)\n",
        "\n",
        "\n",
        "        instruction = \"Based on the provided PubMed research context, answer whether the research question can be answered with 'Yes', 'No', or 'Maybe/Uncertain'. Provide your reasoning.\"\n",
        "\n",
        "        # Input.\n",
        "        input_text = f\"Research Question: {question}\\n\\nContext from PubMed:\\n{formatted_contexts}\"\n",
        "\n",
        "        # Output.\n",
        "        output_text = f\"{standardized_answer}\\n\\nReasoning: {reasoning}\"\n",
        "\n",
        "        return {\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": input_text,\n",
        "            \"output\": output_text\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing entry {key}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def convert_pubmedqa_to_jsonl(input_file: str, output_file: str, max_entries: int = None):\n",
        "\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
        "\n",
        "    logger.info(f\"Loading PubMedQA dataset from: {input_file}\")\n",
        "\n",
        "    # Original dataset.\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    logger.info(f\"Loaded {len(data)} entries from dataset\")\n",
        "\n",
        "\n",
        "    processed_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for key, entry in data.items():\n",
        "            if max_entries and processed_count >= max_entries:\n",
        "                break\n",
        "\n",
        "            processed_entry = process_pubmedqa_entry(key, entry)\n",
        "\n",
        "            if processed_entry:\n",
        "                json_line = json.dumps(processed_entry, ensure_ascii=False)\n",
        "                f.write(json_line + '\\n')\n",
        "                processed_count += 1\n",
        "\n",
        "\n",
        "                if processed_count % 1000 == 0:\n",
        "                    logger.info(f\"Processed {processed_count} entries.\")\n",
        "            else:\n",
        "                error_count += 1\n",
        "\n",
        "    logger.info(f\"Conversion done\")\n",
        "    logger.info(f\"Successfully processed: {processed_count} entries\")\n",
        "    logger.info(f\"Errors encountered: {error_count} entries\")\n",
        "    logger.info(f\"Output saved to: {output_file}\")\n",
        "\n",
        "\n",
        "    total_chars = 0\n",
        "    max_chars = 0\n",
        "\n",
        "    with open(output_file, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "                entry_length = len(entry['instruction'] + entry['input'] + entry['output'])\n",
        "                total_chars += entry_length\n",
        "                max_chars = max(max_chars, entry_length)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    if processed_count > 0:\n",
        "        avg_chars = total_chars / processed_count\n",
        "        avg_tokens = avg_chars / 4\n",
        "        max_tokens = max_chars / 4\n",
        "\n",
        "        logger.info(f\"\\n Dataset Statistics:\")\n",
        "        logger.info(f\"Average characters per entry: {avg_chars:.0f}\")\n",
        "        logger.info(f\"Average tokens per entry: {avg_tokens:.0f}\")\n",
        "        logger.info(f\"Maximum tokens per entry: {max_tokens:.0f}\")\n",
        "\n",
        "    return processed_count, error_count\n",
        "\n",
        "def validate_jsonl_output(output_file: str, sample_size: int = 3):\n",
        "    logger.info(f\"\\n Validating output file: {output_file}\")\n",
        "\n",
        "    valid_lines = 0\n",
        "    total_lines = 0\n",
        "\n",
        "    with open(output_file, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            total_lines += 1\n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "\n",
        "\n",
        "                required_fields = ['instruction', 'input', 'output']\n",
        "                if all(field in entry and isinstance(entry[field], str) and entry[field].strip()\n",
        "                       for field in required_fields):\n",
        "                    valid_lines += 1\n",
        "\n",
        "\n",
        "                    if valid_lines <= sample_size:\n",
        "                        logger.info(f\"\\n-- Sample Entry {valid_lines} ---\")\n",
        "                        logger.info(f\"Instruction: {entry['instruction'][:100]}...\")\n",
        "                        logger.info(f\"Input: {entry['input'][:200]}...\")\n",
        "                        logger.info(f\"Output: {entry['output'][:150]}...\")\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.error(f\"Invalid JSON on line {line_num}: {e}\")\n",
        "\n",
        "    logger.info(f\"\\n Validation Results:\")\n",
        "    logger.info(f\"Total lines: {total_lines}\")\n",
        "    logger.info(f\"Valid JSONL entries: {valid_lines}\")\n",
        "    logger.info(f\"Success rate: {(valid_lines/total_lines)*100:.1f}%\" if total_lines > 0 else \"No entries found.\")\n",
        "\n",
        "    return valid_lines, total_lines\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    INPUT_FILE = \"/content/pubmedqa/ori_pqal.json\"\n",
        "    OUTPUT_FILE = \"pubmedqa_training.jsonl\"\n",
        "    MAX_ENTRIES = None\n",
        "    VALIDATE_OUTPUT = True\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(\"PubMedQA to JSONL Converter\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(INPUT_FILE):\n",
        "            print(f\"Error: Input file not found at {INPUT_FILE}\")\n",
        "            print(\"Check the PubMedQA dataset is uploaded.\")\n",
        "        else:\n",
        "            print(f\"Input file: {INPUT_FILE}\")\n",
        "            print(f\"Output file: {OUTPUT_FILE}\")\n",
        "            if MAX_ENTRIES:\n",
        "                print(f\"  Max entries: {MAX_ENTRIES}\")\n",
        "\n",
        "            processed, errors = convert_pubmedqa_to_jsonl(INPUT_FILE, OUTPUT_FILE, MAX_ENTRIES)\n",
        "\n",
        "\n",
        "            if VALIDATE_OUTPUT and processed > 0:\n",
        "                valid, total = validate_jsonl_output(OUTPUT_FILE)\n",
        "                print(f\"\\n Conversion done.\")\n",
        "                print(f\" Final Stats: {valid}/{total} valid entries.\")\n",
        "            else:\n",
        "                print(f\"\\n Conversion completed: {processed} entries processed, {errors} errors.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Conversion failed: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtVgmKCN7U78",
        "outputId": "3e43a665-929b-4d08-e3ac-68eb27093bf3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "PubMedQA to JSONL Converter\n",
            "------------------------------------------------------------\n",
            "Input file: /content/pubmedqa/ori_pqal.json\n",
            "Output file: pubmedqa_training.jsonl\n",
            "\n",
            " Conversion done.\n",
            " Final Stats: 1000/1000 valid entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9adc7667",
        "outputId": "32d64c2c-15f9-4d54-9011-c3d5cd48eff1"
      },
      "source": [
        "import json\n",
        "\n",
        "file_path = 'pubmedqa_training.jsonl'\n",
        "num_rows_to_show = 5\n",
        "\n",
        "print(f\"First {num_rows_to_show} rows of {file_path}:\")\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= num_rows_to_show:\n",
        "                break\n",
        "            try:\n",
        "                json_obj = json.loads(line)\n",
        "                print(f\"--- Row {i+1} ---\")\n",
        "                print(json.dumps(json_obj, indent=2))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON on line {i+1}: {e}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found at {file_path}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of pubmedqa_training.jsonl:\n",
            "--- Row 1 ---\n",
            "{\n",
            "  \"instruction\": \"Based on the provided PubMed research context, answer whether the research question can be answered with 'Yes', 'No', or 'Maybe/Uncertain'. Provide your reasoning.\",\n",
            "  \"input\": \"Research Question: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\\n\\nContext from PubMed:\\n[1] Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.\\n\\n[2] The following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (\\u0394\\u03a8m). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.\",\n",
            "  \"output\": \"Yes\\n\\nReasoning: Results depicted mitochondrial dynamics in vivo as PCD progresses within the lace plant, and highlight the correlation of this organelle with other organelles during developmental PCD. To the best of our knowledge, this is the first report of mitochondria and chloroplasts moving on transvacuolar strands to form a ring structure surrounding the nucleus during developmental PCD. Also, for the first time, we have shown the feasibility for the use of CsA in a whole plant system. Overall, our findings implicate the mitochondria as playing a critical and early role in developmentally regulated PCD in the lace plant. This conclusion is based on analysis of 2 research context provided.\"\n",
            "}\n",
            "--- Row 2 ---\n",
            "{\n",
            "  \"instruction\": \"Based on the provided PubMed research context, answer whether the research question can be answered with 'Yes', 'No', or 'Maybe/Uncertain'. Provide your reasoning.\",\n",
            "  \"input\": \"Research Question: Landolt C and snellen e acuity: differences in strabismus amblyopia?\\n\\nContext from PubMed:\\n[1] Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differences in the lower visual acuity range cannot be excluded. In this study, visual acuity measured with the Snellen E was compared to the Landolt C acuity.\\n\\n[2] 100 patients (age 8 - 90 years, median 60.5 years) with various eye disorders, among them 39 with amblyopia due to strabismus, and 13 healthy volunteers were tested. Charts with the Snellen E and the Landolt C (Precision Vision) which mimic the ETDRS charts were used to assess visual acuity. Three out of 5 optotypes per line had to be correctly identified, while wrong answers were monitored. In the group of patients, the eyes with the lower visual acuity, and the right eyes of the healthy subjects, were evaluated.\\n\\n[3] Differences between Landolt C acuity (LR) and Snellen E acuity (SE) were small. The mean decimal values for LR and SE were 0.25 and 0.29 in the entire group and 0.14 and 0.16 for the eyes with strabismus amblyopia. The mean difference between LR and SE was 0.55 lines in the entire group and 0.55 lines for the eyes with strabismus amblyopia, with higher values of SE in both groups. The results of the other groups were similar with only small differences between LR and SE.\",\n",
            "  \"output\": \"No\\n\\nReasoning: Using the charts described, there was only a slight overestimation of visual acuity by the Snellen E compared to the Landolt C, even in strabismus amblyopia. Small differences in the lower visual acuity range have to be considered. This conclusion is based on analysis of 3 research context provided.\"\n",
            "}\n",
            "--- Row 3 ---\n",
            "{\n",
            "  \"instruction\": \"Based on the provided PubMed research context, answer whether the research question can be answered with 'Yes', 'No', or 'Maybe/Uncertain'. Provide your reasoning.\",\n",
            "  \"input\": \"Research Question: Syncope during bathing in infants, a pediatric form of water-induced urticaria?\\n\\nContext from PubMed:\\n[1] Apparent life-threatening events in infants are a difficult and frequent problem in pediatric practice. The prognosis is uncertain because of risk of sudden infant death syndrome.\\n\\n[2] Eight infants aged 2 to 15 months were admitted during a period of 6 years; they suffered from similar maladies in the bath: on immersion, they became pale, hypotonic, still and unreactive; recovery took a few seconds after withdrawal from the bath and stimulation. Two diagnoses were initially considered: seizure or gastroesophageal reflux but this was doubtful. The hypothesis of an equivalent of aquagenic urticaria was then considered; as for patients with this disease, each infant's family contained members suffering from dermographism, maladies or eruption after exposure to water or sun. All six infants had dermographism. We found an increase in blood histamine levels after a trial bath in the two infants tested. The evolution of these \\\"aquagenic maladies\\\" was favourable after a few weeks without baths. After a 2-7 year follow-up, three out of seven infants continue to suffer from troubles associated with sun or water.\",\n",
            "  \"output\": \"Yes\\n\\nReasoning: \\\"Aquagenic maladies\\\" could be a pediatric form of the aquagenic urticaria. This conclusion is based on analysis of 2 research context provided.\"\n",
            "}\n",
            "--- Row 4 ---\n",
            "{\n",
            "  \"instruction\": \"Based on the provided PubMed research context, answer whether the research question can be answered with 'Yes', 'No', or 'Maybe/Uncertain'. Provide your reasoning.\",\n",
            "  \"input\": \"Research Question: Are the long-term results of the transanal pull-through equal to those of the transabdominal pull-through?\\n\\nContext from PubMed:\\n[1] The transanal endorectal pull-through (TERPT) is becoming the most popular procedure in the treatment of Hirschsprung disease (HD), but overstretching of the anal sphincters remains a critical issue that may impact the continence. This study examined the long-term outcome of TERPT versus conventional transabdominal (ABD) pull-through for HD.\\n\\n[2] Records of 41 patients more than 3 years old who underwent a pull-through for HD (TERPT, n = 20; ABD, n = 21) were reviewed, and their families were thoroughly interviewed and scored via a 15-item post-pull-through long-term outcome questionnaire. Patients were operated on between the years 1995 and 2003. During this time, our group transitioned from the ABD to the TERPT technique. Total scoring ranged from 0 to 40: 0 to 10, excellent; 11 to 20 good; 21 to 30 fair; 31 to 40 poor. A 2-tailed Student t test, analysis of covariance, as well as logistic and linear regression were used to analyze the collected data with confidence interval higher than 95%.\\n\\n[3] Overall scores were similar. However, continence score was significantly better in the ABD group, and the stool pattern score was better in the TERPT group. A significant difference in age at interview between the 2 groups was noted; we therefore reanalyzed the data controlling for age, and this showed that age did not significantly affect the long-term scoring outcome between groups.\",\n",
            "  \"output\": \"No\\n\\nReasoning: Our long-term study showed significantly better (2-fold) results regarding the continence score for the abdominal approach compared with the transanal pull-through. The stool pattern and enterocolitis scores were somewhat better for the TERPT group. These findings raise an important issue about the current surgical management of HD; however, more cases will need to be studied before a definitive conclusion can be drawn. This conclusion is based on analysis of 3 research context provided.\"\n",
            "}\n",
            "--- Row 5 ---\n",
            "{\n",
            "  \"instruction\": \"Based on the provided PubMed research context, answer whether the research question can be answered with 'Yes', 'No', or 'Maybe/Uncertain'. Provide your reasoning.\",\n",
            "  \"input\": \"Research Question: Can tailored interventions increase mammography use among HMO women?\\n\\nContext from PubMed:\\n[1] Telephone counseling and tailored print communications have emerged as promising methods for promoting mammography screening. However, there has been little research testing, within the same randomized field trial, of the efficacy of these two methods compared to a high-quality usual care system for enhancing screening. This study addressed the question: Compared to usual care, is tailored telephone counseling more effective than tailored print materials for promoting mammography screening?\\n\\n[2] Three-year randomized field trial.\\n\\n[3] One thousand ninety-nine women aged 50 and older recruited from a health maintenance organization in North Carolina.\\n\\n[4] Women were randomized to 1 of 3 groups: (1) usual care, (2) tailored print communications, and (3) tailored telephone counseling.\\n\\n[5] Adherence to mammography screening based on self-reports obtained during 1995, 1996, and 1997.\\n\\n[6] Compared to usual care alone, telephone counseling promoted a significantly higher proportion of women having mammograms on schedule (71% vs 61%) than did tailored print (67% vs 61%) but only after the first year of intervention (during 1996). Furthermore, compared to usual care, telephone counseling was more effective than tailored print materials at promoting being on schedule with screening during 1996 and 1997 among women who were off-schedule during the previous year.\",\n",
            "  \"output\": \"Yes\\n\\nReasoning: The effects of the intervention were most pronounced after the first intervention. Compared to usual care, telephone counseling seemed particularly effective at promoting change among nonadherent women, the group for whom the intervention was developed. These results suggest that telephone counseling, rather than tailored print, might be the preferred first-line intervention for getting nonadherent women on schedule for mammography screening. Many questions would have to be answered about why the tailored print intervention was not more powerful. Nevertheless, it is clear that additional interventions will be needed to maintain women's adherence to mammography. Medical Subject Headings (MeSH): mammography screening, telephone counseling, tailored print communications, barriers. This conclusion is based on analysis of 6 research context provided.\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}