{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0if1VT9g8/eCEDL6GRV0a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#!/usr/bin/env python3"],"metadata":{"id":"YRA6wpapbENF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Chain-of-Thought (CoT) Generation Script\n","=======================================\n","\n","This script generates Chain-of-Thought reasoning for medical chatbot training data.\n","It supports multiple AI models and includes robust error handling, batching, and retry logic.\n","\n","Supported Models:\n","- OpenAI GPT-4, GPT-3.5-turbo\n","- Anthropic Claude models (via API)\n","- Hugging Face models (local or API)\n","- Ollama models (local)"],"metadata":{"id":"Fx-bXC1HbGle"}},{"cell_type":"code","source":["import subprocess\n","import sys"],"metadata":{"id":"D5NILkSWv7uM","executionInfo":{"status":"ok","timestamp":1753332302465,"user_tz":420,"elapsed":7,"user":{"displayName":"Cassandra Maldonado","userId":"09968165232903448496"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def install_packages():\n","    packages = [\n","        'backoff',\n","        'nest-asyncio',\n","        'aiohttp',\n","        'openai',\n","        'transformers',\n","        'torch',\n","        'tqdm'\n","    ]\n","\n","    for package in packages:\n","        try:\n","            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","            print(f\"Installed {package}\")\n","        except subprocess.CalledProcessError:\n","            print(f\"Failed to install {package}\")"],"metadata":{"id":"iCE1klr8wDIr","executionInfo":{"status":"ok","timestamp":1753332332940,"user_tz":420,"elapsed":10,"user":{"displayName":"Cassandra Maldonado","userId":"09968165232903448496"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["install_packages()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ef7hapJpwFgA","executionInfo":{"status":"ok","timestamp":1753332491896,"user_tz":420,"elapsed":149303,"user":{"displayName":"Cassandra Maldonado","userId":"09968165232903448496"}},"outputId":"d2b5b8ed-8901-4c1b-99da-d3796c5023a7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Installed backoff\n","Installed nest-asyncio\n","Installed aiohttp\n","Installed openai\n","Installed transformers\n","Installed torch\n","Installed tqdm\n"]}]},{"cell_type":"code","source":["import json\n","import os\n","import time\n","import asyncio\n","import aiohttp\n","from typing import Dict, List, Any, Optional, Tuple\n","from dataclasses import dataclass, field\n","import logging\n","from pathlib import Path\n","import backoff\n","from tqdm.asyncio import tqdm\n","import nest_asyncio\n","import openai"],"metadata":{"id":"P8BUA0DCwIZB","executionInfo":{"status":"ok","timestamp":1753333649477,"user_tz":420,"elapsed":42,"user":{"displayName":"Cassandra Maldonado","userId":"09968165232903448496"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# For Colab compatibility.\n","nest_asyncio.apply()"],"metadata":{"id":"RkTXw2mTwI4q","executionInfo":{"status":"ok","timestamp":1753333651228,"user_tz":420,"elapsed":4,"user":{"displayName":"Cassandra Maldonado","userId":"09968165232903448496"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)"],"metadata":{"id":"dI5iodlZwLyM","executionInfo":{"status":"ok","timestamp":1753333652233,"user_tz":420,"elapsed":5,"user":{"displayName":"Cassandra Maldonado","userId":"09968165232903448496"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["@dataclass\n","class CoTConfig:\n","    model_name: str = \"gpt-3.5-turbo\"\n","    api_key: Optional[str] = None\n","    batch_size: int = 5\n","    max_retries: int = 3\n","    retry_delay: float = 1.0\n","    max_tokens: int = 512\n","    temperature: float = 0.7\n","    concurrent_requests: int = 3\n","    rate_limit_delay: float = 1.0\n","    local_model_path: Optional[str] = None\n","    use_gpu: bool = True\n","\n","    openai_base_url: str = \"https://api.openai.com/v1\"\n","    anthropic_base_url: str = \"https://api.anthropic.com/v1\"\n","\n","class CoTGenerator:\n","    def __init__(self, config: CoTConfig):\n","        self.config = config\n","        self.session = None\n","        self.semaphore = asyncio.Semaphore(config.concurrent_requests)\n","        self.setup_apis()\n","\n","    def setup_apis(self):\n","\n","        if self.config.model_name.startswith(('gpt-', 'text-davinci')):\n","            if not self.config.api_key:\n","                self.config.api_key = os.getenv('OPENAI_API_KEY')\n","            if not self.config.api_key:\n","                logger.warning(\"OpenAI API key not found. Please set it manually.\")\n","            else:\n","                openai.api_key = self.config.api_key\n","\n","        elif self.config.model_name.startswith('claude'):\n","            if not self.config.api_key:\n","                self.config.api_key = os.getenv('ANTHROPIC_API_KEY')\n","            if not self.config.api_key:\n","                logger.warning(\"Anthropic API key not found.\")\n","\n","    def create_cot_prompt(self, instruction: str, answer: str) -> str:\n","        \"\"\"Create prompt for generating CoT reasoning\"\"\"\n","        prompt = f\"\"\"You are a medical expert assistant. For the given medical question and answer, provide detailed Chain-of-Thought reasoning that explains the step-by-step thinking process leading to the answer.\n","\n","Your reasoning should:\n","1. Break down the problem systematically\n","2. Identify key medical concepts and symptoms\n","3. Consider differential diagnoses where applicable\n","4. Explain the logical progression of thoughts\n","5. Be clear, educational, and medically accurate\n","\n","Question: {instruction}\n","\n","Expected Answer: {answer}\n","\n","Please provide the Chain-of-Thought reasoning that leads to this answer. Start your response with \"Chain-of-Thought:\" and then provide the detailed reasoning.\n","\n","Chain-of-Thought:\"\"\"\n","        return prompt\n","\n","    @backoff.on_exception(\n","        backoff.expo,\n","        (Exception,),\n","        max_tries=3\n","    )\n","    async def generate_cot_openai(self, prompt: str) -> str:\n","        try:\n","            client = openai.AsyncOpenAI(api_key=self.config.api_key)\n","\n","            response = await client.chat.completions.create(\n","                model=self.config.model_name,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                max_tokens=self.config.max_tokens,\n","                temperature=self.config.temperature,\n","                timeout=30\n","            )\n","\n","            return response.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            logger.error(f\"OpenAI API error: {e}\")\n","            raise\n","\n","    @backoff.on_exception(\n","        backoff.expo,\n","        (aiohttp.ClientError, asyncio.TimeoutError),\n","        max_tries=3\n","    )\n","    async def generate_cot_anthropic(self, prompt: str) -> str:\n","        headers = {\n","            \"Content-Type\": \"application/json\",\n","            \"x-api-key\": self.config.api_key,\n","            \"anthropic-version\": \"2023-06-01\"\n","        }\n","\n","        payload = {\n","            \"model\": self.config.model_name,\n","            \"max_tokens\": self.config.max_tokens,\n","            \"temperature\": self.config.temperature,\n","            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n","        }\n","\n","        async with self.session.post(\n","            f\"{self.config.anthropic_base_url}/messages\",\n","            headers=headers,\n","            json=payload,\n","            timeout=30\n","        ) as response:\n","            if response.status == 200:\n","                result = await response.json()\n","                return result[\"content\"][0][\"text\"].strip()\n","            else:\n","                error_text = await response.text()\n","                raise aiohttp.ClientError(f\"Anthropic API error: {response.status} - {error_text}\")\n","\n","    async def generate_single_cot(self, sample: Dict[str, Any]) -> Dict[str, Any]:\n","        async with self.semaphore:\n","            try:\n","                instruction = sample.get(\"instruction\", \"\")\n","                answer = sample.get(\"output\", \"\")\n","\n","                if not instruction or not answer:\n","                    logger.warning(\"Skipping sample with missing instruction or answer.\")\n","                    return {**sample, \"cot_reasoning\": \"\"}\n","\n","                prompt = self.create_cot_prompt(instruction, answer)\n","\n","                # Route to appropriate model for CoT generation.\n","                if self.config.model_name.startswith(('gpt-', 'text-davinci')):\n","                    cot_reasoning = await self.generate_cot_openai(prompt)\n","                elif self.config.model_name.startswith('claude'):\n","                    cot_reasoning = await self.generate_cot_anthropic(prompt)\n","                else:\n","                    raise ValueError(f\"Unsupported model: {self.config.model_name}\")\n","\n","                # Cleaning up the reasoning.\n","                if cot_reasoning.startswith(\"Chain-of-Thought:\"):\n","                    cot_reasoning = cot_reasoning[len(\"Chain-of-Thought:\"):].strip()\n","\n","                # Add rate limit delay.\n","                await asyncio.sleep(self.config.rate_limit_delay)\n","\n","                return {**sample, \"cot_reasoning\": cot_reasoning}\n","\n","            except Exception as e:\n","                logger.error(f\"Error generating CoT for sample: {e}\")\n","                return {**sample, \"cot_reasoning\": f\"Error: {str(e)}\"}\n","\n","    async def process_batch(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n","        tasks = [self.generate_single_cot(sample) for sample in batch]\n","        return await asyncio.gather(*tasks, return_exceptions=True)\n","\n","    async def generate_cot_for_dataset(self, input_file: str, output_file: str, limit: Optional[int] = None):\n","        logger.info(f\"Loading dataset from {input_file}\")\n","\n","        # Loading the dataset.\n","        with open(input_file, 'r', encoding='utf-8') as f:\n","            dataset = json.load(f)\n","\n","        if limit is not None:\n","            dataset = dataset[:limit]\n","            logger.info(f\"Limiting dataset to {len(dataset)} samples\")\n","\n","\n","        logger.info(f\"Loaded {len(dataset)} samples\")\n","\n","        # Creating batches.\n","        batches = [\n","            dataset[i:i + self.config.batch_size]\n","            for i in range(0, len(dataset), self.config.batch_size)\n","        ]\n","\n","        logger.info(f\"Processing {len(batches)} batches of size {self.config.batch_size}\")\n","\n","        # Setting up session for API calls.\n","        connector = aiohttp.TCPConnector(limit=self.config.concurrent_requests)\n","        timeout = aiohttp.ClientTimeout(total=60)\n","        self.session = aiohttp.ClientSession(connector=connector, timeout=timeout)\n","\n","        try:\n","            results = []\n","\n","            # Processing batches.\n","            for i, batch in enumerate(batches):\n","                print(f\"Processing batch {i+1}/{len(batches)}.\")\n","\n","                batch_results = await self.process_batch(batch)\n","\n","                # Handling exceptions in results.\n","                for result in batch_results:\n","                    if isinstance(result, Exception):\n","                        logger.error(f\"Batch processing error: {result}\")\n","                        results.append({\"error\": str(result)})\n","                    else:\n","                        results.append(result)\n","\n","                # Save results every 10 batches.\n","                if (i + 1) % 10 == 0:\n","                    self.save_intermediate_results(results, output_file)\n","\n","                print(f\"Completed {len(results)}/{len(dataset)} samples\")\n","\n","            # Final results.\n","            logger.info(f\"Saving final results to {output_file}\")\n","            os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n","\n","            with open(output_file, 'w', encoding='utf-8') as f:\n","                json.dump(results, f, indent=2, ensure_ascii=False)\n","\n","            # Statistics.\n","            self.generate_statistics(results, input_file, output_file)\n","\n","        finally:\n","            if self.session:\n","                await self.session.close()\n","\n","    def save_intermediate_results(self, results: List[Dict[str, Any]], output_file: str):\n","        intermediate_file = output_file.replace('.json', '_intermediate.json')\n","        output_dir = os.path.dirname(intermediate_file)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","        with open(intermediate_file, 'w', encoding='utf-8') as f:\n","            json.dump(results, f, indent=2, ensure_ascii=False)\n","        logger.info(f\"Saved intermediate results: {len(results)} samples.\")\n","\n","    def generate_statistics(self, results: List[Dict[str, Any]], input_file: str, output_file: str):\n","        stats = {\n","            \"total_samples\": len(results),\n","            \"successful_generations\": sum(1 for r in results if r.get(\"cot_reasoning\") and not r.get(\"cot_reasoning\", \"\").startswith(\"Error:\")),\n","            \"failed_generations\": sum(1 for r in results if r.get(\"cot_reasoning\", \"\").startswith(\"Error:\") or not r.get(\"cot_reasoning\")),\n","            \"average_cot_length\": 0,\n","            \"model_config\": {\n","                \"model_name\": self.config.model_name,\n","                \"max_tokens\": self.config.max_tokens,\n","                \"temperature\": self.config.temperature,\n","                \"batch_size\": self.config.batch_size\n","            }\n","        }\n","\n","        # Average CoT length.\n","        valid_cots = [r.get(\"cot_reasoning\", \"\") for r in results if r.get(\"cot_reasoning\") and not r.get(\"cot_reasoning\", \"\").startswith(\"Error:\")]\n","        if valid_cots:\n","            stats[\"average_cot_length\"] = sum(len(cot.split()) for cot in valid_cots) / len(valid_cots)\n","\n","        # Statistics.\n","        stats_file = output_file.replace('.json', '_cot_stats.json')\n","        output_dir = os.path.dirname(stats_file)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","        with open(stats_file, 'w', encoding='utf-8') as f:\n","            json.dump(stats, f, indent=2, ensure_ascii=False)\n","\n","        logger.info(f\"CoT generation statistics saved to {stats_file}\")\n","        logger.info(f\"Success rate: {stats['successful_generations']}/{stats['total_samples']} ({stats['successful_generations']/stats['total_samples']*100:.1f}%)\")\n","\n","\n","def setup_colab_environment():\n","    try:\n","        import google.colab\n","        IN_COLAB = True\n","        print(\"Running in Google Colab environment.\")\n","\n","        # Google Drive.\n","        from google.colab import drive\n","        print(\"Mounting Google Drive.\")\n","        drive.mount('/content/drive')\n","        print(\"Google Drive successful.\")\n","\n","    except ImportError:\n","        IN_COLAB = False\n","        print(\"Running in local environment.\")\n","\n","    return IN_COLAB\n","\n","\n","def find_train_json():\n","    search_paths = [\n","        '/content/drive/MyDrive/train.json',\n","        '/content/drive/My Drive/train.json',\n","        '/content/drive/MyDrive/data/train.json',\n","        '/content/drive/My Drive/data/train.json',\n","        '/content/train.json',\n","        './train.json',\n","        './data/train.json'\n","    ]\n","\n","    for path in search_paths:\n","        if os.path.exists(path):\n","            print(f\"Found train.json at: {path}\")\n","            return path\n","\n","    return None\n","\n","\n","# Function for CoT generation.\n","async def run_cot_generation():\n","    print(\"Chain-of-Thought Generation for our Medical Chatbot\")\n","    print(\"=\" * 50)\n","\n","    # Environment.\n","    IN_COLAB = setup_colab_environment()\n","\n","    train_file = find_train_json()\n","\n","    if not train_file:\n","        print(\"Train.json file not found.\")\n","        print(\"Please provide the path to the train.json file:\")\n","        train_file = input(\"Train file path: \").strip()\n","\n","        if not os.path.exists(train_file):\n","            print(f\"File not found: {train_file}\")\n","            return\n","\n","    # Configuration\n","    print(\"\\n📋 Configuration:\")\n","    print(\"Available models:\")\n","    print(\"  1. gpt-3.5-turbo (recommended, cheaper)\")\n","    print(\"  2. gpt-4 (more expensive, better quality)\")\n","    print(\"  3. claude-3-sonnet-20240229\")\n","\n","    model_choice = input(\"Choose model (1/2/3) or enter custom name: \").strip()\n","\n","    if model_choice == \"1\":\n","        model_name = \"gpt-3.5-turbo\"\n","    elif model_choice == \"2\":\n","        model_name = \"gpt-4\"\n","    elif model_choice == \"3\":\n","        model_name = \"claude-3-sonnet-20240229\"\n","    elif model_choice:\n","        model_name = model_choice\n","    else:\n","        model_name = \"gpt-3.5-turbo\"\n","\n","    print(f\"✓ Selected model: {model_name}\")\n","\n","    # API Key\n","    api_key = None\n","    if model_name.startswith('gpt'):\n","        print(\"\\n🔑 API Key Setup:\")\n","        print(\"You need an OpenAI API key for GPT models.\")\n","        print(\"Get one at: https://platform.openai.com/api-keys\")\n","\n","        api_key = input(\"Enter your OpenAI API Key (starts with sk-): \").strip()\n","        if not api_key:\n","            api_key = os.getenv('OPENAI_API_KEY')\n","\n","        if not api_key:\n","            print(\"❌ Error: OpenAI API key is required for GPT models!\")\n","            return\n","        elif not api_key.startswith('sk-'):\n","            print(\"⚠️ Warning: OpenAI API keys usually start with 'sk-'\")\n","            confirm = input(\"Continue anyway? (y/n): \").strip().lower()\n","            if confirm != 'y':\n","                return\n","\n","    elif model_name.startswith('claude'):\n","        print(\"\\n🔑 API Key Setup:\")\n","        print(\"You need an Anthropic API key for Claude models.\")\n","        print(\"Get one at: https://console.anthropic.com/\")\n","\n","        api_key = input(\"Enter your Anthropic API Key: \").strip()\n","        if not api_key:\n","            api_key = os.getenv('ANTHROPIC_API_KEY')\n","\n","        if not api_key:\n","            print(\"❌ Error: Anthropic API key is required for Claude models!\")\n","            return\n","\n","    print(\"✅ API key configured successfully!\")\n","\n","    # Batch configuration\n","    print(\"\\n⚙️ Processing Configuration:\")\n","\n","    while True:\n","        try:\n","            batch_input = input(\"Batch size (5 recommended): \").strip()\n","            batch_size = int(batch_input) if batch_input else 5\n","            if batch_size > 0:\n","                break\n","            else:\n","                print(\"❌ Batch size must be positive!\")\n","        except ValueError:\n","            print(\"❌ Please enter a valid number for batch size!\")\n","\n","    while True:\n","        try:\n","            tokens_input = input(\"Max tokens for CoT (512 recommended): \").strip()\n","            max_tokens = int(tokens_input) if tokens_input else 512\n","            if max_tokens > 0:\n","                break\n","            else:\n","                print(\"❌ Max tokens must be positive!\")\n","        except ValueError:\n","            print(\"❌ Please enter a valid number for max tokens!\")\n","\n","    # Get limit from user\n","    limit_input = input(\"Number of samples to process (enter for all): \").strip()\n","    limit = int(limit_input) if limit_input else None\n","\n","\n","    # Output file\n","    output_dir = input(\"Output directory (./data): \").strip() or \"./data\"\n","    output_file = os.path.join(output_dir, \"train_with_cot.json\")\n","\n","    # Preview the dataset\n","    print(f\"\\n👀 Previewing dataset: {train_file}\")\n","    try:\n","        with open(train_file, 'r', encoding='utf-8') as f:\n","            dataset = json.load(f)\n","\n","        print(f\"📊 Dataset size: {len(dataset)} samples\")\n","        if dataset:\n","            sample = dataset[0]\n","            print(f\"📝 Sample keys: {list(sample.keys())}\")\n","            print(f\"📏 Sample instruction length: {len(sample.get('instruction', '').split())} words\")\n","            print(f\"📏 Sample output length: {len(sample.get('output', '').split())} words\")\n","    except Exception as e:\n","        print(f\"❌ Error reading dataset: {e}\")\n","        return\n","\n","    # Estimate costs (rough)\n","    total_samples_to_process = limit if limit is not None else len(dataset)\n","    estimated_tokens_per_sample = 300  # Rough estimate\n","    total_tokens = total_samples_to_process * estimated_tokens_per_sample\n","\n","    if model_name == \"gpt-3.5-turbo\":\n","        cost_per_1k = 0.0015  # $0.0015 per 1K tokens\n","    elif model_name == \"gpt-4\":\n","        cost_per_1k = 0.03    # $0.03 per 1K tokens\n","    else:\n","        cost_per_1k = 0.01    # Rough estimate for other models\n","\n","    estimated_cost = (total_tokens / 1000) * cost_per_1k\n","\n","    print(f\"\\n💰 Estimated cost: ${estimated_cost:.2f}\")\n","    print(f\"⏱️ Estimated time: {(total_samples_to_process * 2) / 60:.1f} minutes\")\n","\n","    confirm = input(\"\\nProceed with CoT generation? (y/n): \").strip().lower()\n","    if confirm != 'y':\n","        print(\"❌ Generation cancelled.\")\n","        return\n","\n","    # Create configuration\n","    config = CoTConfig(\n","        model_name=model_name,\n","        api_key=api_key,\n","        batch_size=batch_size,\n","        max_tokens=max_tokens,\n","        temperature=0.7,\n","        concurrent_requests=3,\n","        rate_limit_delay=1.0\n","    )\n","\n","    # Initialize generator\n","    generator = CoTGenerator(config)\n","\n","    try:\n","        print(f\"\\n🚀 Starting CoT generation...\")\n","        await generator.generate_cot_for_dataset(train_file, output_file, limit=limit)\n","\n","        print(\"\\n🎉 CoT generation completed successfully!\")\n","        print(f\"📁 Output file: {output_file}\")\n","\n","        # Download in Colab\n","        if IN_COLAB:\n","            try:\n","                from google.colab import files\n","                files.download(output_file)\n","\n","                # Also download stats file\n","                stats_file = output_file.replace('.json', '_cot_stats.json')\n","                if os.path.exists(stats_file):\n","                    files.download(stats_file)\n","\n","                print(\"✅ Files downloaded successfully!\")\n","            except Exception as e:\n","                print(f\"⚠️ Could not download files: {e}\")\n","                print(f\"Files are available at: {output_file}\")\n","\n","    except Exception as e:\n","        print(f\"❌ Error during CoT generation: {e}\")\n","        import traceback\n","        traceback.print_exc()"],"metadata":{"id":"Am_MeOfdwOOM","executionInfo":{"status":"ok","timestamp":1753334826118,"user_tz":420,"elapsed":89,"user":{"displayName":"Cassandra Maldonado","userId":"09968165232903448496"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Main execution\n","async def main():\n","    await run_cot_generation()\n","\n","# For Colab execution\n","if __name__ == \"__main__\":\n","    asyncio.run(main())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BoY8IbJ8wSZw","executionInfo":{"status":"ok","timestamp":1753337741962,"user_tz":420,"elapsed":2909711,"user":{"displayName":"Cassandra Maldonado","userId":"09968165232903448496"}},"outputId":"23a25ce0-6be7-4266-eecf-30aa95690a3f"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Chain-of-Thought Generation for our Medical Chatbot\n","==================================================\n","Running in Google Colab environment.\n","Mounting Google Drive.\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Google Drive successful.\n","Found train.json at: /content/train.json\n","\n","📋 Configuration:\n","Available models:\n","  1. gpt-3.5-turbo (recommended, cheaper)\n","  2. gpt-4 (more expensive, better quality)\n","  3. claude-3-sonnet-20240229\n","Choose model (1/2/3) or enter custom name: 1\n","✓ Selected model: gpt-3.5-turbo\n","\n","🔑 API Key Setup:\n","You need an OpenAI API key for GPT models.\n","Get one at: https://platform.openai.com/api-keys\n","Enter your OpenAI API Key (starts with sk-): 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\n","✅ API key configured successfully!\n","\n","⚙️ Processing Configuration:\n","Batch size (5 recommended): 5\n","Max tokens for CoT (512 recommended): \n","Number of samples to process (enter for all): 2000\n","Output directory (./data): \n","\n","👀 Previewing dataset: /content/train.json\n","📊 Dataset size: 124980 samples\n","📝 Sample keys: ['instruction', 'input', 'output']\n","📏 Sample instruction length: 37 words\n","📏 Sample output length: 93 words\n","\n","💰 Estimated cost: $0.90\n","⏱️ Estimated time: 66.7 minutes\n","\n","Proceed with CoT generation? (y/n): y\n","\n","🚀 Starting CoT generation...\n","Processing batch 1/400.\n","Completed 5/2000 samples\n","Processing batch 2/400.\n","Completed 10/2000 samples\n","Processing batch 3/400.\n","Completed 15/2000 samples\n","Processing batch 4/400.\n","Completed 20/2000 samples\n","Processing batch 5/400.\n","Completed 25/2000 samples\n","Processing batch 6/400.\n","Completed 30/2000 samples\n","Processing batch 7/400.\n","Completed 35/2000 samples\n","Processing batch 8/400.\n","Completed 40/2000 samples\n","Processing batch 9/400.\n","Completed 45/2000 samples\n","Processing batch 10/400.\n","Completed 50/2000 samples\n","Processing batch 11/400.\n","Completed 55/2000 samples\n","Processing batch 12/400.\n","Completed 60/2000 samples\n","Processing batch 13/400.\n","Completed 65/2000 samples\n","Processing batch 14/400.\n","Completed 70/2000 samples\n","Processing batch 15/400.\n","Completed 75/2000 samples\n","Processing batch 16/400.\n","Completed 80/2000 samples\n","Processing batch 17/400.\n","Completed 85/2000 samples\n","Processing batch 18/400.\n","Completed 90/2000 samples\n","Processing batch 19/400.\n","Completed 95/2000 samples\n","Processing batch 20/400.\n","Completed 100/2000 samples\n","Processing batch 21/400.\n","Completed 105/2000 samples\n","Processing batch 22/400.\n","Completed 110/2000 samples\n","Processing batch 23/400.\n","Completed 115/2000 samples\n","Processing batch 24/400.\n","Completed 120/2000 samples\n","Processing batch 25/400.\n","Completed 125/2000 samples\n","Processing batch 26/400.\n","Completed 130/2000 samples\n","Processing batch 27/400.\n","Completed 135/2000 samples\n","Processing batch 28/400.\n","Completed 140/2000 samples\n","Processing batch 29/400.\n","Completed 145/2000 samples\n","Processing batch 30/400.\n","Completed 150/2000 samples\n","Processing batch 31/400.\n","Completed 155/2000 samples\n","Processing batch 32/400.\n","Completed 160/2000 samples\n","Processing batch 33/400.\n","Completed 165/2000 samples\n","Processing batch 34/400.\n","Completed 170/2000 samples\n","Processing batch 35/400.\n","Completed 175/2000 samples\n","Processing batch 36/400.\n","Completed 180/2000 samples\n","Processing batch 37/400.\n","Completed 185/2000 samples\n","Processing batch 38/400.\n","Completed 190/2000 samples\n","Processing batch 39/400.\n","Completed 195/2000 samples\n","Processing batch 40/400.\n","Completed 200/2000 samples\n","Processing batch 41/400.\n","Completed 205/2000 samples\n","Processing batch 42/400.\n","Completed 210/2000 samples\n","Processing batch 43/400.\n","Completed 215/2000 samples\n","Processing batch 44/400.\n","Completed 220/2000 samples\n","Processing batch 45/400.\n","Completed 225/2000 samples\n","Processing batch 46/400.\n","Completed 230/2000 samples\n","Processing batch 47/400.\n","Completed 235/2000 samples\n","Processing batch 48/400.\n","Completed 240/2000 samples\n","Processing batch 49/400.\n","Completed 245/2000 samples\n","Processing batch 50/400.\n","Completed 250/2000 samples\n","Processing batch 51/400.\n","Completed 255/2000 samples\n","Processing batch 52/400.\n","Completed 260/2000 samples\n","Processing batch 53/400.\n","Completed 265/2000 samples\n","Processing batch 54/400.\n","Completed 270/2000 samples\n","Processing batch 55/400.\n","Completed 275/2000 samples\n","Processing batch 56/400.\n","Completed 280/2000 samples\n","Processing batch 57/400.\n","Completed 285/2000 samples\n","Processing batch 58/400.\n","Completed 290/2000 samples\n","Processing batch 59/400.\n","Completed 295/2000 samples\n","Processing batch 60/400.\n","Completed 300/2000 samples\n","Processing batch 61/400.\n","Completed 305/2000 samples\n","Processing batch 62/400.\n","Completed 310/2000 samples\n","Processing batch 63/400.\n","Completed 315/2000 samples\n","Processing batch 64/400.\n","Completed 320/2000 samples\n","Processing batch 65/400.\n","Completed 325/2000 samples\n","Processing batch 66/400.\n","Completed 330/2000 samples\n","Processing batch 67/400.\n","Completed 335/2000 samples\n","Processing batch 68/400.\n","Completed 340/2000 samples\n","Processing batch 69/400.\n","Completed 345/2000 samples\n","Processing batch 70/400.\n","Completed 350/2000 samples\n","Processing batch 71/400.\n","Completed 355/2000 samples\n","Processing batch 72/400.\n","Completed 360/2000 samples\n","Processing batch 73/400.\n","Completed 365/2000 samples\n","Processing batch 74/400.\n","Completed 370/2000 samples\n","Processing batch 75/400.\n","Completed 375/2000 samples\n","Processing batch 76/400.\n","Completed 380/2000 samples\n","Processing batch 77/400.\n","Completed 385/2000 samples\n","Processing batch 78/400.\n","Completed 390/2000 samples\n","Processing batch 79/400.\n","Completed 395/2000 samples\n","Processing batch 80/400.\n","Completed 400/2000 samples\n","Processing batch 81/400.\n","Completed 405/2000 samples\n","Processing batch 82/400.\n","Completed 410/2000 samples\n","Processing batch 83/400.\n","Completed 415/2000 samples\n","Processing batch 84/400.\n","Completed 420/2000 samples\n","Processing batch 85/400.\n","Completed 425/2000 samples\n","Processing batch 86/400.\n","Completed 430/2000 samples\n","Processing batch 87/400.\n","Completed 435/2000 samples\n","Processing batch 88/400.\n","Completed 440/2000 samples\n","Processing batch 89/400.\n","Completed 445/2000 samples\n","Processing batch 90/400.\n","Completed 450/2000 samples\n","Processing batch 91/400.\n","Completed 455/2000 samples\n","Processing batch 92/400.\n","Completed 460/2000 samples\n","Processing batch 93/400.\n","Completed 465/2000 samples\n","Processing batch 94/400.\n","Completed 470/2000 samples\n","Processing batch 95/400.\n","Completed 475/2000 samples\n","Processing batch 96/400.\n","Completed 480/2000 samples\n","Processing batch 97/400.\n","Completed 485/2000 samples\n","Processing batch 98/400.\n","Completed 490/2000 samples\n","Processing batch 99/400.\n","Completed 495/2000 samples\n","Processing batch 100/400.\n","Completed 500/2000 samples\n","Processing batch 101/400.\n","Completed 505/2000 samples\n","Processing batch 102/400.\n","Completed 510/2000 samples\n","Processing batch 103/400.\n","Completed 515/2000 samples\n","Processing batch 104/400.\n","Completed 520/2000 samples\n","Processing batch 105/400.\n","Completed 525/2000 samples\n","Processing batch 106/400.\n","Completed 530/2000 samples\n","Processing batch 107/400.\n","Completed 535/2000 samples\n","Processing batch 108/400.\n","Completed 540/2000 samples\n","Processing batch 109/400.\n","Completed 545/2000 samples\n","Processing batch 110/400.\n","Completed 550/2000 samples\n","Processing batch 111/400.\n","Completed 555/2000 samples\n","Processing batch 112/400.\n","Completed 560/2000 samples\n","Processing batch 113/400.\n","Completed 565/2000 samples\n","Processing batch 114/400.\n","Completed 570/2000 samples\n","Processing batch 115/400.\n","Completed 575/2000 samples\n","Processing batch 116/400.\n","Completed 580/2000 samples\n","Processing batch 117/400.\n","Completed 585/2000 samples\n","Processing batch 118/400.\n","Completed 590/2000 samples\n","Processing batch 119/400.\n","Completed 595/2000 samples\n","Processing batch 120/400.\n","Completed 600/2000 samples\n","Processing batch 121/400.\n","Completed 605/2000 samples\n","Processing batch 122/400.\n","Completed 610/2000 samples\n","Processing batch 123/400.\n","Completed 615/2000 samples\n","Processing batch 124/400.\n","Completed 620/2000 samples\n","Processing batch 125/400.\n","Completed 625/2000 samples\n","Processing batch 126/400.\n","Completed 630/2000 samples\n","Processing batch 127/400.\n","Completed 635/2000 samples\n","Processing batch 128/400.\n","Completed 640/2000 samples\n","Processing batch 129/400.\n","Completed 645/2000 samples\n","Processing batch 130/400.\n","Completed 650/2000 samples\n","Processing batch 131/400.\n","Completed 655/2000 samples\n","Processing batch 132/400.\n","Completed 660/2000 samples\n","Processing batch 133/400.\n","Completed 665/2000 samples\n","Processing batch 134/400.\n","Completed 670/2000 samples\n","Processing batch 135/400.\n","Completed 675/2000 samples\n","Processing batch 136/400.\n","Completed 680/2000 samples\n","Processing batch 137/400.\n","Completed 685/2000 samples\n","Processing batch 138/400.\n","Completed 690/2000 samples\n","Processing batch 139/400.\n","Completed 695/2000 samples\n","Processing batch 140/400.\n","Completed 700/2000 samples\n","Processing batch 141/400.\n","Completed 705/2000 samples\n","Processing batch 142/400.\n","Completed 710/2000 samples\n","Processing batch 143/400.\n","Completed 715/2000 samples\n","Processing batch 144/400.\n","Completed 720/2000 samples\n","Processing batch 145/400.\n","Completed 725/2000 samples\n","Processing batch 146/400.\n","Completed 730/2000 samples\n","Processing batch 147/400.\n","Completed 735/2000 samples\n","Processing batch 148/400.\n","Completed 740/2000 samples\n","Processing batch 149/400.\n","Completed 745/2000 samples\n","Processing batch 150/400.\n","Completed 750/2000 samples\n","Processing batch 151/400.\n","Completed 755/2000 samples\n","Processing batch 152/400.\n","Completed 760/2000 samples\n","Processing batch 153/400.\n","Completed 765/2000 samples\n","Processing batch 154/400.\n","Completed 770/2000 samples\n","Processing batch 155/400.\n","Completed 775/2000 samples\n","Processing batch 156/400.\n","Completed 780/2000 samples\n","Processing batch 157/400.\n","Completed 785/2000 samples\n","Processing batch 158/400.\n","Completed 790/2000 samples\n","Processing batch 159/400.\n","Completed 795/2000 samples\n","Processing batch 160/400.\n","Completed 800/2000 samples\n","Processing batch 161/400.\n","Completed 805/2000 samples\n","Processing batch 162/400.\n","Completed 810/2000 samples\n","Processing batch 163/400.\n","Completed 815/2000 samples\n","Processing batch 164/400.\n","Completed 820/2000 samples\n","Processing batch 165/400.\n","Completed 825/2000 samples\n","Processing batch 166/400.\n","Completed 830/2000 samples\n","Processing batch 167/400.\n","Completed 835/2000 samples\n","Processing batch 168/400.\n","Completed 840/2000 samples\n","Processing batch 169/400.\n","Completed 845/2000 samples\n","Processing batch 170/400.\n","Completed 850/2000 samples\n","Processing batch 171/400.\n","Completed 855/2000 samples\n","Processing batch 172/400.\n","Completed 860/2000 samples\n","Processing batch 173/400.\n","Completed 865/2000 samples\n","Processing batch 174/400.\n","Completed 870/2000 samples\n","Processing batch 175/400.\n","Completed 875/2000 samples\n","Processing batch 176/400.\n","Completed 880/2000 samples\n","Processing batch 177/400.\n","Completed 885/2000 samples\n","Processing batch 178/400.\n","Completed 890/2000 samples\n","Processing batch 179/400.\n","Completed 895/2000 samples\n","Processing batch 180/400.\n","Completed 900/2000 samples\n","Processing batch 181/400.\n","Completed 905/2000 samples\n","Processing batch 182/400.\n","Completed 910/2000 samples\n","Processing batch 183/400.\n","Completed 915/2000 samples\n","Processing batch 184/400.\n","Completed 920/2000 samples\n","Processing batch 185/400.\n","Completed 925/2000 samples\n","Processing batch 186/400.\n","Completed 930/2000 samples\n","Processing batch 187/400.\n","Completed 935/2000 samples\n","Processing batch 188/400.\n","Completed 940/2000 samples\n","Processing batch 189/400.\n","Completed 945/2000 samples\n","Processing batch 190/400.\n","Completed 950/2000 samples\n","Processing batch 191/400.\n","Completed 955/2000 samples\n","Processing batch 192/400.\n","Completed 960/2000 samples\n","Processing batch 193/400.\n","Completed 965/2000 samples\n","Processing batch 194/400.\n","Completed 970/2000 samples\n","Processing batch 195/400.\n","Completed 975/2000 samples\n","Processing batch 196/400.\n","Completed 980/2000 samples\n","Processing batch 197/400.\n","Completed 985/2000 samples\n","Processing batch 198/400.\n","Completed 990/2000 samples\n","Processing batch 199/400.\n","Completed 995/2000 samples\n","Processing batch 200/400.\n","Completed 1000/2000 samples\n","Processing batch 201/400.\n","Completed 1005/2000 samples\n","Processing batch 202/400.\n","Completed 1010/2000 samples\n","Processing batch 203/400.\n","Completed 1015/2000 samples\n","Processing batch 204/400.\n","Completed 1020/2000 samples\n","Processing batch 205/400.\n","Completed 1025/2000 samples\n","Processing batch 206/400.\n","Completed 1030/2000 samples\n","Processing batch 207/400.\n","Completed 1035/2000 samples\n","Processing batch 208/400.\n","Completed 1040/2000 samples\n","Processing batch 209/400.\n","Completed 1045/2000 samples\n","Processing batch 210/400.\n","Completed 1050/2000 samples\n","Processing batch 211/400.\n","Completed 1055/2000 samples\n","Processing batch 212/400.\n","Completed 1060/2000 samples\n","Processing batch 213/400.\n","Completed 1065/2000 samples\n","Processing batch 214/400.\n","Completed 1070/2000 samples\n","Processing batch 215/400.\n","Completed 1075/2000 samples\n","Processing batch 216/400.\n","Completed 1080/2000 samples\n","Processing batch 217/400.\n","Completed 1085/2000 samples\n","Processing batch 218/400.\n","Completed 1090/2000 samples\n","Processing batch 219/400.\n","Completed 1095/2000 samples\n","Processing batch 220/400.\n","Completed 1100/2000 samples\n","Processing batch 221/400.\n","Completed 1105/2000 samples\n","Processing batch 222/400.\n","Completed 1110/2000 samples\n","Processing batch 223/400.\n","Completed 1115/2000 samples\n","Processing batch 224/400.\n","Completed 1120/2000 samples\n","Processing batch 225/400.\n","Completed 1125/2000 samples\n","Processing batch 226/400.\n","Completed 1130/2000 samples\n","Processing batch 227/400.\n","Completed 1135/2000 samples\n","Processing batch 228/400.\n","Completed 1140/2000 samples\n","Processing batch 229/400.\n","Completed 1145/2000 samples\n","Processing batch 230/400.\n","Completed 1150/2000 samples\n","Processing batch 231/400.\n","Completed 1155/2000 samples\n","Processing batch 232/400.\n","Completed 1160/2000 samples\n","Processing batch 233/400.\n","Completed 1165/2000 samples\n","Processing batch 234/400.\n","Completed 1170/2000 samples\n","Processing batch 235/400.\n","Completed 1175/2000 samples\n","Processing batch 236/400.\n","Completed 1180/2000 samples\n","Processing batch 237/400.\n","Completed 1185/2000 samples\n","Processing batch 238/400.\n","Completed 1190/2000 samples\n","Processing batch 239/400.\n","Completed 1195/2000 samples\n","Processing batch 240/400.\n","Completed 1200/2000 samples\n","Processing batch 241/400.\n","Completed 1205/2000 samples\n","Processing batch 242/400.\n","Completed 1210/2000 samples\n","Processing batch 243/400.\n","Completed 1215/2000 samples\n","Processing batch 244/400.\n","Completed 1220/2000 samples\n","Processing batch 245/400.\n","Completed 1225/2000 samples\n","Processing batch 246/400.\n","Completed 1230/2000 samples\n","Processing batch 247/400.\n","Completed 1235/2000 samples\n","Processing batch 248/400.\n","Completed 1240/2000 samples\n","Processing batch 249/400.\n","Completed 1245/2000 samples\n","Processing batch 250/400.\n","Completed 1250/2000 samples\n","Processing batch 251/400.\n","Completed 1255/2000 samples\n","Processing batch 252/400.\n","Completed 1260/2000 samples\n","Processing batch 253/400.\n","Completed 1265/2000 samples\n","Processing batch 254/400.\n","Completed 1270/2000 samples\n","Processing batch 255/400.\n","Completed 1275/2000 samples\n","Processing batch 256/400.\n","Completed 1280/2000 samples\n","Processing batch 257/400.\n","Completed 1285/2000 samples\n","Processing batch 258/400.\n","Completed 1290/2000 samples\n","Processing batch 259/400.\n","Completed 1295/2000 samples\n","Processing batch 260/400.\n","Completed 1300/2000 samples\n","Processing batch 261/400.\n","Completed 1305/2000 samples\n","Processing batch 262/400.\n","Completed 1310/2000 samples\n","Processing batch 263/400.\n","Completed 1315/2000 samples\n","Processing batch 264/400.\n","Completed 1320/2000 samples\n","Processing batch 265/400.\n","Completed 1325/2000 samples\n","Processing batch 266/400.\n","Completed 1330/2000 samples\n","Processing batch 267/400.\n","Completed 1335/2000 samples\n","Processing batch 268/400.\n","Completed 1340/2000 samples\n","Processing batch 269/400.\n","Completed 1345/2000 samples\n","Processing batch 270/400.\n","Completed 1350/2000 samples\n","Processing batch 271/400.\n","Completed 1355/2000 samples\n","Processing batch 272/400.\n","Completed 1360/2000 samples\n","Processing batch 273/400.\n","Completed 1365/2000 samples\n","Processing batch 274/400.\n","Completed 1370/2000 samples\n","Processing batch 275/400.\n","Completed 1375/2000 samples\n","Processing batch 276/400.\n","Completed 1380/2000 samples\n","Processing batch 277/400.\n","Completed 1385/2000 samples\n","Processing batch 278/400.\n","Completed 1390/2000 samples\n","Processing batch 279/400.\n","Completed 1395/2000 samples\n","Processing batch 280/400.\n","Completed 1400/2000 samples\n","Processing batch 281/400.\n","Completed 1405/2000 samples\n","Processing batch 282/400.\n","Completed 1410/2000 samples\n","Processing batch 283/400.\n","Completed 1415/2000 samples\n","Processing batch 284/400.\n","Completed 1420/2000 samples\n","Processing batch 285/400.\n","Completed 1425/2000 samples\n","Processing batch 286/400.\n","Completed 1430/2000 samples\n","Processing batch 287/400.\n","Completed 1435/2000 samples\n","Processing batch 288/400.\n","Completed 1440/2000 samples\n","Processing batch 289/400.\n","Completed 1445/2000 samples\n","Processing batch 290/400.\n","Completed 1450/2000 samples\n","Processing batch 291/400.\n","Completed 1455/2000 samples\n","Processing batch 292/400.\n","Completed 1460/2000 samples\n","Processing batch 293/400.\n","Completed 1465/2000 samples\n","Processing batch 294/400.\n","Completed 1470/2000 samples\n","Processing batch 295/400.\n","Completed 1475/2000 samples\n","Processing batch 296/400.\n","Completed 1480/2000 samples\n","Processing batch 297/400.\n","Completed 1485/2000 samples\n","Processing batch 298/400.\n","Completed 1490/2000 samples\n","Processing batch 299/400.\n","Completed 1495/2000 samples\n","Processing batch 300/400.\n","Completed 1500/2000 samples\n","Processing batch 301/400.\n","Completed 1505/2000 samples\n","Processing batch 302/400.\n","Completed 1510/2000 samples\n","Processing batch 303/400.\n","Completed 1515/2000 samples\n","Processing batch 304/400.\n","Completed 1520/2000 samples\n","Processing batch 305/400.\n","Completed 1525/2000 samples\n","Processing batch 306/400.\n","Completed 1530/2000 samples\n","Processing batch 307/400.\n","Completed 1535/2000 samples\n","Processing batch 308/400.\n","Completed 1540/2000 samples\n","Processing batch 309/400.\n","Completed 1545/2000 samples\n","Processing batch 310/400.\n","Completed 1550/2000 samples\n","Processing batch 311/400.\n","Completed 1555/2000 samples\n","Processing batch 312/400.\n","Completed 1560/2000 samples\n","Processing batch 313/400.\n","Completed 1565/2000 samples\n","Processing batch 314/400.\n","Completed 1570/2000 samples\n","Processing batch 315/400.\n","Completed 1575/2000 samples\n","Processing batch 316/400.\n","Completed 1580/2000 samples\n","Processing batch 317/400.\n","Completed 1585/2000 samples\n","Processing batch 318/400.\n","Completed 1590/2000 samples\n","Processing batch 319/400.\n","Completed 1595/2000 samples\n","Processing batch 320/400.\n","Completed 1600/2000 samples\n","Processing batch 321/400.\n","Completed 1605/2000 samples\n","Processing batch 322/400.\n","Completed 1610/2000 samples\n","Processing batch 323/400.\n","Completed 1615/2000 samples\n","Processing batch 324/400.\n","Completed 1620/2000 samples\n","Processing batch 325/400.\n","Completed 1625/2000 samples\n","Processing batch 326/400.\n","Completed 1630/2000 samples\n","Processing batch 327/400.\n","Completed 1635/2000 samples\n","Processing batch 328/400.\n","Completed 1640/2000 samples\n","Processing batch 329/400.\n","Completed 1645/2000 samples\n","Processing batch 330/400.\n","Completed 1650/2000 samples\n","Processing batch 331/400.\n","Completed 1655/2000 samples\n","Processing batch 332/400.\n","Completed 1660/2000 samples\n","Processing batch 333/400.\n","Completed 1665/2000 samples\n","Processing batch 334/400.\n","Completed 1670/2000 samples\n","Processing batch 335/400.\n","Completed 1675/2000 samples\n","Processing batch 336/400.\n","Completed 1680/2000 samples\n","Processing batch 337/400.\n","Completed 1685/2000 samples\n","Processing batch 338/400.\n","Completed 1690/2000 samples\n","Processing batch 339/400.\n","Completed 1695/2000 samples\n","Processing batch 340/400.\n","Completed 1700/2000 samples\n","Processing batch 341/400.\n","Completed 1705/2000 samples\n","Processing batch 342/400.\n","Completed 1710/2000 samples\n","Processing batch 343/400.\n","Completed 1715/2000 samples\n","Processing batch 344/400.\n","Completed 1720/2000 samples\n","Processing batch 345/400.\n","Completed 1725/2000 samples\n","Processing batch 346/400.\n","Completed 1730/2000 samples\n","Processing batch 347/400.\n","Completed 1735/2000 samples\n","Processing batch 348/400.\n","Completed 1740/2000 samples\n","Processing batch 349/400.\n","Completed 1745/2000 samples\n","Processing batch 350/400.\n","Completed 1750/2000 samples\n","Processing batch 351/400.\n","Completed 1755/2000 samples\n","Processing batch 352/400.\n","Completed 1760/2000 samples\n","Processing batch 353/400.\n","Completed 1765/2000 samples\n","Processing batch 354/400.\n","Completed 1770/2000 samples\n","Processing batch 355/400.\n","Completed 1775/2000 samples\n","Processing batch 356/400.\n","Completed 1780/2000 samples\n","Processing batch 357/400.\n","Completed 1785/2000 samples\n","Processing batch 358/400.\n","Completed 1790/2000 samples\n","Processing batch 359/400.\n","Completed 1795/2000 samples\n","Processing batch 360/400.\n","Completed 1800/2000 samples\n","Processing batch 361/400.\n","Completed 1805/2000 samples\n","Processing batch 362/400.\n","Completed 1810/2000 samples\n","Processing batch 363/400.\n","Completed 1815/2000 samples\n","Processing batch 364/400.\n","Completed 1820/2000 samples\n","Processing batch 365/400.\n","Completed 1825/2000 samples\n","Processing batch 366/400.\n","Completed 1830/2000 samples\n","Processing batch 367/400.\n","Completed 1835/2000 samples\n","Processing batch 368/400.\n","Completed 1840/2000 samples\n","Processing batch 369/400.\n","Completed 1845/2000 samples\n","Processing batch 370/400.\n","Completed 1850/2000 samples\n","Processing batch 371/400.\n","Completed 1855/2000 samples\n","Processing batch 372/400.\n","Completed 1860/2000 samples\n","Processing batch 373/400.\n","Completed 1865/2000 samples\n","Processing batch 374/400.\n","Completed 1870/2000 samples\n","Processing batch 375/400.\n","Completed 1875/2000 samples\n","Processing batch 376/400.\n","Completed 1880/2000 samples\n","Processing batch 377/400.\n","Completed 1885/2000 samples\n","Processing batch 378/400.\n","Completed 1890/2000 samples\n","Processing batch 379/400.\n","Completed 1895/2000 samples\n","Processing batch 380/400.\n","Completed 1900/2000 samples\n","Processing batch 381/400.\n","Completed 1905/2000 samples\n","Processing batch 382/400.\n","Completed 1910/2000 samples\n","Processing batch 383/400.\n","Completed 1915/2000 samples\n","Processing batch 384/400.\n","Completed 1920/2000 samples\n","Processing batch 385/400.\n","Completed 1925/2000 samples\n","Processing batch 386/400.\n","Completed 1930/2000 samples\n","Processing batch 387/400.\n","Completed 1935/2000 samples\n","Processing batch 388/400.\n","Completed 1940/2000 samples\n","Processing batch 389/400.\n","Completed 1945/2000 samples\n","Processing batch 390/400.\n","Completed 1950/2000 samples\n","Processing batch 391/400.\n","Completed 1955/2000 samples\n","Processing batch 392/400.\n","Completed 1960/2000 samples\n","Processing batch 393/400.\n","Completed 1965/2000 samples\n","Processing batch 394/400.\n","Completed 1970/2000 samples\n","Processing batch 395/400.\n","Completed 1975/2000 samples\n","Processing batch 396/400.\n","Completed 1980/2000 samples\n","Processing batch 397/400.\n","Completed 1985/2000 samples\n","Processing batch 398/400.\n","Completed 1990/2000 samples\n","Processing batch 399/400.\n","Completed 1995/2000 samples\n","Processing batch 400/400.\n","Completed 2000/2000 samples\n","\n","🎉 CoT generation completed successfully!\n","📁 Output file: ./data/train_with_cot.json\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_df8385a7-22d5-484d-8ed5-843969cffcb2\", \"train_with_cot.json\", 4277626)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_80a8d001-a51b-46f6-af01-68b3d98feda0\", \"train_with_cot_cot_stats.json\", 249)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Files downloaded successfully!\n"]}]}]}