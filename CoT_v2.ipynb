{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-fbKFPvxUug"
      },
      "source": [
        "# **Chain-of-Thought (CoT) Generation**\n",
        "\n",
        "This script generates Chain-of-Thought reasoning for medical chatbot training data.\n",
        "It supports multiple AI models and includes robust error handling, batching, and retry logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uw2po7HAxrj8"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zwqqxdo8zoFO"
      },
      "outputs": [],
      "source": [
        "def install_packages():\n",
        "    packages = [\n",
        "        'backoff',\n",
        "        'nest-asyncio',\n",
        "        'aiohttp',\n",
        "        'openai',\n",
        "        'transformers',\n",
        "        'torch',\n",
        "        'tqdm'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "            print(f\"Installed {package}\")\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"Failed to install {package}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX42jqyIzr-8",
        "outputId": "051d26ef-e8ff-4063-ef80-0252a3d08517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Installed backoff\n",
            "‚úì Installed nest-asyncio\n",
            "‚úì Installed aiohttp\n",
            "‚úì Installed openai\n",
            "‚úì Installed transformers\n",
            "‚úì Installed torch\n",
            "‚úì Installed tqdm\n"
          ]
        }
      ],
      "source": [
        "install_packages()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2XTrmrWrzqxH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import backoff\n",
        "from tqdm.asyncio import tqdm\n",
        "import nest_asyncio\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0zstKaa3z0hU"
      },
      "outputs": [],
      "source": [
        "# For Colab compatibility.\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lWtMDfeX0947"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CoTConfig:\n",
        "    model_name: str = \"gpt-3.5-turbo\"\n",
        "    api_key: Optional[str] = None\n",
        "    batch_size: int = 5\n",
        "    max_retries: int = 3\n",
        "    retry_delay: float = 1.0\n",
        "    max_tokens: int = 512\n",
        "    temperature: float = 0.7\n",
        "    concurrent_requests: int = 3\n",
        "    rate_limit_delay: float = 1.0\n",
        "\n",
        "    openai_base_url: str = \"https://api.openai.com/v1\"\n",
        "    anthropic_base_url: str = \"https://api.anthropic.com/v1\"\n",
        "\n",
        "class CoTGenerator:\n",
        "    def __init__(self, config: CoTConfig):\n",
        "        self.config = config\n",
        "        self.session = None\n",
        "        self.semaphore = asyncio.Semaphore(config.concurrent_requests)\n",
        "        self.setup_apis()\n",
        "\n",
        "    def setup_apis(self):\n",
        "\n",
        "        if self.config.model_name.startswith(('gpt-', 'text-davinci')):\n",
        "            if not self.config.api_key:\n",
        "                self.config.api_key = os.getenv('OPENAI_API_KEY')\n",
        "            if not self.config.api_key:\n",
        "                logger.warning(\"OpenAI API key not found. Please set it manually.\")\n",
        "            else:\n",
        "                openai.api_key = self.config.api_key\n",
        "\n",
        "        elif self.config.model_name.startswith('claude'):\n",
        "            if not self.config.api_key:\n",
        "                self.config.api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "            if not self.config.api_key:\n",
        "                logger.warning(\"Anthropic API key not found.\")\n",
        "\n",
        "    def create_cot_prompt(self, instruction: str, answer: str) -> str:\n",
        "        \"\"\"Create prompt for generating CoT reasoning\"\"\"\n",
        "        prompt = f\"\"\"You are a medical expert assistant. For the given medical question and answer, provide detailed Chain-of-Thought reasoning that explains the step-by-step thinking process leading to the answer.\n",
        "\n",
        "Your reasoning should:\n",
        "1. Break down the problem systematically\n",
        "2. Identify key medical concepts and symptoms\n",
        "3. Consider differential diagnoses where applicable\n",
        "4. Explain the logical progression of thoughts\n",
        "5. Be clear, educational, and medically accurate\n",
        "\n",
        "Question: {instruction}\n",
        "\n",
        "Expected Answer: {answer}\n",
        "\n",
        "Please provide the Chain-of-Thought reasoning that leads to this answer. Start your response with \"Chain-of-Thought:\" and then provide the detailed reasoning.\n",
        "\n",
        "Chain-of-Thought:\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    @backoff.on_exception(\n",
        "        backoff.expo,\n",
        "        (Exception,),\n",
        "        max_tries=3\n",
        "    )\n",
        "    async def generate_cot_openai(self, prompt: str) -> str:\n",
        "        try:\n",
        "            client = openai.AsyncOpenAI(api_key=self.config.api_key)\n",
        "\n",
        "            response = await client.chat.completions.create(\n",
        "                model=self.config.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=self.config.max_tokens,\n",
        "                temperature=self.config.temperature,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"OpenAI API error: {e}\")\n",
        "            raise\n",
        "\n",
        "    @backoff.on_exception(\n",
        "        backoff.expo,\n",
        "        (aiohttp.ClientError, asyncio.TimeoutError),\n",
        "        max_tries=3\n",
        "    )\n",
        "    async def generate_cot_anthropic(self, prompt: str) -> str:\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"x-api-key\": self.config.api_key,\n",
        "            \"anthropic-version\": \"2023-06-01\"\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "            \"model\": self.config.model_name,\n",
        "            \"max_tokens\": self.config.max_tokens,\n",
        "            \"temperature\": self.config.temperature,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        }\n",
        "\n",
        "        async with self.session.post(\n",
        "            f\"{self.config.anthropic_base_url}/messages\",\n",
        "            headers=headers,\n",
        "            json=payload,\n",
        "            timeout=30\n",
        "        ) as response:\n",
        "            if response.status == 200:\n",
        "                result = await response.json()\n",
        "                return result[\"content\"][0][\"text\"].strip()\n",
        "            else:\n",
        "                error_text = await response.text()\n",
        "                raise aiohttp.ClientError(f\"Anthropic API error: {response.status} - {error_text}\")\n",
        "\n",
        "    async def generate_single_cot(self, sample: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        async with self.semaphore:\n",
        "            try:\n",
        "                instruction = sample.get(\"instruction\", \"\")\n",
        "                answer = sample.get(\"output\", \"\")\n",
        "\n",
        "                if not instruction or not answer:\n",
        "                    logger.warning(\"Skipping sample with missing instruction or answer.\")\n",
        "                    return {**sample, \"cot_reasoning\": \"\"}\n",
        "\n",
        "                prompt = self.create_cot_prompt(instruction, answer)\n",
        "\n",
        "                # Route to appropriate model for CoT generation.\n",
        "                if self.config.model_name.startswith(('gpt-', 'text-davinci')):\n",
        "                    cot_reasoning = await self.generate_cot_openai(prompt)\n",
        "                elif self.config.model_name.startswith('claude'):\n",
        "                    cot_reasoning = await self.generate_cot_anthropic(prompt)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported model: {self.config.model_name}\")\n",
        "\n",
        "                # Cleaning up the reasoning.\n",
        "                if cot_reasoning.startswith(\"Chain-of-Thought:\"):\n",
        "                    cot_reasoning = cot_reasoning[len(\"Chain-of-Thought:\"):].strip()\n",
        "\n",
        "                # Add rate limit delay.\n",
        "                await asyncio.sleep(self.config.rate_limit_delay)\n",
        "\n",
        "                return {**sample, \"cot_reasoning\": cot_reasoning}\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error generating CoT for sample: {e}\")\n",
        "                return {**sample, \"cot_reasoning\": f\"Error: {str(e)}\"}\n",
        "\n",
        "    async def process_batch(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        tasks = [self.generate_single_cot(sample) for sample in batch]\n",
        "        return await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    async def generate_cot_for_dataset(self, input_file: str, output_file: str):\n",
        "        logger.info(f\"Loading dataset from {input_file}\")\n",
        "\n",
        "        # Loading the dataset.\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            dataset = json.load(f)\n",
        "\n",
        "        logger.info(f\"Loaded {len(dataset)} samples\")\n",
        "\n",
        "        # Creating batches.\n",
        "        batches = [\n",
        "            dataset[i:i + self.config.batch_size]\n",
        "            for i in range(0, len(dataset), self.config.batch_size)\n",
        "        ]\n",
        "\n",
        "        logger.info(f\"Processing {len(batches)} batches of size {self.config.batch_size}\")\n",
        "\n",
        "        # Setting up session for API calls.\n",
        "        connector = aiohttp.TCPConnector(limit=self.config.concurrent_requests)\n",
        "        timeout = aiohttp.ClientTimeout(total=60)\n",
        "        self.session = aiohttp.ClientSession(connector=connector, timeout=timeout)\n",
        "\n",
        "        try:\n",
        "            results = []\n",
        "\n",
        "            # Processing batches.\n",
        "            for i, batch in enumerate(batches):\n",
        "                print(f\"Processing batch {i+1}/{len(batches)}.\")\n",
        "\n",
        "                batch_results = await self.process_batch(batch)\n",
        "\n",
        "                # Handling exceptions in results.\n",
        "                for result in batch_results:\n",
        "                    if isinstance(result, Exception):\n",
        "                        logger.error(f\"Batch processing error: {result}\")\n",
        "                        results.append({\"error\": str(result)})\n",
        "                    else:\n",
        "                        results.append(result)\n",
        "\n",
        "                # Save results every 10 batches.\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    self.save_intermediate_results(results, output_file)\n",
        "\n",
        "                print(f\"Completed {len(results)}/{len(dataset)} samples\")\n",
        "\n",
        "            # Final results.\n",
        "            logger.info(f\"Saving final results to {output_file}\")\n",
        "            os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
        "\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            # Statistics.\n",
        "            self.generate_statistics(results, input_file, output_file)\n",
        "\n",
        "        finally:\n",
        "            if self.session:\n",
        "                await self.session.close()\n",
        "\n",
        "    def save_intermediate_results(self, results: List[Dict[str, Any]], output_file: str):\n",
        "        intermediate_file = output_file.replace('.json', '_intermediate.json')\n",
        "        with open(intermediate_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        logger.info(f\"Saved intermediate results: {len(results)} samples.\")\n",
        "\n",
        "    def generate_statistics(self, results: List[Dict[str, Any]], input_file: str, output_file: str):\n",
        "        stats = {\n",
        "            \"total_samples\": len(results),\n",
        "            \"successful_generations\": sum(1 for r in results if r.get(\"cot_reasoning\") and not r.get(\"cot_reasoning\", \"\").startswith(\"Error:\")),\n",
        "            \"failed_generations\": sum(1 for r in results if r.get(\"cot_reasoning\", \"\").startswith(\"Error:\") or not r.get(\"cot_reasoning\")),\n",
        "            \"average_cot_length\": 0,\n",
        "            \"model_config\": {\n",
        "                \"model_name\": self.config.model_name,\n",
        "                \"max_tokens\": self.config.max_tokens,\n",
        "                \"temperature\": self.config.temperature,\n",
        "                \"batch_size\": self.config.batch_size\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Average CoT length.\n",
        "        valid_cots = [r.get(\"cot_reasoning\", \"\") for r in results if r.get(\"cot_reasoning\") and not r.get(\"cot_reasoning\", \"\").startswith(\"Error:\")]\n",
        "        if valid_cots:\n",
        "            stats[\"average_cot_length\"] = sum(len(cot.split()) for cot in valid_cots) / len(valid_cots)\n",
        "\n",
        "        # Statistics.\n",
        "        stats_file = output_file.replace('.json', '_cot_stats.json')\n",
        "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"CoT generation statistics saved to {stats_file}\")\n",
        "        logger.info(f\"Success rate: {stats['successful_generations']}/{stats['total_samples']} ({stats['successful_generations']/stats['total_samples']*100:.1f}%)\")\n",
        "\n",
        "\n",
        "def setup_colab_environment():\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "        print(\"Running in Google Colab environment.\")\n",
        "\n",
        "        # Google Drive.\n",
        "        from google.colab import drive\n",
        "        print(\"Mounting Google Drive.\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive successful.\")\n",
        "\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "        print(\"Running in local environment.\")\n",
        "\n",
        "    return IN_COLAB\n",
        "\n",
        "\n",
        "def find_train_json():\n",
        "    search_paths = [\n",
        "        '/content/drive/MyDrive/train.json',\n",
        "        '/content/drive/My Drive/train.json',\n",
        "        '/content/drive/MyDrive/data/train.json',\n",
        "        '/content/drive/My Drive/data/train.json',\n",
        "        '/content/train.json',\n",
        "        './train.json',\n",
        "        './data/train.json'\n",
        "    ]\n",
        "\n",
        "    for path in search_paths:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"Found train.json at: {path}\")\n",
        "            return path\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# Function for CoT generation.\n",
        "async def run_cot_generation():\n",
        "    print(\"Chain-of-Thought Generation for our Medical Chatbot\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Environment.\n",
        "    IN_COLAB = setup_colab_environment()\n",
        "\n",
        "    train_file = find_train_json()\n",
        "\n",
        "    if not train_file:\n",
        "        print(\"Train.json file not found.\")\n",
        "        print(\"Please provide the path to the train.json file:\")\n",
        "        train_file = input(\"Train file path: \").strip()\n",
        "\n",
        "        if not os.path.exists(train_file):\n",
        "            print(f\"File not found: {train_file}\")\n",
        "            return\n",
        "\n",
        "    # Configuration\n",
        "    print(\"\\nüìã Configuration:\")\n",
        "    print(\"Available models:\")\n",
        "    print(\"  1. gpt-3.5-turbo (recommended, cheaper)\")\n",
        "    print(\"  2. gpt-4 (more expensive, better quality)\")\n",
        "    print(\"  3. claude-3-sonnet-20240229\")\n",
        "\n",
        "    model_choice = input(\"Choose model (1/2/3) or enter custom name: \").strip()\n",
        "\n",
        "    if model_choice == \"1\":\n",
        "        model_name = \"gpt-3.5-turbo\"\n",
        "    elif model_choice == \"2\":\n",
        "        model_name = \"gpt-4\"\n",
        "    elif model_choice == \"3\":\n",
        "        model_name = \"claude-3-sonnet-20240229\"\n",
        "    elif model_choice:\n",
        "        model_name = model_choice\n",
        "    else:\n",
        "        model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "    print(f\"‚úì Selected model: {model_name}\")\n",
        "\n",
        "    # API Key\n",
        "    api_key = None\n",
        "    if model_name.startswith('gpt'):\n",
        "        print(\"\\nüîë API Key Setup:\")\n",
        "        print(\"You need an OpenAI API key for GPT models.\")\n",
        "        print(\"Get one at: https://platform.openai.com/api-keys\")\n",
        "\n",
        "        api_key = input(\"Enter your OpenAI API Key (starts with sk-): \").strip()\n",
        "        if not api_key:\n",
        "            api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "        if not api_key:\n",
        "            print(\"‚ùå Error: OpenAI API key is required for GPT models!\")\n",
        "            return\n",
        "        elif not api_key.startswith('sk-'):\n",
        "            print(\"‚ö†Ô∏è Warning: OpenAI API keys usually start with 'sk-'\")\n",
        "            confirm = input(\"Continue anyway? (y/n): \").strip().lower()\n",
        "            if confirm != 'y':\n",
        "                return\n",
        "\n",
        "    elif model_name.startswith('claude'):\n",
        "        print(\"\\nüîë API Key Setup:\")\n",
        "        print(\"You need an Anthropic API key for Claude models.\")\n",
        "        print(\"Get one at: https://console.anthropic.com/\")\n",
        "\n",
        "        api_key = input(\"Enter your Anthropic API Key: \").strip()\n",
        "        if not api_key:\n",
        "            api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "\n",
        "        if not api_key:\n",
        "            print(\"‚ùå Error: Anthropic API key is required for Claude models!\")\n",
        "            return\n",
        "\n",
        "    print(\"‚úÖ API key configured successfully!\")\n",
        "\n",
        "    # Batch configuration\n",
        "    print(\"\\n‚öôÔ∏è Processing Configuration:\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            batch_input = input(\"Batch size (5 recommended): \").strip()\n",
        "            batch_size = int(batch_input) if batch_input else 5\n",
        "            if batch_size > 0:\n",
        "                break\n",
        "            else:\n",
        "                print(\"‚ùå Batch size must be positive!\")\n",
        "        except ValueError:\n",
        "            print(\"‚ùå Please enter a valid number for batch size!\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            tokens_input = input(\"Max tokens for CoT (512 recommended): \").strip()\n",
        "            max_tokens = int(tokens_input) if tokens_input else 512\n",
        "            if max_tokens > 0:\n",
        "                break\n",
        "            else:\n",
        "                print(\"‚ùå Max tokens must be positive!\")\n",
        "        except ValueError:\n",
        "            print(\"‚ùå Please enter a valid number for max tokens!\")\n",
        "\n",
        "    # Output file\n",
        "    output_dir = input(\"Output directory (./data): \").strip() or \"./data\"\n",
        "    output_file = os.path.join(output_dir, \"train_with_cot.json\")\n",
        "\n",
        "    # Preview the dataset\n",
        "    print(f\"\\nüëÄ Previewing dataset: {train_file}\")\n",
        "    try:\n",
        "        with open(train_file, 'r', encoding='utf-8') as f:\n",
        "            dataset = json.load(f)\n",
        "\n",
        "        print(f\"üìä Dataset size: {len(dataset)} samples\")\n",
        "        if dataset:\n",
        "            sample = dataset[0]\n",
        "            print(f\"üìù Sample keys: {list(sample.keys())}\")\n",
        "            print(f\"üìè Sample instruction length: {len(sample.get('instruction', '').split())} words\")\n",
        "            print(f\"üìè Sample output length: {len(sample.get('output', '').split())} words\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    # Estimate costs (rough)\n",
        "    total_samples = len(dataset)\n",
        "    estimated_tokens_per_sample = 300  # Rough estimate\n",
        "    total_tokens = total_samples * estimated_tokens_per_sample\n",
        "\n",
        "    if model_name == \"gpt-3.5-turbo\":\n",
        "        cost_per_1k = 0.0015  # $0.0015 per 1K tokens\n",
        "    elif model_name == \"gpt-4\":\n",
        "        cost_per_1k = 0.03    # $0.03 per 1K tokens\n",
        "    else:\n",
        "        cost_per_1k = 0.01    # Rough estimate for other models\n",
        "\n",
        "    estimated_cost = (total_tokens / 1000) * cost_per_1k\n",
        "\n",
        "    print(f\"\\nüí∞ Estimated cost: ${estimated_cost:.2f}\")\n",
        "    print(f\"‚è±Ô∏è Estimated time: {(total_samples * 2) / 60:.1f} minutes\")\n",
        "\n",
        "    confirm = input(\"\\nProceed with CoT generation? (y/n): \").strip().lower()\n",
        "    if confirm != 'y':\n",
        "        print(\"‚ùå Generation cancelled.\")\n",
        "        return\n",
        "\n",
        "    # Create configuration\n",
        "    config = CoTConfig(\n",
        "        model_name=model_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=batch_size,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        concurrent_requests=3,\n",
        "        rate_limit_delay=1.0\n",
        "    )\n",
        "\n",
        "    # Initialize generator\n",
        "    generator = CoTGenerator(config)\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nüöÄ Starting CoT generation...\")\n",
        "        await generator.generate_cot_for_dataset(train_file, output_file)\n",
        "\n",
        "        print(\"\\nüéâ CoT generation completed successfully!\")\n",
        "        print(f\"üìÅ Output file: {output_file}\")\n",
        "\n",
        "        # Download in Colab\n",
        "        if IN_COLAB:\n",
        "            try:\n",
        "                from google.colab import files\n",
        "                files.download(output_file)\n",
        "\n",
        "                # Also download stats file\n",
        "                stats_file = output_file.replace('.json', '_cot_stats.json')\n",
        "                if os.path.exists(stats_file):\n",
        "                    files.download(stats_file)\n",
        "\n",
        "                print(\"‚úÖ Files downloaded successfully!\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not download files: {e}\")\n",
        "                print(f\"Files are available at: {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during CoT generation: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONVvk8ZkzZRF",
        "outputId": "4cbb67ec-22ef-4e3b-83ce-2d4fb82b29e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chain-of-Thought Generation for Medical Chatbot\n",
            "==================================================\n",
            "Running in Google Colab environment\n",
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully!\n",
            "‚úì Found train.json at: /content/drive/MyDrive/train.json\n",
            "\n",
            "üìã Configuration:\n",
            "Available models:\n",
            "  1. gpt-3.5-turbo (recommended, cheaper)\n",
            "  2. gpt-4 (more expensive, better quality)\n",
            "  3. claude-3-sonnet-20240229\n",
            "Choose model (1/2/3) or enter custom name: 1\n",
            "‚úì Selected model: gpt-3.5-turbo\n",
            "\n",
            "üîë API Key Setup:\n",
            "You need an OpenAI API key for GPT models.\n",
            "Get one at: https://platform.openai.com/api-keys\n",
            "Enter your OpenAI API Key (starts with sk-): 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\n",
            "‚úÖ API key configured successfully!\n",
            "\n",
            "‚öôÔ∏è Processing Configuration:\n",
            "Batch size (5 recommended): 5\n",
            "Max tokens for CoT (512 recommended): 512\n",
            "Output directory (./data): \n",
            "\n",
            "üëÄ Previewing dataset: /content/drive/MyDrive/train.json\n",
            "üìä Dataset size: 124980 samples\n",
            "üìù Sample keys: ['instruction', 'input', 'output']\n",
            "üìè Sample instruction length: 37 words\n",
            "üìè Sample output length: 93 words\n",
            "\n",
            "üí∞ Estimated cost: $56.24\n",
            "‚è±Ô∏è Estimated time: 4166.0 minutes\n",
            "\n",
            "Proceed with CoT generation? (y/n): n\n",
            "‚ùå Generation cancelled.\n"
          ]
        }
      ],
      "source": [
        "# Main execution\n",
        "async def main():\n",
        "    await run_cot_generation()\n",
        "\n",
        "# For Colab execution\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
