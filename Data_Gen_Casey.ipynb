{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Trlon31t0OJ3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rIciuqeOQvhs"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"rlaif_generation.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def load_pubmedqa_labeled(path):\n",
        "    \"\"\"Load the PubMedQA dataset from a JSON file\"\"\"\n",
        "    try:\n",
        "        with open(path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        logger.info(f\"Successfully loaded PubMedQA dataset from {path}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading PubMedQA dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def setup_model(model_name, load_in_4bit=True):\n",
        "    \"\"\"Set up the tokenizer and model with specified configuration\"\"\"\n",
        "    try:\n",
        "        # Quantization parameters\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=load_in_4bit,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "\n",
        "        # Tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # Model with quantization\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Successfully loaded model: {model_name}\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error setting up model: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_rIKruPwRWrH"
      },
      "outputs": [],
      "source": [
        "def get_cot_template():\n",
        "    \"\"\"Return the chain-of-thought template\"\"\"\n",
        "    return \"\"\"Please analyze the following medical case step by step:\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Let's think through this step by step:\n",
        "\n",
        "1. First, identify the key information from the context:\n",
        "2. Then, analyze the specific question being asked:\n",
        "3. Next, consider the relevant medical concepts:\n",
        "4. After that, evaluate the possible answers:\n",
        "5. Finally, provide a comprehensive conclusion:\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def generate_answer(model, tokenizer, prompt, max_length=1024, max_new_tokens=512,\n",
        "                   temperature=0.7, num_beams=4):\n",
        "    \"\"\"Generate an answer using the model\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Checking if the input is too long\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        if input_length > max_length:\n",
        "            logger.warning(f\"Input too long: {input_length} tokens (max: {max_length})\")\n",
        "            return None\n",
        "\n",
        "        # Output\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.2,\n",
        "            length_penalty=1.5,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Decode output\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating answer: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qXj4GfmARbGj"
      },
      "outputs": [],
      "source": [
        "def process_generated_answer(answer):\n",
        "    \"\"\"Process the generated answer to extract the answer and chain of thought\"\"\"\n",
        "    if not answer:\n",
        "        return \"\", \"\"\n",
        "\n",
        "    if \"Answer:\" in answer:\n",
        "        cot_part = answer.split(\"Answer:\")[0].strip()\n",
        "        answer_part = answer.split(\"Answer:\")[1].strip()\n",
        "    else:\n",
        "        cot_part = \"\"\n",
        "        answer_part = answer.strip()\n",
        "\n",
        "    return answer_part, cot_part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yGei-tsERfAT"
      },
      "outputs": [],
      "source": [
        "def generate_rlaif_data(dataset_path=None, use_huggingface=True,\n",
        "                        model_name=\"microsoft/BioGPT-Large-PubMedQA\",\n",
        "                        output_file=\"rlaif_data.json\",\n",
        "                        save_interval=10, max_samples=None):\n",
        "    \"\"\"Generate RLAIF data from PubMedQA dataset\"\"\"\n",
        "\n",
        "    # Dataset\n",
        "    if use_huggingface:\n",
        "        try:\n",
        "            dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
        "            train_data = dataset[\"train\"]\n",
        "            logger.info(f\"Loaded dataset from Hugging Face: {len(train_data)} items\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading dataset from Hugging Face: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        if not dataset_path:\n",
        "            logger.error(\"Dataset path required when not using Hugging Face\")\n",
        "            return None\n",
        "        train_data = load_pubmedqa_labeled(dataset_path)\n",
        "        if not train_data:\n",
        "            return None\n",
        "\n",
        "    # Model and tokenizer\n",
        "    tokenizer, model = setup_model(model_name)\n",
        "    if not tokenizer or not model:\n",
        "        return None\n",
        "\n",
        "    #CoT\n",
        "    cot_template = get_cot_template()\n",
        "\n",
        "    # RLAIF data\n",
        "    rlaif_data = []\n",
        "\n",
        "    data_items = train_data[:max_samples] if max_samples else train_data\n",
        "\n",
        "    for idx, item in enumerate(tqdm(data_items, desc=\"Generating RLAIF data\")):\n",
        "        try:\n",
        "            if use_huggingface:\n",
        "                question = item[\"question\"]\n",
        "                context_data = item[\"context\"]\n",
        "                long_answer = item[\"long_answer\"]\n",
        "                contexts = context_data[\"contexts\"]\n",
        "                context = \" \".join(contexts)\n",
        "            else:\n",
        "                question = item.get(\"question\", \"\")\n",
        "                contexts = item.get(\"contexts\", [])\n",
        "                context = \" \".join(contexts)\n",
        "                long_answer = item.get(\"long_answer\", \"\")\n",
        "\n",
        "            # Format the prompt\n",
        "            prompt = cot_template.format(context=context, question=question)\n",
        "\n",
        "            # Generate an answer\n",
        "            answer = generate_answer(model, tokenizer, prompt)\n",
        "            if not answer:\n",
        "                continue\n",
        "\n",
        "            # Process the generated answer\n",
        "            answer_part, cot_part = process_generated_answer(answer)\n",
        "\n",
        "            # Create a data point\n",
        "            data_point = {\n",
        "                \"prompt\": prompt,\n",
        "                \"chosen\": {\n",
        "                    \"answer\": answer_part,\n",
        "                    \"chain_of_thought\": cot_part\n",
        "                },\n",
        "                \"rejected\": {\n",
        "                    \"answer\": long_answer,\n",
        "                    \"chain_of_thought\": \"\"\n",
        "                },\n",
        "                \"metadata\": {\n",
        "                    \"question\": question,\n",
        "                    \"context\": context,\n",
        "                    \"model\": model_name\n",
        "                }\n",
        "            }\n",
        "\n",
        "            rlaif_data.append(data_point)\n",
        "\n",
        "            # Save at intervals\n",
        "            if (idx + 1) % save_interval == 0:\n",
        "                with open(output_file, \"w\") as f:\n",
        "                    json.dump(rlaif_data, f, indent=2)\n",
        "                logger.info(f\"Saved {len(rlaif_data)} items to {output_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing item {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(rlaif_data, f, indent=2)\n",
        "    logger.info(f\"Finished generating RLAIF data. Total items: {len(rlaif_data)}\")\n",
        "\n",
        "    return rlaif_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mKqgA6bkRi5K"
      },
      "outputs": [],
      "source": [
        "def get_parser():\n",
        "    \"\"\"Set up command line argument parser\"\"\"\n",
        "    parser = argparse.ArgumentParser(description='Generate RLAIF data from PubMedQA dataset')\n",
        "    parser.add_argument('--dataset_path', type=str, default=None,\n",
        "                        help='Path to local PubMedQA dataset JSON file')\n",
        "    parser.add_argument('--use_huggingface', action='store_true',\n",
        "                        help='Whether to use the Hugging Face dataset')\n",
        "    parser.add_argument('--model_name', type=str, default=\"microsoft/BioGPT-Large-PubMedQA\",\n",
        "                        help='Name of the model to use')\n",
        "    parser.add_argument('--output_file', type=str, default=\"rlaif_data.json\",\n",
        "                        help='Path to output file')\n",
        "    parser.add_argument('--save_interval', type=int, default=10,\n",
        "                        help='Number of items to process before saving')\n",
        "    parser.add_argument('--max_samples', type=int, default=None,\n",
        "                        help='Maximum number of samples to process')\n",
        "    return parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "29KCy8RKRmDt",
        "outputId": "fa1a11bb-a798-4050-eaff-7463048aa70f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--dataset_path DATASET_PATH]\n",
            "                                [--use_huggingface] [--model_name MODEL_NAME]\n",
            "                                [--output_file OUTPUT_FILE]\n",
            "                                [--save_interval SAVE_INTERVAL]\n",
            "                                [--max_samples MAX_SAMPLES]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-8020627b-57ca-4b36-808f-a3432941434f.json\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "2",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = get_parser()\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    generate_rlaif_data(\n",
        "        dataset_path=args.dataset_path,\n",
        "        use_huggingface=args.use_huggingface,\n",
        "        model_name=args.model_name,\n",
        "        output_file=args.output_file,\n",
        "        save_interval=args.save_interval,\n",
        "        max_samples=args.max_samples\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
