{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Trlon31t0OJ3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"rlaif_generation.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def load_pubmedqa_labeled(path):\n",
        "    \"\"\"Load the PubMedQA dataset from a JSON file\"\"\"\n",
        "    try:\n",
        "        with open(path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        logger.info(f\"Successfully loaded PubMedQA dataset from {path}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading PubMedQA dataset: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_model(model_name, load_in_4bit=True):\n",
        "    \"\"\"Set up the tokenizer and model with specified configuration\"\"\"\n",
        "    try:\n",
        "        # Configure quantization parameters\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=load_in_4bit,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "        \n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        \n",
        "        # Load model with quantization\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        \n",
        "        logger.info(f\"Successfully loaded model: {model_name}\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error setting up model: {e}\")\n",
        "        return None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cot_template():\n",
        "    \"\"\"Return the chain-of-thought template\"\"\"\n",
        "    return \"\"\"Please analyze the following medical case step by step:\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Let's think through this step by step:\n",
        "\n",
        "1. First, identify the key information from the context:\n",
        "2. Then, analyze the specific question being asked:\n",
        "3. Next, consider the relevant medical concepts:\n",
        "4. After that, evaluate the possible answers:\n",
        "5. Finally, provide a comprehensive conclusion:\n",
        "\n",
        "Answer:\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(model, tokenizer, prompt, max_length=1024, max_new_tokens=512, \n",
        "                   temperature=0.7, num_beams=4):\n",
        "    \"\"\"Generate an answer using the model\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        # Check if input is too long\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        if input_length > max_length:\n",
        "            logger.warning(f\"Input too long: {input_length} tokens (max: {max_length})\")\n",
        "            return None\n",
        "        \n",
        "        # Generate output\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.2,\n",
        "            length_penalty=1.5,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        \n",
        "        # Decode output\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating answer: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_generated_answer(answer):\n",
        "    \"\"\"Process the generated answer to extract the answer and chain of thought\"\"\"\n",
        "    if not answer:\n",
        "        return \"\", \"\"\n",
        "    \n",
        "    if \"Answer:\" in answer:\n",
        "        cot_part = answer.split(\"Answer:\")[0].strip()\n",
        "        answer_part = answer.split(\"Answer:\")[1].strip()\n",
        "    else:\n",
        "        cot_part = \"\"\n",
        "        answer_part = answer.strip()\n",
        "    \n",
        "    return answer_part, cot_part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_rlaif_data(dataset_path=None, use_huggingface=True, \n",
        "                        model_name=\"microsoft/BioGPT-Large-PubMedQA\", \n",
        "                        output_file=\"rlaif_data.json\", \n",
        "                        save_interval=10, max_samples=None):\n",
        "    \"\"\"Generate RLAIF data from PubMedQA dataset\"\"\"\n",
        "    \n",
        "    # Load dataset\n",
        "    if use_huggingface:\n",
        "        try:\n",
        "            dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
        "            train_data = dataset[\"train\"]\n",
        "            logger.info(f\"Loaded dataset from Hugging Face: {len(train_data)} items\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading dataset from Hugging Face: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        if not dataset_path:\n",
        "            logger.error(\"Dataset path required when not using Hugging Face\")\n",
        "            return None\n",
        "        train_data = load_pubmedqa_labeled(dataset_path)\n",
        "        if not train_data:\n",
        "            return None\n",
        "    \n",
        "    # Set up model and tokenizer\n",
        "    tokenizer, model = setup_model(model_name)\n",
        "    if not tokenizer or not model:\n",
        "        return None\n",
        "    \n",
        "    # Get CoT template\n",
        "    cot_template = get_cot_template()\n",
        "    \n",
        "    # Generate RLAIF data\n",
        "    rlaif_data = []\n",
        "    \n",
        "    # Limit number of samples if specified\n",
        "    data_items = train_data[:max_samples] if max_samples else train_data\n",
        "    \n",
        "    for idx, item in enumerate(tqdm(data_items, desc=\"Generating RLAIF data\")):\n",
        "        try:\n",
        "            if use_huggingface:\n",
        "                question = item[\"question\"]\n",
        "                context_data = item[\"context\"]\n",
        "                long_answer = item[\"long_answer\"]\n",
        "                contexts = context_data[\"contexts\"]\n",
        "                context = \" \".join(contexts)\n",
        "            else:\n",
        "                # Adjust this based on your local JSON structure\n",
        "                question = item.get(\"question\", \"\")\n",
        "                contexts = item.get(\"contexts\", [])\n",
        "                context = \" \".join(contexts)\n",
        "                long_answer = item.get(\"long_answer\", \"\")\n",
        "            \n",
        "            # Format prompt\n",
        "            prompt = cot_template.format(context=context, question=question)\n",
        "            \n",
        "            # Generate answer\n",
        "            answer = generate_answer(model, tokenizer, prompt)\n",
        "            if not answer:\n",
        "                continue\n",
        "            \n",
        "            # Process generated answer\n",
        "            answer_part, cot_part = process_generated_answer(answer)\n",
        "            \n",
        "            # Create data point\n",
        "            data_point = {\n",
        "                \"prompt\": prompt,\n",
        "                \"chosen\": {\n",
        "                    \"answer\": answer_part,\n",
        "                    \"chain_of_thought\": cot_part\n",
        "                },\n",
        "                \"rejected\": {\n",
        "                    \"answer\": long_answer,\n",
        "                    \"chain_of_thought\": \"\"\n",
        "                },\n",
        "                \"metadata\": {\n",
        "                    \"question\": question,\n",
        "                    \"context\": context,\n",
        "                    \"model\": model_name\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            rlaif_data.append(data_point)\n",
        "            \n",
        "            # Save at intervals\n",
        "            if (idx + 1) % save_interval == 0:\n",
        "                with open(output_file, \"w\") as f:\n",
        "                    json.dump(rlaif_data, f, indent=2)\n",
        "                logger.info(f\"Saved {len(rlaif_data)} items to {output_file}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing item {idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Final save\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(rlaif_data, f, indent=2)\n",
        "    logger.info(f\"Finished generating RLAIF data. Total items: {len(rlaif_data)}\")\n",
        "    \n",
        "    return rlaif_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-30 19:52:36,932 - ERROR - Error loading PubMedQA dataset: [Errno 21] Is a directory: '/Users/casey/Documents/GitHub'\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"/Users/casey/Documents/GitHub\"\n",
        "use_huggingface = False  # Set to True if you want to use HuggingFace dataset instead\n",
        "model_name = \"microsoft/BioGPT-Large-PubMedQA\"\n",
        "output_file = \"rlaif_data.json\"\n",
        "save_interval = 10\n",
        "max_samples = 100  # Set to None to process all samples\n",
        "\n",
        "# Call the function with parameters\n",
        "rlaif_data = generate_rlaif_data(\n",
        "    dataset_path=dataset_path, \n",
        "    use_huggingface=use_huggingface,\n",
        "    model_name=model_name,\n",
        "    output_file=output_file,\n",
        "    save_interval=save_interval,\n",
        "    max_samples=max_samples\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
