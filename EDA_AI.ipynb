{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubmedQA Artificial Set (PQA-A) Analysis Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "import textstat\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/casey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/casey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/casey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubMedQAAnalyzer:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the PubMedQA analyzer with dataset path\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the PubMedQA artificial dataset (PQA-A) JSON file\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.df = None\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load the PubMedQA artificial dataset (PQA-A) and convert to DataFrame\"\"\"\n",
    "        print(f\"Loading data from {self.data_path}...\")\n",
    "        \n",
    "        # Load JSON data\n",
    "        with open(self.data_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        rows = []\n",
    "        for pmid, item in data.items():\n",
    "            row = {\n",
    "                'pmid': pmid,\n",
    "                'question': item.get('question', ''),\n",
    "                'context': ' '.join(item.get('context', {}).get('contexts', [])),\n",
    "                'abstract': item.get('abstract', []),\n",
    "                'year': self._extract_year(item),\n",
    "                'final_decision': item.get('final_decision', ''),  # PQA-A has final_decision instead of long_answer\n",
    "                'mesh_terms': item.get('mesh', [])\n",
    "            }\n",
    "            rows.append(row)\n",
    "            \n",
    "        self.df = pd.DataFrame(rows)\n",
    "        self.df['context_length'] = self.df['context'].apply(len)\n",
    "        self.df['question_length'] = self.df['question'].apply(len)\n",
    "        self.df['abstract_text'] = self.df['abstract'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "        self.df['abstract_length'] = self.df['abstract_text'].apply(len)\n",
    "        \n",
    "        # Process final_decision field which is specific to PQA-A\n",
    "        if 'final_decision' in self.df.columns:\n",
    "            # Ensure final_decision is standardized\n",
    "            self.df['final_decision'] = self.df['final_decision'].str.lower()\n",
    "            \n",
    "            # Create binary and categorical versions\n",
    "            self.df['is_yes'] = self.df['final_decision'] == 'yes'\n",
    "            self.df['is_no'] = self.df['final_decision'] == 'no'\n",
    "            self.df['is_maybe'] = self.df['final_decision'] == 'maybe'\n",
    "        \n",
    "        print(f\"Loaded {len(self.df)} question-article pairs from PQA-A dataset\")\n",
    "        \n",
    "    def _extract_year(self, item):\n",
    "        \"\"\"Extract publication year from item metadata\"\"\"\n",
    "        try:\n",
    "            if 'pubmed' in item and 'content' in item['pubmed']:\n",
    "                if 'PubmedArticle' in item['pubmed']['content']:\n",
    "                    article = item['pubmed']['content']['PubmedArticle']\n",
    "                    if 'MedlineCitation' in article:\n",
    "                        if 'DateCompleted' in article['MedlineCitation']:\n",
    "                            return int(article['MedlineCitation']['DateCompleted']['Year'])\n",
    "                        elif 'Article' in article['MedlineCitation'] and 'Journal' in article['MedlineCitation']['Article']:\n",
    "                            if 'JournalIssue' in article['MedlineCitation']['Article']['Journal']:\n",
    "                                if 'PubDate' in article['MedlineCitation']['Article']['Journal']['JournalIssue']:\n",
    "                                    return int(article['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year'])\n",
    "        except (KeyError, TypeError):\n",
    "            pass\n",
    "        \n",
    "        # Try to extract year from PMID (may not be accurate)\n",
    "        try:\n",
    "            pmid_int = int(item.get('pmid', '0'))\n",
    "            if 1 <= pmid_int <= 100:  # Very early PMIDs\n",
    "                return 1975  # Approximate\n",
    "            elif 100 <= pmid_int <= 1000000:  # Old PMIDs\n",
    "                return 1985  # Approximate\n",
    "            elif 1000000 <= pmid_int <= 10000000:  # Mid PMIDs\n",
    "                return 1995  # Approximate\n",
    "            elif 10000000 <= pmid_int <= 20000000:\n",
    "                return 2005  # Approximate\n",
    "            elif 20000000 <= pmid_int <= 30000000:\n",
    "                return 2015  # Approximate\n",
    "            else:\n",
    "                return 2020  # Recent\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    def dataset_overview(self, output_dir='results'):\n",
    "        \"\"\"Generate dataset overview and context analysis\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        print(\"Generating dataset overview...\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        stats = {\n",
    "            'Total QA pairs': len(self.df),\n",
    "            'Unique PMIDs': self.df['pmid'].nunique(),\n",
    "            'Avg question length (chars)': self.df['question_length'].mean(),\n",
    "            'Avg context length (chars)': self.df['context_length'].mean(),\n",
    "            'Avg abstract length (chars)': self.df['abstract_length'].mean(),\n",
    "            'Min question length': self.df['question_length'].min(),\n",
    "            'Max question length': self.df['question_length'].max(),\n",
    "            'Min context length': self.df['context_length'].min(),\n",
    "            'Max context length': self.df['context_length'].max()\n",
    "        }\n",
    "        \n",
    "        # Years coverage\n",
    "        year_range = (self.df['year'].min(), self.df['year'].max())\n",
    "        stats['Publication years range'] = f\"{year_range[0]} - {year_range[1]}\"\n",
    "        \n",
    "        # Create summary table\n",
    "        stats_df = pd.DataFrame(list(stats.items()), columns=['Metric', 'Value'])\n",
    "        stats_df.to_csv(f\"{output_dir}/dataset_overview.csv\", index=False)\n",
    "        \n",
    "        # Distribution plots\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(16, 14))\n",
    "        \n",
    "        # Question length distribution\n",
    "        sns.histplot(self.df['question_length'], kde=True, ax=axs[0, 0])\n",
    "        axs[0, 0].set_title('Question Length Distribution (characters)')\n",
    "        axs[0, 0].set_xlabel('Length (characters)')\n",
    "        \n",
    "        # Context length distribution\n",
    "        sns.histplot(self.df['context_length'], kde=True, ax=axs[0, 1])\n",
    "        axs[0, 1].set_title('Context Length Distribution (characters)')\n",
    "        axs[0, 1].set_xlabel('Length (characters)')\n",
    "        \n",
    "        # Abstract length distribution\n",
    "        sns.histplot(self.df['abstract_length'], kde=True, ax=axs[1, 0])\n",
    "        axs[1, 0].set_title('Abstract Length Distribution (characters)')\n",
    "        axs[1, 0].set_xlabel('Length (characters)')\n",
    "        \n",
    "        # Year distribution\n",
    "        year_counts = self.df['year'].value_counts().sort_index()\n",
    "        year_counts.plot(kind='bar', ax=axs[1, 1])\n",
    "        axs[1, 1].set_title('Publication Year Distribution')\n",
    "        axs[1, 1].set_xlabel('Year')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/dataset_distributions.png\", dpi=300)\n",
    "        \n",
    "        # Generate detailed descriptive statistics\n",
    "        desc_stats = self.df[['question_length', 'context_length', 'abstract_length']].describe()\n",
    "        desc_stats.to_csv(f\"{output_dir}/descriptive_statistics.csv\")\n",
    "        \n",
    "        # Question word count statistics\n",
    "        self.df['question_word_count'] = self.df['question'].apply(lambda x: len(word_tokenize(x)))\n",
    "        q_word_stats = self.df['question_word_count'].describe()\n",
    "        \n",
    "        # Context word count statistics\n",
    "        sample_size = min(1000, len(self.df))  # Sample for efficiency\n",
    "        sample_indices = np.random.choice(len(self.df), sample_size, replace=False)\n",
    "        context_word_counts = [len(word_tokenize(text)) for text in self.df.iloc[sample_indices]['context']]\n",
    "        c_word_stats = pd.Series(context_word_counts).describe()\n",
    "        \n",
    "        # Combine word count statistics\n",
    "        word_stats = pd.DataFrame({\n",
    "            'Question Word Count': q_word_stats,\n",
    "            'Context Word Count (Sample)': c_word_stats\n",
    "        })\n",
    "        word_stats.to_csv(f\"{output_dir}/word_count_statistics.csv\")\n",
    "        \n",
    "        print(\"Dataset overview analysis completed\")\n",
    "        return stats\n",
    "    \n",
    "    def distribution_analysis(self, output_dir='results'):\n",
    "        \"\"\"Generate data distribution analysis\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        print(\"Generating data distribution analysis...\")\n",
    "        \n",
    "        # Create a figure with multiple subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Question Length Distribution (Words)', \n",
    "                           'Context Length Distribution (Words)',\n",
    "                           'Publication Year Distribution',\n",
    "                           'MeSH Terms Distribution (Top 20)')\n",
    "        )\n",
    "        \n",
    "        # Question word count distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=self.df['question_word_count'], nbinsx=30, opacity=0.7,\n",
    "                         marker=dict(color='royalblue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Context length distribution (sampled)\n",
    "        sample_size = min(1000, len(self.df))\n",
    "        sample_indices = np.random.choice(len(self.df), sample_size, replace=False)\n",
    "        context_word_counts = [len(word_tokenize(text)) for text in self.df.iloc[sample_indices]['context']]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=context_word_counts, nbinsx=30, opacity=0.7,\n",
    "                        marker=dict(color='green')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Publication year distribution\n",
    "        year_counts = self.df['year'].value_counts().sort_index()\n",
    "        years = list(year_counts.index.astype(str))\n",
    "        counts = list(year_counts.values)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=years, y=counts, marker=dict(color='purple')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # MeSH terms distribution\n",
    "        all_mesh_terms = []\n",
    "        for terms in self.df['mesh_terms']:\n",
    "            if isinstance(terms, list):\n",
    "                all_mesh_terms.extend(terms)\n",
    "                \n",
    "        top_terms = Counter(all_mesh_terms).most_common(20)\n",
    "        term_labels = [term[0] for term in top_terms]\n",
    "        term_counts = [term[1] for term in top_terms]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(y=term_labels, x=term_counts, marker=dict(color='orangered'), orientation='h'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            showlegend=False,\n",
    "        )\n",
    "        \n",
    "        # Save the figure\n",
    "        fig.write_html(f\"{output_dir}/distribution_analysis.html\")\n",
    "        fig.write_image(f\"{output_dir}/distribution_analysis.png\")\n",
    "        \n",
    "        # Calculate correlations between features\n",
    "        correlations = self.df[['question_length', 'context_length', 'abstract_length', 'question_word_count', 'year']].corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlations, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.title('Feature Correlations')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/feature_correlations.png\", dpi=300)\n",
    "        \n",
    "        # Save correlation matrix\n",
    "        correlations.to_csv(f\"{output_dir}/feature_correlations.csv\")\n",
    "        \n",
    "        print(\"Data distribution analysis completed\")\n",
    "    \n",
    "    def temporal_analysis(self, output_dir='results'):\n",
    "        \"\"\"Analyze how dataset characteristics change over time\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        print(\"Generating temporal analysis...\")\n",
    "        \n",
    "        # Ensure we have year data\n",
    "        self.df = self.df.dropna(subset=['year'])\n",
    "        \n",
    "        # Group by year and calculate statistics\n",
    "        yearly_stats = self.df.groupby('year').agg({\n",
    "            'pmid': 'count',\n",
    "            'question_length': 'mean',\n",
    "            'context_length': 'mean',\n",
    "            'question_word_count': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        yearly_stats.columns = ['Year', 'Count', 'Avg Question Length', \n",
    "                                'Avg Context Length', 'Avg Question Word Count']\n",
    "        \n",
    "        # Save statistics to CSV\n",
    "        yearly_stats.to_csv(f\"{output_dir}/yearly_statistics.csv\", index=False)\n",
    "        \n",
    "        # Create temporal plots\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Number of articles per year\n",
    "        axs[0, 0].bar(yearly_stats['Year'], yearly_stats['Count'], color='skyblue')\n",
    "        axs[0, 0].set_title('Number of Articles by Year')\n",
    "        axs[0, 0].set_xlabel('Year')\n",
    "        axs[0, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Average question length by year\n",
    "        axs[0, 1].plot(yearly_stats['Year'], yearly_stats['Avg Question Length'], \n",
    "                      marker='o', color='green', linestyle='-')\n",
    "        axs[0, 1].set_title('Average Question Length by Year')\n",
    "        axs[0, 1].set_xlabel('Year')\n",
    "        axs[0, 1].set_ylabel('Average Question Length (chars)')\n",
    "        \n",
    "        # Average context length by year\n",
    "        axs[1, 0].plot(yearly_stats['Year'], yearly_stats['Avg Context Length'], \n",
    "                      marker='o', color='purple', linestyle='-')\n",
    "        axs[1, 0].set_title('Average Context Length by Year')\n",
    "        axs[1, 0].set_xlabel('Year')\n",
    "        axs[1, 0].set_ylabel('Average Context Length (chars)')\n",
    "        \n",
    "        # Average question word count by year\n",
    "        axs[1, 1].plot(yearly_stats['Year'], yearly_stats['Avg Question Word Count'], \n",
    "                      marker='o', color='red', linestyle='-')\n",
    "        axs[1, 1].set_title('Average Question Word Count by Year')\n",
    "        axs[1, 1].set_xlabel('Year')\n",
    "        axs[1, 1].set_ylabel('Average Word Count')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/temporal_analysis.png\", dpi=300)\n",
    "        \n",
    "        # Advanced temporal analysis with trend detection\n",
    "        # Filter to years with sufficient data points\n",
    "        min_year_count = 50  # Minimum number of articles per year to include\n",
    "        filtered_years = yearly_stats[yearly_stats['Count'] >= min_year_count]\n",
    "        \n",
    "        if len(filtered_years) >= 5:  # At least 5 years with sufficient data\n",
    "            # Perform trend analysis using polynomial regression\n",
    "            years = filtered_years['Year'].values\n",
    "            q_lengths = filtered_years['Avg Question Length'].values\n",
    "            \n",
    "            # Normalize years for better numerical stability\n",
    "            years_norm = (years - years.min()) / (years.max() - years.min())\n",
    "            \n",
    "            # Fit polynomial regression\n",
    "            degree = min(3, len(years) - 1)  # Up to cubic regression depending on data points\n",
    "            coeffs = np.polyfit(years_norm, q_lengths, degree)\n",
    "            poly = np.poly1d(coeffs)\n",
    "            \n",
    "            # Generate trend line\n",
    "            years_seq = np.linspace(years_norm.min(), years_norm.max(), 100)\n",
    "            trend = poly(years_seq)\n",
    "            \n",
    "            # Convert back to original year scale\n",
    "            years_seq = years_seq * (years.max() - years.min()) + years.min()\n",
    "            \n",
    "            # Plot trend\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(years, q_lengths, color='blue', label='Actual data')\n",
    "            plt.plot(years_seq, trend, color='red', label=f'Trend (degree {degree})')\n",
    "            plt.title('Question Length Trend Analysis')\n",
    "            plt.xlabel('Year')\n",
    "            plt.ylabel('Average Question Length')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(f\"{output_dir}/question_length_trend.png\", dpi=300)\n",
    "            \n",
    "            # Calculate trend direction and rate of change\n",
    "            trend_direction = \"increasing\" if coeffs[0] > 0 else \"decreasing\"\n",
    "            avg_annual_change = (q_lengths[-1] - q_lengths[0]) / (len(years) - 1)\n",
    "            \n",
    "            # Save trend analysis results\n",
    "            trend_results = pd.DataFrame({\n",
    "                'Metric': ['Trend direction', 'Average annual change', 'Polynomial degree'],\n",
    "                'Value': [trend_direction, avg_annual_change, degree]\n",
    "            })\n",
    "            trend_results.to_csv(f\"{output_dir}/question_length_trend_analysis.csv\", index=False)\n",
    "        \n",
    "        print(\"Temporal analysis completed\")\n",
    "    \n",
    "    def topic_domain_analysis(self, output_dir='results', n_topics=10):\n",
    "        \"\"\"Analyze topics and domains in the dataset\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        print(\"Generating topic/domain analysis...\")\n",
    "        \n",
    "        # MeSH terms analysis\n",
    "        all_mesh = []\n",
    "        for terms in self.df['mesh_terms']:\n",
    "            if isinstance(terms, list):\n",
    "                all_mesh.extend(terms)\n",
    "        \n",
    "        mesh_counts = Counter(all_mesh)\n",
    "        top_mesh = mesh_counts.most_common(30)\n",
    "        \n",
    "        # Save MeSH terms data\n",
    "        mesh_df = pd.DataFrame(top_mesh, columns=['MeSH Term', 'Count'])\n",
    "        mesh_df.to_csv(f\"{output_dir}/top_mesh_terms.csv\", index=False)\n",
    "        \n",
    "        # Plot top MeSH terms\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Count', y='MeSH Term', data=mesh_df.head(20))\n",
    "        plt.title('Top 20 MeSH Terms')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/top_mesh_terms.png\", dpi=300)\n",
    "        \n",
    "        # Create word cloud of MeSH terms\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                              max_words=100, contour_width=3, contour_color='steelblue')\n",
    "        wordcloud.generate_from_frequencies(mesh_counts)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('MeSH Terms Word Cloud')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/mesh_wordcloud.png\", dpi=300)\n",
    "        \n",
    "        # Topic modeling using abstracts\n",
    "        print(\"Performing topic modeling...\")\n",
    "        \n",
    "        # Filter out empty abstracts\n",
    "        abstracts = self.df['abstract_text'].dropna().tolist()\n",
    "        \n",
    "        if len(abstracts) > 100:  # Only perform if we have sufficient data\n",
    "            # Sample abstracts for efficiency\n",
    "            sample_size = min(2000, len(abstracts))\n",
    "            abstract_sample = np.random.choice(abstracts, sample_size, replace=False)\n",
    "            \n",
    "            # Preprocess text\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            processed_abstracts = []\n",
    "            \n",
    "            for abstract in abstract_sample:\n",
    "                # Tokenize, lowercase, remove stopwords and short words\n",
    "                tokens = word_tokenize(abstract.lower())\n",
    "                tokens = [token for token in tokens if token.isalpha() and token not in stop_words and len(token) > 3]\n",
    "                processed_abstracts.append(' '.join(tokens))\n",
    "            \n",
    "            # Create TF-IDF matrix\n",
    "            vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.7)\n",
    "            tfidf_matrix = vectorizer.fit_transform(processed_abstracts)\n",
    "            \n",
    "            # LDA Topic Modeling\n",
    "            lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=20)\n",
    "            lda.fit(tfidf_matrix)\n",
    "            \n",
    "            # Get feature names\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Extract top words for each topic\n",
    "            n_top_words = 15\n",
    "            topic_words = []\n",
    "            \n",
    "            for topic_idx, topic in enumerate(lda.components_):\n",
    "                top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "                top_words = [feature_names[i] for i in top_words_idx]\n",
    "                topic_words.append((topic_idx, top_words))\n",
    "            \n",
    "            # Save topic words\n",
    "            topic_df = pd.DataFrame([(idx, ', '.join(words)) for idx, words in topic_words], \n",
    "                                   columns=['Topic ID', 'Top Words'])\n",
    "            topic_df.to_csv(f\"{output_dir}/lda_topics.csv\", index=False)\n",
    "            \n",
    "            # Visualize topics\n",
    "            fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, (topic_idx, top_words) in enumerate(topic_words):\n",
    "                if i < 10:  # Plot only first 10 topics\n",
    "                    ax = axes[i]\n",
    "                    ax.barh(range(len(top_words)), lda.components_[topic_idx][topic.argsort()[:-n_top_words - 1:-1]], \n",
    "                           align='center')\n",
    "                    ax.set_yticks(range(len(top_words)))\n",
    "                    ax.set_yticklabels(top_words)\n",
    "                    ax.invert_yaxis()\n",
    "                    ax.set_title(f'Topic {topic_idx+1}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/lda_topics_visualized.png\", dpi=300)\n",
    "            \n",
    "            # Document-Topic Distribution\n",
    "            doc_topic_distr = lda.transform(tfidf_matrix)\n",
    "            \n",
    "            # Create a heatmap of document-topic distribution for a sample\n",
    "            sample_docs = min(50, doc_topic_distr.shape[0])\n",
    "            sample_indices = np.random.choice(doc_topic_distr.shape[0], sample_docs, replace=False)\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(doc_topic_distr[sample_indices], cmap='YlGnBu', \n",
    "                       xticklabels=[f'Topic {i+1}' for i in range(n_topics)])\n",
    "            plt.title('Document-Topic Distribution (Sample)')\n",
    "            plt.ylabel('Document')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/doc_topic_distribution.png\", dpi=300)\n",
    "        \n",
    "        print(\"Topic/domain analysis completed\")\n",
    "    \n",
    "    def answer_distribution_analysis(self, output_dir='results'):\n",
    "        \"\"\"Analyze the distribution of answers in the PQA-A dataset\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        print(\"Generating answer distribution analysis...\")\n",
    "        \n",
    "        # Check if we have final_decision column (specific to PQA-A)\n",
    "        if 'final_decision' not in self.df.columns:\n",
    "            print(\"No final_decision column found. This analysis is specific to PQA-A dataset.\")\n",
    "            return\n",
    "        \n",
    "        # Get answer distribution\n",
    "        answer_counts = self.df['final_decision'].value_counts()\n",
    "        answer_percentages = 100 * answer_counts / len(self.df)\n",
    "        \n",
    "        # Save to CSV\n",
    "        answer_df = pd.DataFrame({\n",
    "            'Answer': answer_counts.index,\n",
    "            'Count': answer_counts.values,\n",
    "            'Percentage': answer_percentages.values\n",
    "        })\n",
    "        answer_df.to_csv(f\"{output_dir}/answer_distribution.csv\", index=False)\n",
    "        \n",
    "        # Plot answer distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Answer', y='Count', data=answer_df, palette='viridis')\n",
    "        plt.title('Answer Distribution in PQA-A Dataset')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/answer_distribution.png\", dpi=300)\n",
    "        \n",
    "        # Plot as pie chart\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.pie(answer_counts, labels=answer_counts.index, autopct='%1.1f%%', \n",
    "                startangle=90, colors=sns.color_palette('viridis', len(answer_counts)))\n",
    "        plt.axis('equal')\n",
    "        plt.title('Answer Distribution in PQA-A Dataset')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/answer_distribution_pie.png\", dpi=300)\n",
    "        \n",
    "        # Analyze temporal trends in answers if we have year data\n",
    "        if 'year' in self.df.columns:\n",
    "            # Group by year and calculate percentage of each answer type\n",
    "            yearly_answers = pd.crosstab(self.df['year'], self.df['final_decision'], normalize='index')\n",
    "            yearly_answers.to_csv(f\"{output_dir}/yearly_answer_distribution.csv\")\n",
    "            \n",
    "            # Plot trends\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            yearly_answers.plot(kind='line', marker='o')\n",
    "            plt.title('Answer Distribution Trends Over Time')\n",
    "            plt.xlabel('Year')\n",
    "            plt.ylabel('Proportion of Answers')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/answer_trends.png\", dpi=300)\n",
    "        \n",
    "        # Analyze relationship between context length and answer\n",
    "        bins = [0, 500, 1000, 2000, 3000, 5000, 10000, float('inf')]\n",
    "        labels = ['0-500', '501-1000', '1001-2000', '2001-3000', '3001-5000', '5001-10000', '10000+']\n",
    "        \n",
    "        self.df['context_length_bin'] = pd.cut(self.df['context_length'], bins=bins, labels=labels)\n",
    "        \n",
    "        # Create cross-tabulation\n",
    "        context_vs_answer = pd.crosstab(self.df['context_length_bin'], self.df['final_decision'], normalize='index')\n",
    "        context_vs_answer.to_csv(f\"{output_dir}/context_length_vs_answer.csv\")\n",
    "        \n",
    "        # Plot relationship\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        context_vs_answer.plot(kind='bar', stacked=True)\n",
    "        plt.title('Answer Distribution by Context Length')\n",
    "        plt.xlabel('Context Length (characters)')\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.legend(title='Answer')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/context_length_vs_answer.png\", dpi=300)\n",
    "        \n",
    "        print(\"Answer distribution analysis completed\")\n",
    "    \n",
    "    def text_complexity_analysis(self, output_dir='results', sample_size=1000):\n",
    "        \"\"\"Analyze text complexity of questions and contexts\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        print(\"Generating text complexity analysis...\")\n",
    "        \n",
    "        # Sample data for efficiency\n",
    "        df_sample = self.df.sample(min(sample_size, len(self.df)), random_state=42)\n",
    "        \n",
    "        # Calculate readability metrics for questions\n",
    "        print(\"Calculating readability metrics for questions...\")\n",
    "        readability_metrics = {\n",
    "            'flesch_reading_ease': [],\n",
    "            'flesch_kincaid_grade': [],\n",
    "            'smog_index': [],\n",
    "            'automated_readability_index': [],\n",
    "            'coleman_liau_index': []\n",
    "        }\n",
    "        \n",
    "        for q in df_sample['question']:\n",
    "            try:\n",
    "                readability_metrics['flesch_reading_ease'].append(textstat.flesch_reading_ease(q))\n",
    "                readability_metrics['flesch_kincaid_grade'].append(textstat.flesch_kincaid_grade(q))\n",
    "                readability_metrics['smog_index'].append(textstat.smog_index(q))\n",
    "                readability_metrics['automated_readability_index'].append(textstat.automated_readability_index(q))\n",
    "                readability_metrics['coleman_liau_index'].append(textstat.coleman_liau_index(q))\n",
    "            except:\n",
    "                # Skip if text is too short\n",
    "                continue\n",
    "        \n",
    "        # Create DataFrame for readability metrics\n",
    "        metrics_df = pd.DataFrame(readability_metrics)\n",
    "        metrics_df.dropna(inplace=True)  # Remove any NaN values\n",
    "        \n",
    "        # Calculate descriptive statistics for readability metrics\n",
    "        read_stats = metrics_df.describe()\n",
    "        read_stats.to_csv(f\"{output_dir}/question_readability_metrics.csv\")\n",
    "        \n",
    "        # Plot readability distributions\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics_df.boxplot()\n",
    "        plt.title('Question Readability Metrics Distribution')\n",
    "        plt.grid(False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/question_readability_boxplot.png\", dpi=300)\n",
    "        \n",
    "        # Sample contexts for analysis\n",
    "        context_sample = df_sample['context'].sample(min(200, len(df_sample)), random_state=42)\n",
    "        \n",
    "        print(\"Analyzing sentence complexity...\")\n",
    "        # Sentence complexity analysis\n",
    "        sentence_lengths = []\n",
    "        word_lengths = []\n",
    "        \n",
    "        for text in context_sample:\n",
    "            sentences = sent_tokenize(text)\n",
    "            for sentence in sentences:\n",
    "                words = word_tokenize(sentence)\n",
    "                if words:  # Only if there are words\n",
    "                    sentence_lengths.append(len(words))\n",
    "                    word_lengths.extend([len(word) for word in words if word.isalpha()])\n",
    "        \n",
    "        # Save sentence complexity data\n",
    "        sentence_data = pd.DataFrame({\n",
    "            'Sentence Lengths': sentence_lengths,\n",
    "            'Word Lengths': word_lengths[:len(sentence_lengths)]  # Match lengths\n",
    "        })\n",
    "        sentence_data.describe().to_csv(f\"{output_dir}/sentence_complexity.csv\")\n",
    "        \n",
    "        # Plot sentence and word length distributions\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        sns.histplot(sentence_lengths, kde=True, ax=axs[0])\n",
    "        axs[0].set_title('Sentence Length Distribution (words)')\n",
    "        axs[0].set_xlabel('Words per Sentence')\n",
    "        \n",
    "        sns.histplot(word_lengths, kde=True, ax=axs[1])\n",
    "        axs[1].set_title('Word Length Distribution (characters)')\n",
    "        axs[1].set_xlabel('Characters per Word')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/sentence_word_lengths.png\", dpi=300)\n",
    "        \n",
    "        # Technical terminology analysis\n",
    "        print(\"Analyzing medical terminology density...\")\n",
    "        \n",
    "        # Use spaCy to identify medical entities\n",
    "        medical_entity_counts = []\n",
    "        entity_types = Counter()\n",
    "        \n",
    "        for text in context_sample.iloc[:50]:  # Limit for processing time\n",
    "            doc = nlp(text)\n",
    "            entities = [ent.text for ent in doc.ents]\n",
    "            medical_entity_counts.append(len(entities))\n",
    "            \n",
    "            # Count entity types\n",
    "            for ent in doc.ents:\n",
    "                entity_types[ent.label_] += 1\n",
    "        \n",
    "        # Entity type distribution\n",
    "        entity_type_df = pd.DataFrame(list(entity_types.items()), columns=['Entity Type', 'Count'])\n",
    "        entity_type_df = entity_type_df.sort_values('Count', ascending=False)\n",
    "        entity_type_df.to_csv(f\"{output_dir}/entity_type_distribution.csv\", index=False)\n",
    "        \n",
    "        # Plot entity type distribution\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Count', y='Entity Type', data=entity_type_df.head(15))\n",
    "        plt.title('Top 15 Entity Types')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/entity_types.png\", dpi=300)\n",
    "        \n",
    "        # Medical terminology density\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(medical_entity_counts, kde=True)\n",
    "        plt.title('Medical Entity Count Distribution')\n",
    "        plt.xlabel('Entities per Context')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/medical_entity_counts.png\", dpi=300)\n",
    "        \n",
    "        print(\"Text complexity analysis completed\")\n",
    "        \n",
    "    def classwise_feature_analysis(self, output_dir='results'):\n",
    "        \"\"\"Analyze features across different answer classes in the PQA-A dataset\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        print(\"Generating class-wise feature analysis...\")\n",
    "        \n",
    "        # Check if we have final_decision column (specific to PQA-A)\n",
    "        if 'final_decision' not in self.df.columns:\n",
    "            print(\"No final_decision column found. This analysis is specific to PQA-A dataset.\")\n",
    "            return\n",
    "        \n",
    "        # Create a feature matrix for analysis\n",
    "        feature_df = self.df[['question_length', 'context_length', 'question_word_count', \n",
    "                              'final_decision']].copy()\n",
    "        \n",
    "        # Add question complexity metrics\n",
    "        sample_size = min(1000, len(self.df))\n",
    "        sampled_indices = np.random.choice(len(self.df), sample_size, replace=False)\n",
    "        \n",
    "        # Average word length in questions\n",
    "        feature_df.loc[sampled_indices, 'avg_word_length'] = self.df.loc[sampled_indices, 'question'].apply(\n",
    "            lambda q: np.mean([len(word) for word in word_tokenize(q) if word.isalpha()]) if word_tokenize(q) else 0\n",
    "        )\n",
    "        \n",
    "        # Calculate readability of questions for the sample\n",
    "        feature_df.loc[sampled_indices, 'flesch_kincaid_grade'] = self.df.loc[sampled_indices, 'question'].apply(\n",
    "            lambda q: textstat.flesch_kincaid_grade(q) if len(q) > 50 else np.nan\n",
    "        )\n",
    "        \n",
    "        # Group statistics by answer class\n",
    "        class_stats = feature_df.groupby('final_decision').agg({\n",
    "            'question_length': ['mean', 'median', 'std'],\n",
    "            'context_length': ['mean', 'median', 'std'],\n",
    "            'question_word_count': ['mean', 'median', 'std'],\n",
    "            'avg_word_length': ['mean', 'median', 'std'],\n",
    "            'flesch_kincaid_grade': ['mean', 'median', 'std']\n",
    "        })\n",
    "        \n",
    "        # Save class statistics\n",
    "        class_stats.to_csv(f\"{output_dir}/class_feature_statistics.csv\")\n",
    "        \n",
    "        # Plot class-wise feature comparisons\n",
    "        features_to_plot = ['question_length', 'context_length', 'question_word_count']\n",
    "        \n",
    "        # Create boxplots for each feature by class\n",
    "        fig, axes = plt.subplots(len(features_to_plot), 1, figsize=(12, 4*len(features_to_plot)))\n",
    "        \n",
    "        for i, feature in enumerate(features_to_plot):\n",
    "            sns.boxplot(x='final_decision', y=feature, data=feature_df, ax=axes[i])\n",
    "            axes[i].set_title(f'{feature} by Answer Class')\n",
    "            axes[i].set_xlabel('Answer')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/class_feature_boxplots.png\", dpi=300)\n",
    "        \n",
    "        # Statistical significance tests\n",
    "        print(\"Performing statistical significance tests...\")\n",
    "        \n",
    "        from scipy import stats\n",
    "        \n",
    "        # Perform ANOVA for each feature\n",
    "        anova_results = {}\n",
    "        for feature in features_to_plot:\n",
    "            # Get data for each class\n",
    "            groups = [feature_df[feature_df['final_decision'] == cls][feature].dropna() \n",
    "                     for cls in feature_df['final_decision'].unique()]\n",
    "            \n",
    "            # Perform ANOVA\n",
    "            try:\n",
    "                f_stat, p_value = stats.f_oneway(*groups)\n",
    "                anova_results[feature] = {\n",
    "                    'f_statistic': f_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'significant': p_value < 0.05\n",
    "                }\n",
    "            except:\n",
    "                anova_results[feature] = {\n",
    "                    'f_statistic': np.nan,\n",
    "                    'p_value': np.nan,\n",
    "                    'significant': False\n",
    "                }\n",
    "        \n",
    "        # Save ANOVA results\n",
    "        anova_df = pd.DataFrame(anova_results).T\n",
    "        anova_df.to_csv(f\"{output_dir}/feature_anova_results.csv\")\n",
    "        \n",
    "        # Model predictiveness analysis\n",
    "        if len(feature_df) >= 1000:  # Only if we have enough data\n",
    "            print(\"Analyzing feature predictiveness for answer classification...\")\n",
    "            \n",
    "            from sklearn.model_selection import train_test_split\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            from sklearn.metrics import classification_report, confusion_matrix\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            \n",
    "            # Prepare features and target\n",
    "            X = feature_df[features_to_plot].dropna()\n",
    "            y = feature_df.loc[X.index, 'final_decision']\n",
    "            \n",
    "            # Train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train classifier\n",
    "            clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = clf.predict(X_test_scaled)\n",
    "            \n",
    "            # Save classification report\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            report_df = pd.DataFrame(report).T\n",
    "            report_df.to_csv(f\"{output_dir}/classification_report.csv\")\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=clf.classes_, yticklabels=clf.classes_)\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/confusion_matrix.png\", dpi=300)\n",
    "            \n",
    "            # Feature importance\n",
    "            feature_imp = pd.DataFrame({\n",
    "                'Feature': features_to_plot,\n",
    "                'Importance': clf.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            feature_imp.to_csv(f\"{output_dir}/feature_importance.csv\", index=False)\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x='Importance', y='Feature', data=feature_imp)\n",
    "            plt.title('Feature Importance for Answer Classification')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/feature_importance.png\", dpi=300)\n",
    "        \n",
    "        print(\"Class-wise feature analysis completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_type_analysis(self, output_dir='results'):\n",
    "        \"\"\"Analyze question types and patterns\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        print(\"Generating question type analysis...\")\n",
    "        \n",
    "        # Function to classify question type\n",
    "        def classify_question(q):\n",
    "            q = q.lower().strip()\n",
    "            \n",
    "            # Basic question classification\n",
    "            if q.startswith('is ') or q.startswith('are ') or q.startswith('does ') or q.startswith('do ') or q.startswith('can ') or q.startswith('could '):\n",
    "                return 'Yes/No'\n",
    "            elif q.startswith('what '):\n",
    "                return 'What'\n",
    "            elif q.startswith('how '):\n",
    "                return 'How'\n",
    "            elif q.startswith('why '):\n",
    "                return 'Why'\n",
    "            elif q.startswith('when '):\n",
    "                return 'When'\n",
    "            elif q.startswith('where '):\n",
    "                return 'Where'\n",
    "            elif q.startswith('which '):\n",
    "                return 'Which'\n",
    "            elif q.startswith('who '):\n",
    "                return 'Who'\n",
    "            else:\n",
    "                return 'Other'\n",
    "        \n",
    "        # Classify each question\n",
    "        self.df['question_type'] = self.df['question'].apply(classify_question)\n",
    "        \n",
    "        # Get question type distribution\n",
    "        q_type_counts = self.df['question_type'].value_counts()\n",
    "        \n",
    "        # Save results\n",
    "        q_type_df = pd.DataFrame(q_type_counts).reset_index()\n",
    "        q_type_df.columns = ['Question Type', 'Count']\n",
    "        q_type_df.to_csv(f\"{output_dir}/question_type_distribution.csv\", index=False)\n",
    "        \n",
    "        # Plot question type distribution\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Count', y='Question Type', data=q_type_df)\n",
    "        plt.title('Question Type Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/question_type_distribution.png\", dpi=300)\n",
    "        \n",
    "        # Analyze question complexity\n",
    "        self.df['question_word_count'] = self.df['question'].apply(lambda x: len(word_tokenize(x)))\n",
    "        \n",
    "        # Plot question complexity by type\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.boxplot(x='question_type', y='question_word_count', data=self.df)\n",
    "        plt.title('Question Word Count by Question Type')\n",
    "        plt.xlabel('Question Type')\n",
    "        plt.ylabel('Word Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/question_complexity_by_type.png\", dpi=300)\n",
    "        \n",
    "        # If we have final_decision data (specific to PQA-A dataset)\n",
    "        if 'final_decision' in self.df.columns:\n",
    "            # Analyze relationship between question type and answer\n",
    "            q_type_answer = pd.crosstab(self.df['question_type'], self.df['final_decision'])\n",
    "            q_type_answer.to_csv(f\"{output_dir}/question_type_vs_answer.csv\")\n",
    "            \n",
    "            # Normalize for percentage\n",
    "            q_type_answer_pct = pd.crosstab(self.df['question_type'], self.df['final_decision'], normalize='index')\n",
    "            q_type_answer_pct.to_csv(f\"{output_dir}/question_type_vs_answer_pct.csv\")\n",
    "            \n",
    "            # Plot relationship\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            q_type_answer_pct.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "            plt.title('Answer Distribution by Question Type')\n",
    "            plt.xlabel('Question Type')\n",
    "            plt.ylabel('Percentage')\n",
    "            plt.legend(title='Answer')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/answer_by_question_type.png\", dpi=300)\n",
    "            \n",
    "            # Analyze question complexity impact on answer\n",
    "            answer_by_complexity = {}\n",
    "            word_count_bins = [0, 5, 10, 15, 20, 25, 30, 999]\n",
    "            bin_labels = ['1-5', '6-10', '11-15', '16-20', '21-25', '26-30', '30+']\n",
    "            \n",
    "            self.df['word_count_bin'] = pd.cut(self.df['question_word_count'], \n",
    "                                              bins=word_count_bins, \n",
    "                                              labels=bin_labels)\n",
    "            \n",
    "            complexity_vs_answer = pd.crosstab(self.df['word_count_bin'], self.df['final_decision'], normalize='index')\n",
    "            complexity_vs_answer.to_csv(f\"{output_dir}/complexity_vs_answer.csv\")\n",
    "            \n",
    "            # Plot relationship\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            complexity_vs_answer.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "            plt.title('Answer Distribution by Question Complexity')\n",
    "            plt.xlabel('Question Word Count')\n",
    "            plt.ylabel('Percentage')\n",
    "            plt.legend(title='Answer')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/answer_by_complexity.png\", dpi=300)\n",
    "        \n",
    "        # Extract key medical entities from questions\n",
    "        print(\"Extracting medical entities from questions...\")\n",
    "        \n",
    "        # Sample for efficiency\n",
    "        sample_size = min(1000, len(self.df))\n",
    "        sampled_questions = self.df['question'].sample(sample_size, random_state=42)\n",
    "        \n",
    "        medical_entities = []\n",
    "        for question in sampled_questions:\n",
    "            doc = nlp(question)\n",
    "            entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "            medical_entities.extend(entities)\n",
    "        \n",
    "        # Count entity occurrences\n",
    "        entity_counter = Counter([entity[0].lower() for entity in medical_entities])\n",
    "        top_entities = entity_counter.most_common(30)\n",
    "        \n",
    "        # Save top entities\n",
    "        entity_df = pd.DataFrame(top_entities, columns=['Entity', 'Count'])\n",
    "        entity_df.to_csv(f\"{output_dir}/top_question_entities.csv\", index=False)\n",
    "        \n",
    "        # Plot top entities\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        sns.barplot(x='Count', y='Entity', data=entity_df.head(20))\n",
    "        plt.title('Top 20 Medical Entities in Questions')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/top_question_entities.png\", dpi=300)\n",
    "        \n",
    "        print(\"Question type analysis completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
