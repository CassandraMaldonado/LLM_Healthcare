{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3"
      ],
      "metadata": {
        "id": "YK8UseaZPdMu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medical Chatbot Dataset Preparation Script\n",
        "=========================================\n",
        "\n",
        "This script prepares and mixes MedMCQA and Orca datasets for fine-tuning a medical chatbot.\n",
        "It handles loading, cleaning, standardizing, and mixing datasets according to specified ratios.\n",
        "\n",
        "Requirements:\n",
        "- MedMCQA dataset in CSV format with columns: question, exp, cop, opa, opb, opc, opd, correct_answer\n",
        "- Orca dataset in JSON/Alpaca format\n",
        "- Target: 75% MedMCQA + 25% Orca mix\n",
        "- 10% evaluation split from each dataset (stratified)\n"
      ],
      "metadata": {
        "id": "3rnAKF7tPhIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from collections import defaultdict\n",
        "import logging"
      ],
      "metadata": {
        "id": "0M8zx9UaPnR4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DatasetProcessor:\n",
        "    \"\"\"Main class for processing and mixing datasets.\"\"\"\n",
        "\n",
        "    def __init__(self, radqa_ratio: float = 0.75, eval_split: float = 0.1, seed: int = 42):\n",
        "        \"\"\"\n",
        "        Initialize the dataset processor.\n",
        "\n",
        "        Args:\n",
        "            radqa_ratio: Proportion of RadQA samples in final dataset (0.75 = 75%)\n",
        "            eval_split: Proportion of data to use for evaluation (0.1 = 10%)\n",
        "            seed: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.radqa_ratio = radqa_ratio\n",
        "        self.orca_ratio = 1.0 - radqa_ratio\n",
        "        self.eval_split = eval_split\n",
        "        self.seed = seed\n",
        "        random.seed(seed)\n",
        "\n",
        "        # Unified schema for all datasets\n",
        "        self.unified_schema = {\n",
        "            \"instruction\": \"\",\n",
        "            \"input\": \"\",\n",
        "            \"output\": \"\"\n",
        "        }\n",
        "\n",
        "    def load_medmcqa_dataset(self, file_path: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Load and parse MedMCQA dataset from CSV file.\n",
        "\n",
        "        Expected format: question, exp, cop, opa, opb, opc, opd, correct_answer\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to MedMCQA CSV file\n",
        "\n",
        "        Returns:\n",
        "            List of parsed MedMCQA samples\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading MedMCQA dataset from {file_path}\")\n",
        "\n",
        "        try:\n",
        "            import pandas as pd\n",
        "\n",
        "            # Load CSV with proper handling\n",
        "            df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "            # Convert to list of dictionaries\n",
        "            data = df.to_dict('records')\n",
        "\n",
        "            # Clean any NaN values\n",
        "            for sample in data:\n",
        "                for key, value in sample.items():\n",
        "                    if pd.isna(value):\n",
        "                        sample[key] = \"\"\n",
        "\n",
        "            logger.info(f\"Loaded {len(data)} MedMCQA samples\")\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading MedMCQA dataset: {e}\")\n",
        "            raise\n",
        "\n",
        "    def load_orca_dataset(self, file_path: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Load and parse Orca dataset from JSON file.\n",
        "\n",
        "        Expected formats:\n",
        "        - Alpaca format: {\"instruction\": ..., \"input\": ..., \"output\": ...}\n",
        "        - OpenOrca format: {\"question\": ..., \"response\": ...}\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to Orca JSON file\n",
        "\n",
        "        Returns:\n",
        "            List of parsed Orca samples\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading Orca dataset from {file_path}\")\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Handle different possible JSON structures\n",
        "            if isinstance(data, dict):\n",
        "                # If it's a dict, look for common keys that might contain the data\n",
        "                for key in ['data', 'samples', 'examples', 'train']:\n",
        "                    if key in data:\n",
        "                        data = data[key]\n",
        "                        break\n",
        "                else:\n",
        "                    # If no common key found, assume it's a single sample\n",
        "                    data = [data]\n",
        "\n",
        "            logger.info(f\"Loaded {len(data)} Orca samples\")\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading Orca dataset: {e}\")\n",
        "            raise\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean and normalize text data.\n",
        "\n",
        "        Args:\n",
        "            text: Raw text to clean\n",
        "\n",
        "        Returns:\n",
        "            Cleaned text\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Remove special characters that might cause issues\n",
        "        text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
        "\n",
        "        # Strip leading/trailing whitespace\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def standardize_medmcqa_sample(self, sample: Dict[str, Any]) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Convert MedMCQA sample to unified schema.\n",
        "\n",
        "        Args:\n",
        "            sample: Raw MedMCQA sample with columns: question, exp, cop, opa, opb, opc, opd, correct_answer\n",
        "\n",
        "        Returns:\n",
        "            Standardized sample in unified schema\n",
        "        \"\"\"\n",
        "        # Extract fields with fallbacks\n",
        "        question = sample.get('question', '')\n",
        "        explanation = sample.get('exp', '')\n",
        "\n",
        "        # Extract options\n",
        "        option_a = sample.get('opa', '')\n",
        "        option_b = sample.get('opb', '')\n",
        "        option_c = sample.get('opc', '')\n",
        "        option_d = sample.get('opd', '')\n",
        "\n",
        "        # Get correct answer\n",
        "        correct_answer = sample.get('correct_answer', '')\n",
        "\n",
        "        # Clean text\n",
        "        question = self.clean_text(question)\n",
        "        explanation = self.clean_text(explanation)\n",
        "        option_a = self.clean_text(option_a)\n",
        "        option_b = self.clean_text(option_b)\n",
        "        option_c = self.clean_text(option_c)\n",
        "        option_d = self.clean_text(option_d)\n",
        "        correct_answer = self.clean_text(correct_answer)\n",
        "\n",
        "        # Create formatted multiple choice question\n",
        "        options_text = f\"A) {option_a}\\nB) {option_b}\\nC) {option_c}\\nD) {option_d}\"\n",
        "\n",
        "        # Create instruction combining question and options\n",
        "        instruction = f\"Question: {question}\\n\\nOptions:\\n{options_text}\\n\\nPlease select the correct answer and provide a brief explanation.\"\n",
        "\n",
        "        # Create output with correct answer and explanation\n",
        "        # Map correct answer to letter format\n",
        "        answer_mapping = {\n",
        "            option_a: \"A\",\n",
        "            option_b: \"B\",\n",
        "            option_c: \"C\",\n",
        "            option_d: \"D\"\n",
        "        }\n",
        "\n",
        "        # Find the letter corresponding to correct answer\n",
        "        correct_letter = \"A\"  # default\n",
        "        for option_text, letter in answer_mapping.items():\n",
        "            if option_text.lower().strip() == correct_answer.lower().strip():\n",
        "                correct_letter = letter\n",
        "                break\n",
        "\n",
        "        # Format the complete answer\n",
        "        if explanation:\n",
        "            output = f\"The correct answer is {correct_letter}) {correct_answer}.\\n\\nExplanation: {explanation}\"\n",
        "        else:\n",
        "            output = f\"The correct answer is {correct_letter}) {correct_answer}.\"\n",
        "\n",
        "        return {\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": \"\",\n",
        "            \"output\": output\n",
        "        }\n",
        "\n",
        "    def standardize_orca_sample(self, sample: Dict[str, Any]) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Convert Orca sample to unified schema.\n",
        "\n",
        "        Args:\n",
        "            sample: Raw Orca sample\n",
        "\n",
        "        Returns:\n",
        "            Standardized sample in unified schema\n",
        "        \"\"\"\n",
        "        # Handle different Orca formats\n",
        "        if 'instruction' in sample:\n",
        "            # Alpaca format\n",
        "            instruction = sample.get('instruction', '')\n",
        "            input_text = sample.get('input', '')\n",
        "            output_text = sample.get('output', sample.get('response', ''))\n",
        "        elif 'question' in sample:\n",
        "            # OpenOrca format\n",
        "            instruction = sample.get('question', '')\n",
        "            input_text = sample.get('input', '')\n",
        "            output_text = sample.get('response', sample.get('answer', ''))\n",
        "        else:\n",
        "            # Try to infer from available keys\n",
        "            instruction = sample.get('prompt', sample.get('text', ''))\n",
        "            input_text = ''\n",
        "            output_text = sample.get('completion', sample.get('target', ''))\n",
        "\n",
        "        # Clean text\n",
        "        instruction = self.clean_text(instruction)\n",
        "        input_text = self.clean_text(input_text)\n",
        "        output_text = self.clean_text(output_text)\n",
        "\n",
        "        return {\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": input_text,\n",
        "            \"output\": output_text\n",
        "        }\n",
        "\n",
        "    def filter_valid_samples(self, samples: List[Dict[str, str]], dataset_name: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Filter out invalid samples (missing required fields).\n",
        "\n",
        "        Args:\n",
        "            samples: List of standardized samples\n",
        "            dataset_name: Name of dataset for logging\n",
        "\n",
        "        Returns:\n",
        "            List of valid samples\n",
        "        \"\"\"\n",
        "        valid_samples = []\n",
        "\n",
        "        for sample in samples:\n",
        "            # Check if required fields are present and non-empty\n",
        "            if (sample.get('instruction', '').strip() and\n",
        "                sample.get('output', '').strip()):\n",
        "                valid_samples.append(sample)\n",
        "\n",
        "        filtered_count = len(samples) - len(valid_samples)\n",
        "        if filtered_count > 0:\n",
        "            logger.warning(f\"Filtered {filtered_count} invalid samples from {dataset_name}\")\n",
        "\n",
        "        logger.info(f\"Valid {dataset_name} samples: {len(valid_samples)}\")\n",
        "        return valid_samples\n",
        "\n",
        "    def stratified_split(self, samples: List[Dict[str, str]], eval_ratio: float) -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:\n",
        "        \"\"\"\n",
        "        Split samples into train/eval with stratification (if possible).\n",
        "\n",
        "        Args:\n",
        "            samples: List of samples to split\n",
        "            eval_ratio: Ratio of samples to use for evaluation\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (train_samples, eval_samples)\n",
        "        \"\"\"\n",
        "        # Shuffle samples\n",
        "        shuffled_samples = samples.copy()\n",
        "        random.shuffle(shuffled_samples)\n",
        "\n",
        "        # Simple random split (stratification would require more complex logic)\n",
        "        eval_size = int(len(shuffled_samples) * eval_ratio)\n",
        "        eval_samples = shuffled_samples[:eval_size]\n",
        "        train_samples = shuffled_samples[eval_size:]\n",
        "\n",
        "        return train_samples, eval_samples\n",
        "\n",
        "    def process_datasets(self, medmcqa_path: str, orca_path: str) -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:\n",
        "        \"\"\"\n",
        "        Process both datasets and create train/eval splits.\n",
        "\n",
        "        Args:\n",
        "            medmcqa_path: Path to MedMCQA dataset\n",
        "            orca_path: Path to Orca dataset\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (train_samples, eval_samples)\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting dataset processing...\")\n",
        "\n",
        "        # Load datasets\n",
        "        medmcqa_raw = self.load_medmcqa_dataset(medmcqa_path)\n",
        "        orca_raw = self.load_orca_dataset(orca_path)\n",
        "\n",
        "        # Check if Orca dataset is empty\n",
        "        if not orca_raw:\n",
        "            logger.warning(\"Orca dataset is empty - using 100% MedMCQA\")\n",
        "            self.radqa_ratio = 1.0\n",
        "            self.orca_ratio = 0.0\n",
        "\n",
        "        # Standardize formats\n",
        "        logger.info(\"Standardizing MedMCQA samples...\")\n",
        "        medmcqa_standardized = [self.standardize_medmcqa_sample(sample) for sample in medmcqa_raw]\n",
        "        medmcqa_valid = self.filter_valid_samples(medmcqa_standardized, \"MedMCQA\")\n",
        "\n",
        "        if orca_raw:\n",
        "            logger.info(\"Standardizing Orca samples...\")\n",
        "            orca_standardized = [self.standardize_orca_sample(sample) for sample in orca_raw]\n",
        "            orca_valid = self.filter_valid_samples(orca_standardized, \"Orca\")\n",
        "        else:\n",
        "            logger.info(\"Skipping Orca processing (empty dataset)\")\n",
        "            orca_valid = []\n",
        "\n",
        "        # Split each dataset into train/eval\n",
        "        medmcqa_train, medmcqa_eval = self.stratified_split(medmcqa_valid, self.eval_split)\n",
        "\n",
        "        if orca_valid:\n",
        "            orca_train, orca_eval = self.stratified_split(orca_valid, self.eval_split)\n",
        "        else:\n",
        "            orca_train, orca_eval = [], []\n",
        "\n",
        "        logger.info(f\"MedMCQA splits - Train: {len(medmcqa_train)}, Eval: {len(medmcqa_eval)}\")\n",
        "        logger.info(f\"Orca splits - Train: {len(orca_train)}, Eval: {len(orca_eval)}\")\n",
        "\n",
        "        # Calculate target sizes for mixing\n",
        "        if orca_train:\n",
        "            total_train_target = len(medmcqa_train) + len(orca_train)\n",
        "            medmcqa_train_target = int(total_train_target * self.radqa_ratio)\n",
        "            orca_train_target = int(total_train_target * self.orca_ratio)\n",
        "\n",
        "            # Sample according to target ratios\n",
        "            if len(medmcqa_train) > medmcqa_train_target:\n",
        "                medmcqa_train = random.sample(medmcqa_train, medmcqa_train_target)\n",
        "            if len(orca_train) > orca_train_target:\n",
        "                orca_train = random.sample(orca_train, orca_train_target)\n",
        "\n",
        "        # Mix training data\n",
        "        mixed_train = medmcqa_train + orca_train\n",
        "        random.shuffle(mixed_train)\n",
        "\n",
        "        # Mix evaluation data\n",
        "        mixed_eval = medmcqa_eval + orca_eval\n",
        "        random.shuffle(mixed_eval)\n",
        "\n",
        "        logger.info(f\"Final mixed dataset - Train: {len(mixed_train)}, Eval: {len(mixed_eval)}\")\n",
        "        if orca_train:\n",
        "            logger.info(f\"Training set composition - MedMCQA: {len(medmcqa_train)} ({len(medmcqa_train)/len(mixed_train)*100:.1f}%), \"\n",
        "                       f\"Orca: {len(orca_train)} ({len(orca_train)/len(mixed_train)*100:.1f}%)\")\n",
        "        else:\n",
        "            logger.info(f\"Training set composition - MedMCQA: 100%\")\n",
        "\n",
        "        return mixed_train, mixed_eval\n",
        "\n",
        "    def save_dataset(self, samples: List[Dict[str, str]], output_path: str):\n",
        "        \"\"\"\n",
        "        Save dataset to JSON file.\n",
        "\n",
        "        Args:\n",
        "            samples: List of samples to save\n",
        "            output_path: Path to output JSON file\n",
        "        \"\"\"\n",
        "        logger.info(f\"Saving {len(samples)} samples to {output_path}\")\n",
        "\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(samples, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"Dataset saved successfully to {output_path}\")\n",
        "\n",
        "    def generate_dataset_stats(self, train_samples: List[Dict[str, str]], eval_samples: List[Dict[str, str]]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate statistics about the processed dataset.\n",
        "\n",
        "        Args:\n",
        "            train_samples: Training samples\n",
        "            eval_samples: Evaluation samples\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing dataset statistics\n",
        "        \"\"\"\n",
        "        stats = {\n",
        "            \"total_samples\": len(train_samples) + len(eval_samples),\n",
        "            \"train_samples\": len(train_samples),\n",
        "            \"eval_samples\": len(eval_samples),\n",
        "            \"train_eval_ratio\": f\"{len(train_samples)}/{len(eval_samples)}\",\n",
        "            \"avg_instruction_length\": sum(len(s['instruction'].split()) for s in train_samples + eval_samples) / (len(train_samples) + len(eval_samples)),\n",
        "            \"avg_output_length\": sum(len(s['output'].split()) for s in train_samples + eval_samples) / (len(train_samples) + len(eval_samples)),\n",
        "            \"samples_with_input\": sum(1 for s in train_samples + eval_samples if s['input'].strip()),\n",
        "            \"processing_config\": {\n",
        "                \"radqa_ratio\": self.radqa_ratio,\n",
        "                \"orca_ratio\": self.orca_ratio,\n",
        "                \"eval_split\": self.eval_split,\n",
        "                \"seed\": self.seed\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return stats"
      ],
      "metadata": {
        "id": "8S60tTFvXOmI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab Setup Functions\n",
        "def setup_colab_environment():\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "        print(\"Running in Google Colab environment\")\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "        print(\"Running in local environment\")\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(\"Installing required packages...\")\n",
        "        os.system(\"pip install -q pandas scikit-learn datasets huggingface_hub\")\n",
        "\n",
        "        # Mount Google Drive.\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            print(\"Mounting Google Drive...\")\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive mounted successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not mount Google Drive: {e}\")\n",
        "\n",
        "    return IN_COLAB\n",
        "\n",
        "# Upload files in Google Colab environment.\n",
        "def upload_files_colab():\n",
        "    try:\n",
        "        from google.colab import files\n",
        "\n",
        "        print(\"Please upload your dataset files:\")\n",
        "        print(\"1. Upload your MedMCQA CSV file (extracted_medmcqa.csv)\")\n",
        "        print(\"2. Upload your Orca JSON file\")\n",
        "\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        uploaded_files = list(uploaded.keys())\n",
        "        print(f\"Uploaded files: {uploaded_files}\")\n",
        "\n",
        "        # Try to identify file types\n",
        "        medmcqa_path = None\n",
        "        orca_path = None\n",
        "\n",
        "        for filename in uploaded_files:\n",
        "            if filename.endswith('.csv'):\n",
        "                medmcqa_path = filename\n",
        "                print(f\"Detected MedMCQA CSV: {filename}\")\n",
        "            elif filename.endswith('.json'):\n",
        "                orca_path = filename\n",
        "                print(f\"Detected Orca JSON: {filename}\")\n",
        "\n",
        "        return medmcqa_path, orca_path, uploaded_files\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab - file upload not available\")\n",
        "        return None, None, []\n",
        "\n",
        "# Interactive function for Google Colab execution.\n",
        "def run_colab_interactive():\n",
        "    print(\"Medical Chatbot Dataset Preparation - Google Colab Version\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Setup environment\n",
        "    IN_COLAB = setup_colab_environment()\n",
        "\n",
        "    medmcqa_path = None\n",
        "    orca_path = None\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(\"MedMCQA: Loading from Google Drive\")\n",
        "        print(\"OpenOrca: Loading from Hugging Face\")\n",
        "\n",
        "        # Load OpenOrca from Hugging Face\n",
        "        try:\n",
        "            from datasets import load_dataset\n",
        "            print(\"Loading OpenOrca from Hugging Face.\")\n",
        "            print(\"Note: This may take a few minutes for the first download\")\n",
        "\n",
        "            # Use the exact method you specified\n",
        "            print(\"Using your specified loading method.\")\n",
        "            ds = load_dataset(\"Open-Orca/OpenOrca\")\n",
        "\n",
        "            # Get the train split\n",
        "            orca_dataset = ds['train']\n",
        "            print(f\"Successfully loaded OpenOrca dataset with {len(orca_dataset)} samples\")\n",
        "\n",
        "            # Take a reasonable subset for processing (50K samples)\n",
        "            print(\"Taking subset of 50,000 samples for efficient processing.\")\n",
        "            subset_size = min(50000, len(orca_dataset))\n",
        "            orca_dataset = orca_dataset.select(range(subset_size))\n",
        "\n",
        "            # Convert to list\n",
        "            orca_raw = orca_dataset.to_list()\n",
        "\n",
        "            # Save to temporary file for processing\n",
        "            orca_path = '/tmp/orca_hf.json'\n",
        "            print(\"Saving OpenOrca subset to temporary file...\")\n",
        "            with open(orca_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(orca_raw, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"Saved {len(orca_raw)} OpenOrca samples to: {orca_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading OpenOrca from Hugging Face: {e}\")\n",
        "            print(\"Falling back to manual options.\")\n",
        "            print(\"You can:\")\n",
        "            print(\"1. Upload an Orca JSON file manually\")\n",
        "            print(\"2. Use a different Orca dataset (e.g., smaller one)\")\n",
        "            print(\"3. Skip Orca and use 100% MedMCQA\")\n",
        "\n",
        "            fallback_option = input(\"Choose option (1/2/3): \").strip()\n",
        "\n",
        "            if fallback_option == \"1\":\n",
        "                orca_path = input(\"Enter path to Orca JSON file: \").strip()\n",
        "                if not os.path.exists(orca_path):\n",
        "                    print(f\"File not found: {orca_path}\")\n",
        "                    return\n",
        "            elif fallback_option == \"2\":\n",
        "                print(\"Available smaller Orca-like datasets:\")\n",
        "                print(\"- teknium/OpenHermes-2.5 (smaller, high quality)\")\n",
        "                print(\"- microsoft/orca-math-word-problems-200k\")\n",
        "                print(\"- Or provide your own dataset name\")\n",
        "\n",
        "                alt_dataset = input(\"Enter alternative dataset name: \").strip()\n",
        "                if alt_dataset:\n",
        "                    try:\n",
        "                        print(f\"Loading {alt_dataset}...\")\n",
        "                        alt_ds = load_dataset(alt_dataset)\n",
        "                        if 'train' in alt_ds:\n",
        "                            alt_orca_dataset = alt_ds['train']\n",
        "                        else:\n",
        "                            split_name = list(alt_ds.keys())[0]\n",
        "                            alt_orca_dataset = alt_ds[split_name]\n",
        "                            print(f\"Using split: {split_name}\")\n",
        "\n",
        "                        # Subset.\n",
        "                        subset_size = min(20000, len(alt_orca_dataset))\n",
        "                        alt_orca_dataset = alt_orca_dataset.select(range(subset_size))\n",
        "                        orca_raw = alt_orca_dataset.to_list()\n",
        "\n",
        "                        orca_path = '/tmp/alt_orca.json'\n",
        "                        with open(orca_path, 'w', encoding='utf-8') as f:\n",
        "                            json.dump(orca_raw, f, indent=2, ensure_ascii=False)\n",
        "                        print(f\"Loaded {len(orca_raw)} samples from {alt_dataset}\")\n",
        "\n",
        "                    except Exception as e2:\n",
        "                        print(f\"Error loading alternative dataset: {e2}\")\n",
        "                        print(\"Falling back to 100% MedMCQA\")\n",
        "                        orca_path = '/tmp/empty_orca.json'\n",
        "                        with open(orca_path, 'w', encoding='utf-8') as f:\n",
        "                            json.dump([], f)\n",
        "                else:\n",
        "                    print(\"No dataset specified, using 100% MedMCQA\")\n",
        "                    orca_path = '/tmp/empty_orca.json'\n",
        "                    with open(orca_path, 'w', encoding='utf-8') as f:\n",
        "                        json.dump([], f)\n",
        "\n",
        "            elif fallback_option == \"3\":\n",
        "                orca_path = '/tmp/empty_orca.json'\n",
        "                with open(orca_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump([], f)\n",
        "                print(\"Using 100% MedMCQA dataset\")\n",
        "            else:\n",
        "                print(\"Invalid option selected. Using 100% MedMCQA as fallback.\")\n",
        "                orca_path = '/tmp/empty_orca.json'\n",
        "                with open(orca_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump([], f)\n",
        "\n",
        "        # Load MedMCQA from Google Drive\n",
        "        print(\"Loading MedMCQA from Google Drive.\")\n",
        "\n",
        "        #Google Drive paths\n",
        "        possible_paths = [\n",
        "            '/content/drive/MyDrive/extracted_medmcqa.csv',\n",
        "            '/content/drive/My Drive/extracted_medmcqa.csv',\n",
        "            '/content/drive/MyDrive/medmcqa/extracted_medmcqa.csv',\n",
        "            '/content/drive/My Drive/medmcqa/extracted_medmcqa.csv',\n",
        "            '/content/drive/MyDrive/data/extracted_medmcqa.csv',\n",
        "            '/content/drive/My Drive/data/extracted_medmcqa.csv'\n",
        "        ]\n",
        "\n",
        "        found_file = False\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                medmcqa_path = path\n",
        "                found_file = True\n",
        "                print(f\"Found MedMCQA at: {path}\")\n",
        "                break\n",
        "\n",
        "        if not found_file:\n",
        "            print(\"MedMCQA file not found in common locations.\")\n",
        "            print(\"Please provide the full path to your MedMCQA CSV file in Google Drive:\")\n",
        "            print(\"   Example: /content/drive/MyDrive/your_folder/extracted_medmcqa.csv\")\n",
        "            medmcqa_path = input(\"MedMCQA CSV file path: \").strip()\n",
        "\n",
        "            if not os.path.exists(medmcqa_path):\n",
        "                print(f\"File not found: {medmcqa_path}\")\n",
        "                print(\"Tips:\")\n",
        "                print(\"   1. Make sure Google Drive is mounted\")\n",
        "                print(\"   2. Check the exact file path and spelling\")\n",
        "                print(\"   3. Ensure the file is in your Google Drive\")\n",
        "                return\n",
        "\n",
        "        # MedMCQA file\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            test_df = pd.read_csv(medmcqa_path, nrows=1)\n",
        "            print(f\"MedMCQA file verified: {medmcqa_path}\")\n",
        "            print(f\"Columns found: {list(test_df.columns)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading MedMCQA file: {e}\")\n",
        "            return\n",
        "\n",
        "    else:\n",
        "        # Local environment.\n",
        "        print(\"Enter dataset file paths:\")\n",
        "        medmcqa_path = input(\"MedMCQA CSV file path: \").strip()\n",
        "        orca_path = input(\"Orca JSON file path: \").strip()\n",
        "\n",
        "    # Configuration parameters.\n",
        "    print(\"Configuration Parameters:\")\n",
        "    medmcqa_ratio = float(input(\"MedMCQA ratio (0.75): \").strip() or \"0.75\")\n",
        "    eval_split = float(input(\"Evaluation split (0.1): \").strip() or \"0.1\")\n",
        "    seed = int(input(\"Random seed (42): \").strip() or \"42\")\n",
        "    output_dir = input(\"Output directory (./data): \").strip() or \"./data\"\n",
        "\n",
        "    # Validate the inputs.\n",
        "    if not os.path.exists(medmcqa_path):\n",
        "        print(f\"MedMCQA file not found: {medmcqa_path}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(orca_path):\n",
        "        print(f\"Orca file not found: {orca_path}\")\n",
        "        return\n",
        "\n",
        "    if not 0 < medmcqa_ratio < 1:\n",
        "        print(f\"MedMCQA ratio must be between 0 and 1, got {medmcqa_ratio}\")\n",
        "        return\n",
        "\n",
        "    if not 0 < eval_split < 1:\n",
        "        print(f\"Eval split must be between 0 and 1, got {eval_split}\")\n",
        "        return\n",
        "\n",
        "    # Initialize processor\n",
        "    print(f\"Initializing dataset processor.\")\n",
        "    processor = DatasetProcessor(\n",
        "        radqa_ratio=medmcqa_ratio,\n",
        "        eval_split=eval_split,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        print(\"Processing datasets.\")\n",
        "        train_samples, eval_samples = processor.process_datasets(medmcqa_path, orca_path)\n",
        "\n",
        "        train_path = os.path.join(output_dir, 'train.json')\n",
        "        eval_path = os.path.join(output_dir, 'eval.json')\n",
        "\n",
        "        print(\"Saving datasets.\")\n",
        "        processor.save_dataset(train_samples, train_path)\n",
        "        processor.save_dataset(eval_samples, eval_path)\n",
        "\n",
        "        # Statistics.\n",
        "        stats = processor.generate_dataset_stats(train_samples, eval_samples)\n",
        "        stats_path = os.path.join(output_dir, 'dataset_stats.json')\n",
        "\n",
        "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Summary.\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"DATASET PREPARATION COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total samples: {stats['total_samples']}\")\n",
        "        print(f\"Training samples: {stats['train_samples']}\")\n",
        "        print(f\"Evaluation samples: {stats['eval_samples']}\")\n",
        "        print(f\"Average instruction length: {stats['avg_instruction_length']:.1f} words\")\n",
        "        print(f\"Average output length: {stats['avg_output_length']:.1f} words\")\n",
        "        print(f\"Samples with input field: {stats['samples_with_input']}\")\n",
        "        print(f\"Files saved to: {output_dir}\")\n",
        "        print(f\"   - train.json: {stats['train_samples']} samples\")\n",
        "        print(f\"   - eval.json: {stats['eval_samples']} samples\")\n",
        "        print(f\"   - dataset_stats.json: Processing statistics\")\n",
        "\n",
        "        # Download the files in Colab.\n",
        "        if IN_COLAB:\n",
        "            print(\"Downloading processed files.\")\n",
        "            try:\n",
        "                from google.colab import files\n",
        "                files.download(train_path)\n",
        "                files.download(eval_path)\n",
        "                files.download(stats_path)\n",
        "                print(\"Files downloaded successfully!\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not download files: {e}\")\n",
        "                print(f\"Files are available at: {output_dir}\")\n",
        "\n",
        "        print(\"Ready for next step: Chain-of-Thought generation.\")\n",
        "\n",
        "        return train_path, eval_path, stats_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during dataset preparation: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "# Handles both CLI and Colab.\n",
        "def main():\n",
        "    try:\n",
        "        # Try to detect if we're in Colab or have arguments.\n",
        "        import sys\n",
        "\n",
        "        try:\n",
        "            import google.colab\n",
        "            IN_COLAB = True\n",
        "        except ImportError:\n",
        "            IN_COLAB = False\n",
        "\n",
        "        if IN_COLAB or len(sys.argv) == 1:\n",
        "            return run_colab_interactive()\n",
        "\n",
        "        parser = argparse.ArgumentParser(description='Prepare and mix MedMCQA and Orca datasets for medical chatbot fine-tuning')\n",
        "\n",
        "        parser.add_argument('--medmcqa_path', type=str, required=True,\n",
        "                           help='Path to MedMCQA dataset CSV file')\n",
        "        parser.add_argument('--orca_path', type=str, required=True,\n",
        "                           help='Path to Orca dataset JSON file')\n",
        "        parser.add_argument('--output_dir', type=str, default='./data',\n",
        "                           help='Directory to save processed datasets (default: ./data)')\n",
        "        parser.add_argument('--medmcqa_ratio', type=float, default=0.75,\n",
        "                           help='Proportion of MedMCQA samples in final dataset (default: 0.75)')\n",
        "        parser.add_argument('--eval_split', type=float, default=0.1,\n",
        "                           help='Proportion of data to use for evaluation (default: 0.1)')\n",
        "        parser.add_argument('--seed', type=int, default=42,\n",
        "                           help='Random seed for reproducibility (default: 42)')\n",
        "\n",
        "        args = parser.parse_args()\n",
        "\n",
        "        if not os.path.exists(args.medmcqa_path):\n",
        "            logger.error(f\"MedMCQA dataset not found: {args.medmcqa_path}\")\n",
        "            return\n",
        "\n",
        "        if not os.path.exists(args.orca_path):\n",
        "            logger.error(f\"Orca dataset not found: {args.orca_path}\")\n",
        "            return\n",
        "\n",
        "        if not 0 < args.medmcqa_ratio < 1:\n",
        "            logger.error(f\"MedMCQA ratio must be between 0 and 1, got {args.medmcqa_ratio}\")\n",
        "            return\n",
        "\n",
        "        if not 0 < args.eval_split < 1:\n",
        "            logger.error(f\"Eval split must be between 0 and 1, got {args.eval_split}\")\n",
        "            return\n",
        "\n",
        "        # Processor.\n",
        "        processor = DatasetProcessor(\n",
        "            radqa_ratio=args.medmcqa_ratio,\n",
        "            eval_split=args.eval_split,\n",
        "            seed=args.seed\n",
        "        )\n",
        "\n",
        "        train_samples, eval_samples = processor.process_datasets(args.medmcqa_path, args.orca_path)\n",
        "\n",
        "        # Processed datasets.\n",
        "        train_path = os.path.join(args.output_dir, 'train.json')\n",
        "        eval_path = os.path.join(args.output_dir, 'eval.json')\n",
        "\n",
        "        processor.save_dataset(train_samples, train_path)\n",
        "        processor.save_dataset(eval_samples, eval_path)\n",
        "\n",
        "        # Statistics.\n",
        "        stats = processor.generate_dataset_stats(train_samples, eval_samples)\n",
        "        stats_path = os.path.join(args.output_dir, 'dataset_stats.json')\n",
        "\n",
        "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(\"Dataset preparation completed successfully!\")\n",
        "        logger.info(f\"Statistics saved to {stats_path}\")\n",
        "\n",
        "        # Summary.\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"DATASET PREPARATION SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Total samples: {stats['total_samples']}\")\n",
        "        print(f\"Training samples: {stats['train_samples']}\")\n",
        "        print(f\"Evaluation samples: {stats['eval_samples']}\")\n",
        "        print(f\"Average instruction length: {stats['avg_instruction_length']:.1f} words\")\n",
        "        print(f\"Average output length: {stats['avg_output_length']:.1f} words\")\n",
        "        print(f\"Samples with input field: {stats['samples_with_input']}\")\n",
        "        print(f\"\\nFiles saved to: {args.output_dir}\")\n",
        "        print(f\"- train.json: {stats['train_samples']} samples\")\n",
        "        print(f\"- eval.json: {stats['eval_samples']} samples\")\n",
        "        print(f\"- dataset_stats.json: Processing statistics\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during dataset preparation: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "3UcDn_4SXVas",
        "outputId": "b596cf49-b0ad-4a5c-9b26-20742f45acad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical Chatbot Dataset Preparation - Google Colab Version\n",
            "============================================================\n",
            "Running in Google Colab environment\n",
            "Installing required packages...\n",
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully!\n",
            "MedMCQA: Loading from Google Drive\n",
            "OpenOrca: Loading from Hugging Face\n",
            "Loading OpenOrca from Hugging Face.\n",
            "Note: This may take a few minutes for the first download\n",
            "Using your specified loading method.\n",
            "Error loading OpenOrca from Hugging Face: Invalid pattern: '**' can only be an entire path component\n",
            "Falling back to manual options.\n",
            "You can:\n",
            "1. Upload an Orca JSON file manually\n",
            "2. Use a different Orca dataset (e.g., smaller one)\n",
            "3. Skip Orca and use 100% MedMCQA\n",
            "Choose option (1/2/3): 3\n",
            "Using 100% MedMCQA dataset\n",
            "Loading MedMCQA from Google Drive.\n",
            "Found MedMCQA at: /content/drive/MyDrive/extracted_medmcqa.csv\n",
            "MedMCQA file verified: /content/drive/MyDrive/extracted_medmcqa.csv\n",
            "Columns found: ['question', 'exp', 'cop', 'opa', 'opb', 'opc', 'opd', 'correct_answer']\n",
            "Configuration Parameters:\n",
            "MedMCQA ratio (0.75): \n",
            "Evaluation split (0.1): \n",
            "Random seed (42): \n",
            "Output directory (./data): \n",
            "Initializing dataset processor.\n",
            "Processing datasets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Orca dataset is empty - using 100% MedMCQA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving datasets.\n",
            "\n",
            "============================================================\n",
            "DATASET PREPARATION COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Total samples: 182822\n",
            "Training samples: 164540\n",
            "Evaluation samples: 18282\n",
            "Average instruction length: 39.1 words\n",
            "Average output length: 76.0 words\n",
            "Samples with input field: 0\n",
            "Files saved to: ./data\n",
            "   - train.json: 164540 samples\n",
            "   - eval.json: 18282 samples\n",
            "   - dataset_stats.json: Processing statistics\n",
            "Downloading processed files.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6b303f78-4fe3-449e-834d-a2e7271b5a70\", \"train.json\", 139628352)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0b9bf169-6d39-40fa-bab4-3fc7070374cd\", \"eval.json\", 15342209)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_830406a9-25e1-4c5c-a198-4bcb62d6c78e\", \"dataset_stats.json\", 351)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files downloaded successfully!\n",
            "Ready for next step: Chain-of-Thought generation.\n"
          ]
        }
      ]
    }
  ]
}